<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>vimure.synthetic API documentation</title>
<meta name="description" content="Code to generate synthetic networks that emulates directed double-sample questions networks" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vimure.synthetic</code></h1>
</header>
<section id="section-intro">
<p>Code to generate synthetic networks that emulates directed double-sample questions networks</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Code to generate synthetic networks that emulates directed double-sample questions networks&#34;&#34;&#34;

import math

import numpy as np
import pandas as pd
import networkx as nx
import sktensor as skt

from abc import ABCMeta, abstractmethod
from .io import BaseNetwork, DEFAULT_SEED
from .log import setup_logging
from .utils import preprocess, sptensor_from_dense_array

DEFAULT_N = 100
DEFAULT_M = 100
DEFAULT_L = 1
DEFAULT_K = 2

DEFAULT_C = 2
DEFAULT_STRUCTURE = None
DEFAULT_SPARSIFY = True
DEFAULT_OVERLAPPING = 0.0

DEFAULT_EXP_IN = 2
DEFAULT_EXP_OUT = 2.5

DEFAULT_ETA = 0.5
DEFAULT_AVG_DEGREE = 2

module_logger = setup_logging(&#34;vm.synthetic&#34;)

def transpose_ij(M):
    &#34;&#34;&#34;
    Compute the transpose of a matrix.

    Parameters
    ----------
    M : numpy.array
        Numpy matrix.

    Returns
    -------
    Transpose of the matrix.
    &#34;&#34;&#34;

    return np.einsum(&#34;ij-&gt;ji&#34;, M)


class BaseSyntheticNetwork(BaseNetwork, metaclass=ABCMeta):
    &#34;&#34;&#34;
    A base abstract class for generation and management of synthetic networks. 
    Suitable for representing any type of synthetic network (whether SBM or not).
    &#34;&#34;&#34;
    def __init__(
        self,
        N: int = DEFAULT_N,
        M: int = DEFAULT_M,
        L: int = DEFAULT_L,
        K: int = DEFAULT_K,
        seed: int = DEFAULT_SEED,
        **kwargs,
    ):
        super().__init__(N=N, M=M, L=L, K=K, seed=seed, **kwargs)

    @abstractmethod
    def generate_lv(self):
        pass

    @abstractmethod
    def build_Y(self):
        pass

    def build_X(
        self,
        mutuality: float = 0.5,
        sh_theta: float = 2.0,
        sc_theta: float = 0.5,
        flag_self_reporter: bool = True,
        cutoff_X: bool = False,
        lambda_diff: float = None,
        Q: int = None,
        seed: int = None,
        theta: np.ndarray = None,
        verbose: bool = True,
    ):
        &#34;&#34;&#34;
        Any object inhereted from BaseSyntheticNetwork will have a ground truth network Y.
        Given that Y, generate the observed network X.
        
        Parameters
        ----------

        mutuality : float
            The mutuality parameter from 0 to 1.
        sh_theta : float
            Shape of gamma distribution from which to draw theta. 
            The &#39;reliability&#39; of nodes is represented by the parameter `theta_{lm}`
            and by default are modelled as a gamma function with shape `sh_theta` and scale `sc_theta`.
        sc_theta : float
            Scale of gamma distribution from which to draw theta.
        flag_self_reporter : bool
            Indicates whether a node can only report about their own ties.
        Q : int
            Maximum value of X entries. If None, it will use the network&#39;s K parameter.
        cutoff_X : bool
            Whether to set X as a binary.
        lambda_diff : float
            The difference between each subsequent K.
        seed : int
            Pseudo random generator seed to use.
        verbose : bool
            Provides additional details.

        Returns
        -------
        X : sktensor
            Observed network.
        &#34;&#34;&#34;

        logger = setup_logging(&#34;vm.synthetic.generate_X&#34;, verbose)

        if mutuality &lt; 0 or mutuality &gt;= 1:
            msg = &#34;The mutuality parameter has to be in [0, 1)!&#34;
            raise ValueError(msg)

        # generate_x uses its own pseudo-random seed generator, so we can better control the generation of theta.
        # We can generate the same theta, while varying mutuality (to check how X change);
        if seed is None:
            seed = self.seed
        prng = np.random.RandomState(seed)

        Y = self.Y
        Y_subs = Y.subs
        N = self.N
        M = self.M
        L = self.L
        K = self.K

        if theta is not None:
            warn_msg = &#34;Ignoring sh_theta and sc_theta since a full theta matrix was informed&#34;
            logger.debug(warn_msg)

            if type(theta) != np.ndarray or theta.shape != (L, M):
                msg = &#34;theta matrix is not valid. When using this parameter, make sure to inform a %d x %d matrix.&#34;
                raise ValueError(msg % (L, M))
        else:
            # Generate theta (reliability)
            theta = prng.gamma(shape=sh_theta, scale=sc_theta, size=(L, M))

        LAMBDA_0 = 0.01
        # Generate theta (ties average interactions)
        lambda_k = np.ones(shape=Y.shape).astype(&#34;float&#34;) * LAMBDA_0

        if lambda_diff is not None:
            if lambda_diff &lt;= 0:
                msg = &#34;lambda_diff is optional but when set should be higher than 0!&#34;
                raise ValueError(msg)

            for k in range(1, K):
                vals_k = np.argwhere(Y.vals == k).flatten()
                lij = (Y_subs[0][vals_k], Y_subs[1][vals_k], Y_subs[2][vals_k])
                lambda_k[lij] = LAMBDA_0 + lambda_diff
        else:
            for k in range(1, K):
                vals_k = np.argwhere(Y.vals == k).flatten()
                lij = (Y_subs[0][vals_k], Y_subs[1][vals_k], Y_subs[2][vals_k])
                lambda_k[lij] = k

        M_X = np.einsum(&#34;lm,lij-&gt;lijm&#34;, theta, lambda_k)
        MM = (M_X + mutuality * np.transpose(M_X, axes=(0, 2, 1, 3))) / (1.0 - mutuality * mutuality)

        X = np.zeros_like(MM).astype(&#34;int&#34;)

        if cutoff_X and Q is None:
            Q = self.K

        if flag_self_reporter:
            R = build_self_reporter_mask(self)

            for l in range(L):
                for m in range(M):
                    subs_nz = np.where(R[l, :, :, m] &gt; 0)
                    len_subs_nz = subs_nz[0].shape[0]
                    for n in range(len_subs_nz):
                        i, j = subs_nz[0][n], subs_nz[1][n]
                        r = prng.rand(1)[0]

                        # for those reporters that report perfectly, i.e. theta=1, do not extract from a poisson.
                        # Rather, assign the X deterministically using the mean of the poisson
                        if np.allclose(theta[l, m], 1.0) == False:
                            if r &lt; 0.5:
                                X[l, i, j, m] = prng.poisson(MM[l, i, j, m] * R[l, i, j, m])
                                if cutoff_X:
                                    if X[l, i, j, m] &gt; Q - 1:
                                        X[l, i, j, m] = Q - 1
                                cond_exp = M_X[l, j, i, m] * R[l, i, j, m] + mutuality * X[l, i, j, m]
                                X[l, j, i, m] = prng.poisson(cond_exp)
                            else:
                                X[l, j, i, m] = prng.poisson(MM[l, j, i, m] * R[l, j, i, m])
                                if cutoff_X:
                                    if X[l, j, i, m] &gt; Q - 1:
                                        X[l, j, i, m] = Q - 1
                                cond_exp = M_X[l, i, j, m] * R[l, j, i, m] + mutuality * X[l, j, i, m]
                                X[l, i, j, m] = prng.poisson(cond_exp)
                        else:
                            if r &lt; 0.5:
                                X[l, i, j, m] = MM[l, i, j, m] * R[l, i, j, m]
                                if cutoff_X == True:
                                    if X[l, i, j, m] &gt; Q - 1:
                                        X[l, i, j, m] = Q - 1
                                cond_exp = M_X[l, j, i, m] * R[l, i, j, m] + mutuality * X[l, i, j, m]
                                X[l, j, i, m] = cond_exp
                            else:
                                X[l, j, i, m] = MM[l, j, i, m] * R[l, j, i, m]
                                if cutoff_X == True:
                                    if X[l, j, i, m] &gt; Q - 1:
                                        X[l, j, i, m] = Q - 1
                                cond_exp = M_X[l, i, j, m] * R[l, j, i, m] + mutuality * X[l, j, i, m]
                                X[l, i, j, m] = cond_exp

        else:
            R = np.ones((L, N, N, M))
            for l in range(L):
                for m in range(M):
                    for i in range(N):
                        for j in range(i + 1, N):
                            r = prng.rand(1)[0]
                            if r &lt; 0.5:
                                X[l, i, j, m] = prng.poisson(MM[l, i, j, m])
                                if cutoff_X:
                                    if X[l, i, j, m] &gt; Q - 1:
                                        X[l, i, j, m] = Q - 1
                                cond_exp = M_X[l, j, i, m] + mutuality * X[l, i, j, m]
                                X[l, j, i, m] = prng.poisson(cond_exp)
                            else:
                                X[l, j, i, m] = prng.poisson(MM[l, j, i, m])
                                if cutoff_X:
                                    if X[l, j, i, m] &gt; Q - 1:
                                        X[l, j, i, m] = Q - 1
                                cond_exp = M_X[l, i, j, m] + mutuality * X[l, j, i, m]
                                X[l, i, j, m] = prng.poisson(cond_exp)

        if cutoff_X:
            X[X &gt; Q - 1] = Q - 1  # cut-off, max entry has to be equal to K - 1

        self.X = preprocess(X)
        self.R = preprocess(R)
        self.theta = theta
        self.lambda_k = lambda_k
        self.mutuality = mutuality

        subs_lijm = self.X.subs

        &#34;&#34;&#34;
        BASELINE: UNION

        This is one of the simplest possible ways of collating all networks reported by independent reporters (X) 
        into a single adjacency matrix (Y).

        The baseline union we use here takes the union of all ties (l, i, j) that were reported at least once by someone
        regardless of who reported it or the strength given to each tie.

        Note: the output adjacency matrix is binary

        # TODO: Maybe the calculation of baseline adjacency matrices shouldn&#39;t be inside the build_X() function, 
        #       since it isn&#39;t a feature of the package itself?
        &#34;&#34;&#34;
        lij = (subs_lijm[0], subs_lijm[1], subs_lijm[2])
        all_ties = np.stack(lij).T  # dim ((lij)_subs, 3)
        union_ties, count_ties = np.unique(all_ties, axis=0, return_counts=True)

        # Convert (l,i,j) to format understood by sktensor
        union_subs = tuple(np.moveaxis(union_ties, 1, 0))
        X_union = skt.sptensor(
            subs=union_subs,
            vals=np.ones(union_ties.shape[0]),
            shape=(self.L, self.N, self.N),
            dtype=np.int8,
        )
        self.X_union = X_union

        &#34;&#34;&#34;
        BASELINE: INTERSECTION

        The intersection is another simple way to combine all networks reported by independent reporters (X) 
        into a single adjacency matrix (Y).

        Here, a tie (l, i, j) is only considered if all reporters **who were allowed to report on that tie, 
        as indicated by the reporter&#39;s mask R,** reported this tie.

        Similar to the union baseline, here we disregard the strength of ties. 

        # TODO: Maybe the calculation of baseline adjacency matrices shouldn&#39;t be inside the build_X() function, 
        #       since it isn&#39;t a feature of the package itself?
        &#34;&#34;&#34;

        &#34;&#34;&#34;
        Intersection Baseline | Step 1
        
        First create a skt.sptensor called max_reports_lij to hold the maximum number of reports a tie (l, i, j) could receive
        &#34;&#34;&#34;
        if isinstance(R, skt.dtensor) or isinstance(R, np.ndarray):
            # Since R is a dense tensor of dimensions (l, i, j, m), we just need to sum over the m dimension
            max_reports_lij = sptensor_from_dense_array(R.sum(axis=3))

            df_max_reports_lij = pd.DataFrame(max_reports_lij.subs).T
            df_max_reports_lij.columns = [&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]
            df_max_reports_lij[&#34;max&#34;] = max_reports_lij.vals
        elif isinstance(R, skt.sptensor):
            # Because R is a sparse tensor, we need to work out the sum another way.
            
            # When performing operations with skt.sptensor, I (@jonjoncardoso) find it more intuitive -- and faster -- 
            # to work with pandas groupby -&gt; apply() than manipulating their subs+vals with numpy directly.
            R_vals_df = pd.DataFrame(np.stack(self.R.subs).T, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;, &#34;m&#34;])

            df_max_reports_lij = R_vals_df.groupby([&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])\
                .apply(lambda x: pd.Series({&#34;max&#34;: sum([R[l, i, j, m]
                                                   if type(R[l, i, j, m]) == int else R[l, i, j, m][0] 
                                                   for m in range(self.M)])}))


        &#34;&#34;&#34;
        Intersection Baseline | Step 2

        Convert `union_ties` to pd.DataFrame
        &#34;&#34;&#34;
        df_union_ties = pd.DataFrame(union_ties, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])
        df_union_ties[&#34;count&#34;] = count_ties

        &#34;&#34;&#34;
        Intersection Baseline | Step 3

        Find out how many, of the union ties, were unanimous. 
        That is: all reporters **that could report** on the tie (l, i, j) did agree that this tie existed and have reported it.

        NOTE: When performing operations with skt.sptensor, I (@jonjoncardoso) find it more intuitive -- and faster -- 
              to work with pandas groupby -&gt; apply() or pandas merge than manipulating their subs+vals with numpy directly.
        &#34;&#34;&#34;
        aux_df = pd.merge(df_union_ties, df_max_reports_lij, how=&#34;left&#34;)
        # Results DataFrame has columns: [l, i, j, count, max]
        df_intersection_ties = aux_df[aux_df[&#34;count&#34;] == aux_df[&#34;max&#34;]]
        
        &#34;&#34;&#34;
        Intersection Baseline | Step 4

        Create a skt.sptensor to represent the intersection of X ties. 
        &#34;&#34;&#34;

        if df_intersection_ties.empty:
            # If NO REPORTER has agreed on any ties, than intersection is empty
            self.X_intersection = None
        else:
            self.X_intersection = skt.sptensor(
                subs=tuple(df_intersection_ties[[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]].T.values.tolist()),
                vals=np.ones(df_intersection_ties.shape[0]).tolist(),
                shape=(self.L, self.N, self.N),
                dtype=np.int8,
            )          

        # TODO: Rethink the baseline union &amp; intersections for the case that X is not a binary matrix (or even if Y is binary)

        return self

    def __repr__(self):
        return f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, K={self.K}, seed={self.seed})&#34;

    def __str__(self):
        return f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, K={self.K}, seed={self.seed})&#34;


class StandardSBM(BaseSyntheticNetwork):
    &#34;&#34;&#34;
    **Creates a standard stochastic block-model synthetic network.**

    A generative graph model which assumes the probability of connecting two nodes in a graph is determined entirely by their block assignments.
    For more information about this model, see Holland, P. W., Laskey, K. B., &amp; Leinhardt, S. (1983). _Stochastic blockmodels: First steps. Social networks_, 5(2), 109-137.
    [DOI:10.1016/0378-8733(83)90021-7](https://www.sciencedirect.com/science/article/abs/pii/0378873383900217)
    &#34;&#34;&#34;

    # TODO: Document overlapping communities separately as it involves setting several other parameters

    def __init__(
        self,
        C: int = DEFAULT_C,
        structure: str = DEFAULT_STRUCTURE,
        avg_degree: float = DEFAULT_AVG_DEGREE,
        sparsify: bool = DEFAULT_SPARSIFY,
        overlapping: float = DEFAULT_OVERLAPPING,
        **kwargs,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        C : int
            Number of communities
        structure: str
            Structures for the affinity tensor `w`. It can be &#39;assortative&#39; or &#39;disassortative&#39;. 
            It can be a list to map structure for each layer in a multilayer graph.
        avg_degree : float
            Desired average degree for the network. It is not guaranteed that the 
            ultimate network will have that exact average degree value. 
            Try tweaking this parameter if you want to increase or decrease the 
            density of the network.
        sparsify : bool
            If True (default), enforce sparsity.
        overlapping : float
            Fraction of nodes with mixed membership. It has to be in `[0, 1)`.
        kwargs : 
            Additional arguments of BaseSyntheticNetwork.
        &#34;&#34;&#34;
        self.init_sbm_params(
            C=C,
            structure=structure,
            avg_degree=avg_degree,
            sparsify=sparsify,
            overlapping=overlapping,
            **kwargs,
        )
        self.build_Y()

    def init_sbm_params(self, **kwargs):
        &#34;&#34;&#34;
        Check SBM-specific parameters
        &#34;&#34;&#34;

        super().__init__(**kwargs)

        self.u = np.zeros((self.N, self.K), dtype=float)  # out-going membership
        self.v = np.zeros((self.N, self.K), dtype=float)  # in-going membership

        if &#34;C&#34; in kwargs:
            self.C = kwargs[&#34;C&#34;]  # number of communities
        else:
            msg = &#34;C parameter was not set. Defaulting to C=%d&#34; % DEFAULT_C
            module_logger.warning(msg)
            self.C = DEFAULT_C

        if &#34;avg_degree&#34; in kwargs:
            avg_degree = kwargs[&#34;avg_degree&#34;]
        else:
            msg = &#34;avg_degree parameter was not set. Defaulting to avg_degree=%d&#34; % DEFAULT_AVG_DEGREE
            module_logger.warning(msg)
            avg_degree = DEFAULT_AVG_DEGREE
        self.avg_degree = avg_degree

        if &#34;sparsify&#34; in kwargs:
            sparsify = kwargs[&#34;sparsify&#34;]
        else:
            msg = &#34;sparsify parameter was not set. Defaulting to sparsify=False&#34;
            module_logger.warning(msg)
            sparsify = False
        self.sparsify = sparsify

        &#34;&#34;&#34;
        SETUP overlapping communities
        &#34;&#34;&#34;
        if &#34;overlapping&#34; in kwargs:
            overlapping = kwargs[&#34;overlapping&#34;]
            # fraction of nodes with mixed membership
            if (overlapping &lt; 0) or (overlapping &gt; 1):
                err_msg = &#34;The overlapping parameter has to be in [0, 1]!&#34;
                raise ValueError(err_msg)
        else:
            overlapping = False
        self.overlapping = overlapping
        if self.overlapping:

            if &#34;corr&#34; in kwargs:
                # correlation between u and v synthetically generated
                if (kwargs[&#34;corr&#34;] &lt; 0) or (kwargs[&#34;corr&#34;] &gt; 1):
                    msg = &#34;The correlation parameter corr has to be in [0, 1]!&#34;
                    raise ValueError(msg)

                corr = float(kwargs[&#34;corr&#34;])
            else:
                msg = &#34;corr parameter for overlapping communities was not set. Defaulting to corr=0.&#34;
                module_logger.warning(msg)
                corr = 0
            self.corr = corr

        if self.overlapping &gt; 0:
            if &#34;normalization&#34; in kwargs:
                self.normalization = bool(kwargs[&#34;normalization&#34;])
            else:
                msg = &#34;Normalization parameter was not set. Defaulting to normalization=False (Dirichlet overlapping communities)&#34;
                module_logger.warning(msg)
                self.normalization = False

            if self.normalization:
                if &#34;ag&#34; in kwargs:
                    self.ag = float(kwargs[&#34;ag&#34;])
                else:
                    msg = &#34;Parameter alpha for the Gamma distribution was not set. Defaulting to alpha=0.1&#34;
                    module_logger.warning(msg)
                    self.ag = 0.1

                if &#34;beta&#34; in kwargs:
                    self.beta = float(kwargs[&#34;beta&#34;])
                else:
                    msg = &#34;Parameter beta for the Gamma distribution was not set. Defaulting to beta=0.1&#34;

                    module_logger.warning(msg)
                    self.beta = 0.1
            else:
                if &#34;alpha&#34; in kwargs:
                    self.alpha = float(kwargs[&#34;alpha&#34;])
                else:
                    msg = (
                        &#34;Parameter alpha for the Dirichlet distribution was not set. Defaulting to alpha=0.1&#34;
                    )
                    module_logger.warning(msg)
                    self.alpha = 0.1

        &#34;&#34;&#34;
        SETUP informed structure
        &#34;&#34;&#34;
        if &#34;structure&#34; in kwargs:
            structure = kwargs[&#34;structure&#34;]
        else:
            structure = None

        if structure is None:
            structure = [&#34;assortative&#34;] * self.L
        elif type(structure) == str:
            if structure not in [&#34;assortative&#34;, &#34;disassortative&#34;]:
                msg = &#34;The available structures for the affinity tensor w are: assortative, disassortative!&#34;
                raise ValueError(msg)
            else:
                structure = [structure] * self.L
        elif len(structure) != self.L:  # list of structures of the affinity tensor w
            msg = (
                &#34;The parameter structure should be a list of length L. &#34;
                &#34;Each entry defines the structure of the corresponding layer!&#34;
            )
            raise ValueError(msg)
        for e in structure:
            if e not in [&#34;assortative&#34;, &#34;disassortative&#34;]:
                msg = (
                    &#34;The available structures for the affinity tensor w are: &#34; &#34;assortative, disassortative.!&#34;
                )
                raise ValueError(msg)
        self.structure = structure

    def __repr__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping})&#34;
        return return_str

    def __str__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping})&#34;
        return return_str

    def build_Y(self):
        &#34;&#34;&#34;
        Latent variables
        &#34;&#34;&#34;
        self.u, self.v, self.w = self.generate_lv()

        &#34;&#34;&#34;
        Generate Y
        &#34;&#34;&#34;
        M_Y = np.einsum(&#34;ik,jq-&gt;ijkq&#34;, self.u, self.v)
        M_Y = np.einsum(&#34;ijkq,akq-&gt;aij&#34;, M_Y, self.w)
        # sparsity parameter for Y
        if self.sparsify:
            # TODO: Explain rationale behind this particular formula
            c = (float(self.N) * self.avg_degree) / M_Y.sum()
            M_Y *= c
            self.w *= c

        Y = self.prng.poisson(M_Y)
        for l in range(self.L):
            np.fill_diagonal(Y[l], 0)
        Y[Y &gt; self.K - 1] = self.K - 1  # cut-off, max entry has to be equal to K - 1

        self.Y = preprocess(Y)

    def __sample_membership_vectors(self):
        &#34;&#34;&#34;
        Compute the NxK membership vectors u, v using a Dirichlet distribution.

        INPUT
        ----------
        prng: Numpy Random object
              Random number generator container.
        alpha : float
                Parameter for Dirichlet.
        N : int
            Number of nodes.
        C : int
            Number of communities.
        corr : float
               Correlation between u and v synthetically generated.
        over : float
               Fraction of nodes with mixed membership.

        OUTPUT
        -------
        u : Numpy array
            Matrix NxC of out-going membership vectors, positive element-wise.
            With unitary L1 norm computed row-wise.

        v : Numpy array
            Matrix NxC of in-coming membership vectors, positive element-wise.
            With unitary L1 norm computed row-wise.
        &#34;&#34;&#34;

        # Generate equal-size unmixed group membership
        size = int(self.N / self.C)
        u = np.zeros((self.N, self.C))
        v = np.zeros((self.N, self.C))
        for i in range(self.N):
            q = int(math.floor(float(i) / float(size)))
            if q == self.C:
                u[i:, self.C - 1] = 1.0
                v[i:, self.C - 1] = 1.0
            else:
                for j in range(q * size, q * size + size):
                    u[j, q] = 1.0
                    v[j, q] = 1.0

        return u, v

    def __normalize_nonzero_membership(self, u):
        &#34;&#34;&#34;
        Given a matrix, it returns the same matrix normalized by row.

        INPUT
        ----------
        u: Numpy array
           Numpy Matrix.

        OUTPUT
        -------
        The matrix normalized by row.
        &#34;&#34;&#34;

        den1 = u.sum(axis=1, keepdims=True)
        nzz = den1 == 0.0
        den1[nzz] = 1.0

        return u / den1

    def __compute_affinity_matrix(self, structure, a=0.1):
        &#34;&#34;&#34;
        Compute the CxC affinity matrix w with probabilities between and within groups.

        Parameters
        ----------
        structure : string
            Structure of the network.
        a : float
            Parameter for secondary probabilities.

        Returns
        -------
        p : Numpy array
            Array with probabilities between and within groups. Element (k,h)
            gives the density of edges going from the nodes of group k to nodes of group h.
        &#34;&#34;&#34;

        p1 = self.avg_degree * self.C / self.N

        if structure == &#34;assortative&#34;:
            p = p1 * a * np.ones((self.C, self.C))  # secondary-probabilities
            np.fill_diagonal(p, p1 * np.ones(self.C))  # primary-probabilities

        elif structure == &#34;disassortative&#34;:
            p = p1 * np.ones((self.C, self.C))  # primary-probabilities
            np.fill_diagonal(p, a * p1 * np.ones(self.C))  # secondary-probabilities

        return p

    def __apply_overlapping(self, u, v):
        overlapping = int(self.N * self.overlapping)  # number of nodes belonging to more communities
        ind_over = np.random.randint(len(u), size=overlapping)

        if not self.normalization:
            # u and v from a Dirichlet distribution
            u[ind_over] = self.prng.dirichlet(self.alpha * np.ones(self.C), overlapping)
            v[ind_over] = self.corr * u[ind_over] + (1.0 - self.corr) * self.prng.dirichlet(
                self.alpha * np.ones(self.C), overlapping
            )
            if self.corr == 1.0:
                assert np.allclose(u, v)
            if self.corr &gt; 0:
                v = self.__normalize_nonzero_membership(v)
        else:
            # u and v from a Gamma distribution
            u[ind_over] = self.prng.gamma(self.ag, 1.0 / self.beta, size=(overlapping, self.C))
            v[ind_over] = self.corr * self.u[ind_over] + (1.0 - self.corr) * self.prng.gamma(
                self.ag, 1.0 / self.beta, size=(overlapping, self.C)
            )
            u = self.__normalize_nonzero_membership(u)
            v = self.__normalize_nonzero_membership(v)

        return u, v

    def generate_lv(self):
        &#34;&#34;&#34;
        Generate latent variables for a Stochastic BlockModel, assuming network layers are independent.
        &#34;&#34;&#34;

        # Generate u, v for overlapping communities
        u, v = self.__sample_membership_vectors()

        if self.overlapping &gt; 0:
            u, v = self.__apply_overlapping(u, v)

        # Generate w
        w = np.zeros((self.L, self.C, self.C))
        for l in range(self.L):
            w[l, :, :] = self.__compute_affinity_matrix(self.structure[l])

        return u, v, w


class DegreeCorrectedSBM(StandardSBM):
    &#34;&#34;&#34;
    **Degree-corrected stochastic blockmodel.**

    A generative model that incorporates heterogeneous vertex degrees into stochastic blockmodels, improving the performance of the models for statistical inference of group structure.
    For more information about this model, see Karrer, B., &amp; Newman, M. E. (2011). _Stochastic blockmodels and community structure in networks_. Physical review E, 83(1), 016107.
    [DOI:10.1103/PhysRevE.83.016107](https://arxiv.org/pdf/1008.3926.pdf).
    &#34;&#34;&#34;
    def __init__(
            self, exp_in: float = DEFAULT_EXP_IN,
            exp_out: float = DEFAULT_EXP_OUT,
            **kwargs):
        &#34;&#34;&#34;
        Parameters
        ----------
        exp_in : float
            Exponent power law of in-degree distribution.
        exp_out : float
            Exponent power law of out-degree distribution.
        kwargs : 
            Additional arguments of `vimure.synthetic.StandardSBM` 
        &#34;&#34;&#34;

        self.exp_in = exp_in  # exponent power law distribution in degree
        self.exp_out = exp_out  # exponent power law distribution out degree

        # Initialize all other variables
        super().__init__(**kwargs)

    def generate_lv(self):
        &#34;&#34;&#34;
        Overwrite standard SBM model to add degree distribution
        &#34;&#34;&#34;

        u, v, w = super().generate_lv()

        # We add +1 to the degree distributions to avoid creating disconnected nodes
        self.d_in = np.array(
            [int(x) + 2 for x in nx.utils.powerlaw_sequence(self.N, exponent=self.exp_in, seed=self.seed)]
        )
        self.d_out = np.array(
            [int(x) + 1 for x in nx.utils.powerlaw_sequence(self.N, exponent=self.exp_out, seed=self.seed)]
        )

        u_hat = u * self.d_out[:, np.newaxis]
        v_hat = v * self.d_in[:, np.newaxis]

        return u_hat, v_hat, w

    def __repr__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;exp_in={self.exp_in}, exp_out={self.exp_out})&#34;
        return return_str

    def __str__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;exp_in={self.exp_in}, exp_out={self.exp_out})&#34;
        return return_str


class Multitensor(StandardSBM):
    &#34;&#34;&#34;
    **A generative model with reciprocity**

    A mathematically principled generative model for capturing both community and reciprocity patterns in directed networks.
    Adapted from Safdari H., Contisciani M. &amp; De Bacco C. (2021). Generative model for reciprocity and community detection in networks, Phys. Rev. Research 3, 023209.
    [DOI:10.1103/PhysRevResearch.3.023209](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.023209).
    
    Generate a directed, possibly weighted network by using the reciprocity generative model.
    Can be used to generate benchmarks for networks with reciprocity.
    Steps:
        1. Generate the latent variables.
        2. Extract `A_{ij}` entries (network edges) from a Poisson distribution; its mean depends on the latent variables.
        
    .. note:: Open Source code available at https://github.com/mcontisc/CRep and modified in accordance with its [license](https://github.com/mcontisc/CRep/blob/master/LICENSE).

    -------------------------------------------------------------------------------
    Copyright (c) 2020 Hadiseh Safdari, Martina Contisciani and Caterina De Bacco.
    &#34;&#34;&#34;

    def __init__(self, eta=DEFAULT_ETA, ExpM=None, **kwargs):
        &#34;&#34;&#34;       
        Parameters
        ----------
        eta : float
            Initial value for the reciprocity coefficient. Eta has to be in [0, 1).
        ExpM : int
            Expected number of edges
        kwargs : 
            Additional arguments of `vimure.synthetic.StandardSBM` 
        &#34;&#34;&#34;
        super().init_sbm_params(**kwargs)

        if eta &lt; 0 or eta &gt;= 1:
            msg = &#34;The reciprocity parameter eta has to be in [0, 1)!&#34;
            raise ValueError(msg)
        self.eta = eta
        if ExpM is None:  # expected number of edges
            self.ExpM = int(self.N * self.avg_degree / 2.0)
        else:
            self.ExpM = int(ExpM)
            self.avg_degree = 2 * self.ExpM / float(self.N)

        self.build_Y()

    def Exp_ija_matrix(self, u, v, w):
        &#34;&#34;&#34;
        Compute the mean lambda0_ij for all entries.

        Parameters
        ----------
        u : numpy.array
            Out-going membership matrix.
        v : numpy.array
            In-coming membership matrix.
        w : numpy.array
            Affinity matrix.
            
        Returns
        -------
        M : numpy.array
            Mean $\lambda^{0}_{ij}$ for all entries.
        &#34;&#34;&#34;

        M = np.einsum(&#34;ik,jq-&gt;ijkq&#34;, u, v)
        M = np.einsum(&#34;ijkq,kq-&gt;ij&#34;, M, w)

        return M

    def build_Y(self):
        &#34;&#34;&#34;
        Generate network layers G (and adjacency matrix A) using the latent variables,
        with the generative model `(A_{ij},A_{ji}) ~ P(A_{ij}|u,v,w,eta) P(A_{ji}|A_{ij},u,v,w,eta)`
        &#34;&#34;&#34;

        self.Y = np.zeros((self.L, self.N, self.N))

        self.u, self.v, self.w = self.generate_lv()

        for l in range(self.L):

            &#34;&#34;&#34;
            # TODO:Document this section
            &#34;&#34;&#34;

            M0 = self.Exp_ija_matrix(self.u, self.v, self.w[l])  # whose elements are lambda0_{ij}
            np.fill_diagonal(M0, 0)

            if self.sparsify:
                # constant to enforce sparsity
                c = (self.ExpM * (1.0 - self.eta)) / M0.sum()

            # whose elements are m_{ij}
            MM = (M0 + self.eta * transpose_ij(M0)) / (1.0 - self.eta * self.eta)
            Mt = transpose_ij(MM)
            MM0 = M0.copy()  # to be not influenced by c_lambda

            if self.sparsify:
                M0 *= c
                self.w *= c  # To allow reconstruction of the network from u, v, w

            # whose elements are lambda0_{ji}
            M0t = transpose_ij(M0)

            # whose elements are m_{ij}
            M = (M0 + self.eta * M0t) / (1.0 - self.eta * self.eta)
            np.fill_diagonal(M, 0)

            # expected reciprocity
            rw = self.eta + ((MM0 * Mt + self.eta * Mt ** 2).sum() / MM.sum())

            &#34;&#34;&#34;
            # TODO:Document this section
            &#34;&#34;&#34;

            G = nx.DiGraph()
            for i in range(self.N):
                G.add_node(i)

            counter = 0
            totM = 0

            for i in range(self.N):
                for j in range(i + 1, self.N):
                    r = self.prng.rand(1)[0]
                    if r &lt; 0.5:
                        A_ij = self.prng.poisson(M[i, j], 1)[0]  # draw A_ij from P(A_ij) = Poisson(m_ij)
                        if A_ij &gt; 0:
                            G.add_edge(i, j, weight=A_ij)
                        lambda_ji = M0[j, i] + self.eta * A_ij

                        # draw A_ji from P(A_ji|A_ij) = Poisson(lambda0_ji + eta*A_ij)
                        A_ji = self.prng.poisson(lambda_ji, 1)[0]
                        if A_ji &gt; 0:
                            G.add_edge(j, i, weight=A_ji)
                    else:
                        # draw A_ij from P(A_ij) = Poisson(m_ij)
                        A_ji = self.prng.poisson(M[j, i], 1)[0]
                        if A_ji &gt; 0:
                            G.add_edge(j, i, weight=A_ji)
                        lambda_ij = M0[i, j] + self.eta * A_ji

                        # draw A_ji from P(A_ji|A_ij) = Poisson(lambda0_ji + eta*A_ij)
                        A_ij = self.prng.poisson(lambda_ij, 1)[0]
                        if A_ij &gt; 0:
                            G.add_edge(i, j, weight=A_ij)
                    counter += 1
                    totM += A_ij + A_ji

            # number of connected components
            n_connected_comp = len(list(nx.weakly_connected_components(G)))
            if n_connected_comp &gt; 1:
                msg = f&#34;Multitensor has produced a network with {n_connected_comp} connected components. &#34;
                msg += &#34;You can try increasing avg_degree and/or running with different seeds &#34;
                msg += &#34;until you get a network with just a single giant component.&#34;
                module_logger.warning(msg)

            self.Y[l] = nx.to_numpy_array(G)
            # cut-off, max entry has to be equal to K - 1
            self.Y[self.Y &gt; self.K - 1] = self.K - 1

        self.Y = preprocess(self.Y)

    def __repr__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;, eta={self.eta}, ExpM={self.ExpM})&#34;
        return return_str

    def __str__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;, eta={self.eta}, ExpM={self.ExpM})&#34;
        return return_str


class HollandLaskeyLeinhardtModel(BaseSyntheticNetwork):
    def __init__(self, **kwargs):
        raise NotImplementedError


&#34;&#34;&#34;
FUNCTIONS TO GENERATE X (OBSERVED NETWORK) FROM A GENERATED GROUND TRUTH NETWORK, Y
&#34;&#34;&#34;


def build_self_reporter_mask(gt_network):
    &#34;&#34;&#34;
    Build the reporters&#39; mask in a way such that:

    - A reporter `m` can report ties in which she is ego:   `m --&gt; i`
    - A reporter `m` can report ties in which she is alter: `i --&gt; m`
    - A reporter `m` **cannot** report ties she is not involved, that is `i --&gt; j` where `i != m` and `j != m`
    
    Parameters
    ----------
    gt_network : vimure.synthetic.BaseSyntheticNetwork
        Generative ground truth model.
    &#34;&#34;&#34;

    # TODO: Use sparse matrices instead

    R = np.zeros((gt_network.L, gt_network.N, gt_network.N, gt_network.M))
    R[:, np.arange(gt_network.M), :, np.arange(gt_network.M)] = 1
    R[:, :, np.arange(gt_network.M), np.arange(gt_network.M)] = 1

    return R


def build_custom_theta(
    gt_network: BaseSyntheticNetwork,
    theta_ratio: float = 0.5,
    exaggeration_type: str = &#34;over&#34;,
    seed: int = None,
):
    &#34;&#34;&#34;
    Instead of the regular generative model for `theta ~ Gamma(sh,sc)`,
    create a more extreme scenario where some percentage of reporters are exaggerating.

    Parameters
    ----------
    gt_network : vimure.synthetic.BaseSyntheticNetwork
        Generative ground truth model.
    theta_ratio : float
        Percentage of reporters who exaggerate.
    exaggeration_type : str
        (&#34;over&#34;, &#34;under&#34;)
    seed : int
        If not set, use gt_network.prng instead.

    Returns
    ----------
    theta : numpy.array
        A L x M matrix for theta.

    &#34;&#34;&#34;

    if theta_ratio &lt; 0 or theta_ratio &gt; 1:
        raise ValueError(&#34;theta_ratio should be in the interval [0, 1]&#34;)

    if exaggeration_type not in [&#34;over&#34;, &#34;under&#34;]:
        raise ValueError(&#34;Unrecognised exaggeration_type: %s&#34; % exaggeration_type)

    prng = np.random.RandomState(seed)

    nodes = np.arange(gt_network.M)
    theta = np.ones((gt_network.L, gt_network.M))

    # Number of &#34;unreliable&#34; reporters
    N_exa = int(gt_network.M * theta_ratio)
    selected_reporters = prng.choice(nodes, size=N_exa, replace=False)
    if N_exa &gt; 0.0:
        if exaggeration_type == &#34;under&#34;:
            theta[:, selected_reporters] = 0.50 * np.ones((gt_network.L, N_exa))
        else:
            theta[:, selected_reporters] = 50.0 * np.ones((gt_network.L, N_exa))
    return theta</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="vimure.synthetic.build_custom_theta"><code class="name flex">
<span>def <span class="ident">build_custom_theta</span></span>(<span>gt_network: <a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a>, theta_ratio: float = 0.5, exaggeration_type: str = 'over', seed: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Instead of the regular generative model for <code>theta ~ Gamma(sh,sc)</code>,
create a more extreme scenario where some percentage of reporters are exaggerating.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gt_network</code></strong> :&ensp;<code><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></code></dt>
<dd>Generative ground truth model.</dd>
<dt><strong><code>theta_ratio</code></strong> :&ensp;<code>float</code></dt>
<dd>Percentage of reporters who exaggerate.</dd>
<dt><strong><code>exaggeration_type</code></strong> :&ensp;<code>str</code></dt>
<dd>("over", "under")</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>If not set, use gt_network.prng instead.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>theta</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>A L x M matrix for theta.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_custom_theta(
    gt_network: BaseSyntheticNetwork,
    theta_ratio: float = 0.5,
    exaggeration_type: str = &#34;over&#34;,
    seed: int = None,
):
    &#34;&#34;&#34;
    Instead of the regular generative model for `theta ~ Gamma(sh,sc)`,
    create a more extreme scenario where some percentage of reporters are exaggerating.

    Parameters
    ----------
    gt_network : vimure.synthetic.BaseSyntheticNetwork
        Generative ground truth model.
    theta_ratio : float
        Percentage of reporters who exaggerate.
    exaggeration_type : str
        (&#34;over&#34;, &#34;under&#34;)
    seed : int
        If not set, use gt_network.prng instead.

    Returns
    ----------
    theta : numpy.array
        A L x M matrix for theta.

    &#34;&#34;&#34;

    if theta_ratio &lt; 0 or theta_ratio &gt; 1:
        raise ValueError(&#34;theta_ratio should be in the interval [0, 1]&#34;)

    if exaggeration_type not in [&#34;over&#34;, &#34;under&#34;]:
        raise ValueError(&#34;Unrecognised exaggeration_type: %s&#34; % exaggeration_type)

    prng = np.random.RandomState(seed)

    nodes = np.arange(gt_network.M)
    theta = np.ones((gt_network.L, gt_network.M))

    # Number of &#34;unreliable&#34; reporters
    N_exa = int(gt_network.M * theta_ratio)
    selected_reporters = prng.choice(nodes, size=N_exa, replace=False)
    if N_exa &gt; 0.0:
        if exaggeration_type == &#34;under&#34;:
            theta[:, selected_reporters] = 0.50 * np.ones((gt_network.L, N_exa))
        else:
            theta[:, selected_reporters] = 50.0 * np.ones((gt_network.L, N_exa))
    return theta</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.build_self_reporter_mask"><code class="name flex">
<span>def <span class="ident">build_self_reporter_mask</span></span>(<span>gt_network)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the reporters' mask in a way such that:</p>
<ul>
<li>A reporter <code>m</code> can report ties in which she is ego:
<code>m --&gt; i</code></li>
<li>A reporter <code>m</code> can report ties in which she is alter: <code>i --&gt; m</code></li>
<li>A reporter <code>m</code> <strong>cannot</strong> report ties she is not involved, that is <code>i --&gt; j</code> where <code>i != m</code> and <code>j != m</code></li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>gt_network</code></strong> :&ensp;<code><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></code></dt>
<dd>Generative ground truth model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_self_reporter_mask(gt_network):
    &#34;&#34;&#34;
    Build the reporters&#39; mask in a way such that:

    - A reporter `m` can report ties in which she is ego:   `m --&gt; i`
    - A reporter `m` can report ties in which she is alter: `i --&gt; m`
    - A reporter `m` **cannot** report ties she is not involved, that is `i --&gt; j` where `i != m` and `j != m`
    
    Parameters
    ----------
    gt_network : vimure.synthetic.BaseSyntheticNetwork
        Generative ground truth model.
    &#34;&#34;&#34;

    # TODO: Use sparse matrices instead

    R = np.zeros((gt_network.L, gt_network.N, gt_network.N, gt_network.M))
    R[:, np.arange(gt_network.M), :, np.arange(gt_network.M)] = 1
    R[:, :, np.arange(gt_network.M), np.arange(gt_network.M)] = 1

    return R</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.transpose_ij"><code class="name flex">
<span>def <span class="ident">transpose_ij</span></span>(<span>M)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the transpose of a matrix.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Numpy matrix.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Transpose of the matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose_ij(M):
    &#34;&#34;&#34;
    Compute the transpose of a matrix.

    Parameters
    ----------
    M : numpy.array
        Numpy matrix.

    Returns
    -------
    Transpose of the matrix.
    &#34;&#34;&#34;

    return np.einsum(&#34;ij-&gt;ji&#34;, M)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vimure.synthetic.BaseSyntheticNetwork"><code class="flex name class">
<span>class <span class="ident">BaseSyntheticNetwork</span></span>
<span>(</span><span>N: int = 100, M: int = 100, L: int = 1, K: int = 2, seed: int = 10, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A base abstract class for generation and management of synthetic networks.
Suitable for representing any type of synthetic network (whether SBM or not).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of nodes.</dd>
<dt><strong><code>M</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of reporters.</dd>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of layers.</dd>
<dt><strong><code>K</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum edge weight in the adjacency matrix.
When <code>K=2</code>, the adjacency matrix will contain some <code>Y_{ij}=0</code> and <code>Y_{ij}=1</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Pseudo random generator seed to use.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseSyntheticNetwork(BaseNetwork, metaclass=ABCMeta):
    &#34;&#34;&#34;
    A base abstract class for generation and management of synthetic networks. 
    Suitable for representing any type of synthetic network (whether SBM or not).
    &#34;&#34;&#34;
    def __init__(
        self,
        N: int = DEFAULT_N,
        M: int = DEFAULT_M,
        L: int = DEFAULT_L,
        K: int = DEFAULT_K,
        seed: int = DEFAULT_SEED,
        **kwargs,
    ):
        super().__init__(N=N, M=M, L=L, K=K, seed=seed, **kwargs)

    @abstractmethod
    def generate_lv(self):
        pass

    @abstractmethod
    def build_Y(self):
        pass

    def build_X(
        self,
        mutuality: float = 0.5,
        sh_theta: float = 2.0,
        sc_theta: float = 0.5,
        flag_self_reporter: bool = True,
        cutoff_X: bool = False,
        lambda_diff: float = None,
        Q: int = None,
        seed: int = None,
        theta: np.ndarray = None,
        verbose: bool = True,
    ):
        &#34;&#34;&#34;
        Any object inhereted from BaseSyntheticNetwork will have a ground truth network Y.
        Given that Y, generate the observed network X.
        
        Parameters
        ----------

        mutuality : float
            The mutuality parameter from 0 to 1.
        sh_theta : float
            Shape of gamma distribution from which to draw theta. 
            The &#39;reliability&#39; of nodes is represented by the parameter `theta_{lm}`
            and by default are modelled as a gamma function with shape `sh_theta` and scale `sc_theta`.
        sc_theta : float
            Scale of gamma distribution from which to draw theta.
        flag_self_reporter : bool
            Indicates whether a node can only report about their own ties.
        Q : int
            Maximum value of X entries. If None, it will use the network&#39;s K parameter.
        cutoff_X : bool
            Whether to set X as a binary.
        lambda_diff : float
            The difference between each subsequent K.
        seed : int
            Pseudo random generator seed to use.
        verbose : bool
            Provides additional details.

        Returns
        -------
        X : sktensor
            Observed network.
        &#34;&#34;&#34;

        logger = setup_logging(&#34;vm.synthetic.generate_X&#34;, verbose)

        if mutuality &lt; 0 or mutuality &gt;= 1:
            msg = &#34;The mutuality parameter has to be in [0, 1)!&#34;
            raise ValueError(msg)

        # generate_x uses its own pseudo-random seed generator, so we can better control the generation of theta.
        # We can generate the same theta, while varying mutuality (to check how X change);
        if seed is None:
            seed = self.seed
        prng = np.random.RandomState(seed)

        Y = self.Y
        Y_subs = Y.subs
        N = self.N
        M = self.M
        L = self.L
        K = self.K

        if theta is not None:
            warn_msg = &#34;Ignoring sh_theta and sc_theta since a full theta matrix was informed&#34;
            logger.debug(warn_msg)

            if type(theta) != np.ndarray or theta.shape != (L, M):
                msg = &#34;theta matrix is not valid. When using this parameter, make sure to inform a %d x %d matrix.&#34;
                raise ValueError(msg % (L, M))
        else:
            # Generate theta (reliability)
            theta = prng.gamma(shape=sh_theta, scale=sc_theta, size=(L, M))

        LAMBDA_0 = 0.01
        # Generate theta (ties average interactions)
        lambda_k = np.ones(shape=Y.shape).astype(&#34;float&#34;) * LAMBDA_0

        if lambda_diff is not None:
            if lambda_diff &lt;= 0:
                msg = &#34;lambda_diff is optional but when set should be higher than 0!&#34;
                raise ValueError(msg)

            for k in range(1, K):
                vals_k = np.argwhere(Y.vals == k).flatten()
                lij = (Y_subs[0][vals_k], Y_subs[1][vals_k], Y_subs[2][vals_k])
                lambda_k[lij] = LAMBDA_0 + lambda_diff
        else:
            for k in range(1, K):
                vals_k = np.argwhere(Y.vals == k).flatten()
                lij = (Y_subs[0][vals_k], Y_subs[1][vals_k], Y_subs[2][vals_k])
                lambda_k[lij] = k

        M_X = np.einsum(&#34;lm,lij-&gt;lijm&#34;, theta, lambda_k)
        MM = (M_X + mutuality * np.transpose(M_X, axes=(0, 2, 1, 3))) / (1.0 - mutuality * mutuality)

        X = np.zeros_like(MM).astype(&#34;int&#34;)

        if cutoff_X and Q is None:
            Q = self.K

        if flag_self_reporter:
            R = build_self_reporter_mask(self)

            for l in range(L):
                for m in range(M):
                    subs_nz = np.where(R[l, :, :, m] &gt; 0)
                    len_subs_nz = subs_nz[0].shape[0]
                    for n in range(len_subs_nz):
                        i, j = subs_nz[0][n], subs_nz[1][n]
                        r = prng.rand(1)[0]

                        # for those reporters that report perfectly, i.e. theta=1, do not extract from a poisson.
                        # Rather, assign the X deterministically using the mean of the poisson
                        if np.allclose(theta[l, m], 1.0) == False:
                            if r &lt; 0.5:
                                X[l, i, j, m] = prng.poisson(MM[l, i, j, m] * R[l, i, j, m])
                                if cutoff_X:
                                    if X[l, i, j, m] &gt; Q - 1:
                                        X[l, i, j, m] = Q - 1
                                cond_exp = M_X[l, j, i, m] * R[l, i, j, m] + mutuality * X[l, i, j, m]
                                X[l, j, i, m] = prng.poisson(cond_exp)
                            else:
                                X[l, j, i, m] = prng.poisson(MM[l, j, i, m] * R[l, j, i, m])
                                if cutoff_X:
                                    if X[l, j, i, m] &gt; Q - 1:
                                        X[l, j, i, m] = Q - 1
                                cond_exp = M_X[l, i, j, m] * R[l, j, i, m] + mutuality * X[l, j, i, m]
                                X[l, i, j, m] = prng.poisson(cond_exp)
                        else:
                            if r &lt; 0.5:
                                X[l, i, j, m] = MM[l, i, j, m] * R[l, i, j, m]
                                if cutoff_X == True:
                                    if X[l, i, j, m] &gt; Q - 1:
                                        X[l, i, j, m] = Q - 1
                                cond_exp = M_X[l, j, i, m] * R[l, i, j, m] + mutuality * X[l, i, j, m]
                                X[l, j, i, m] = cond_exp
                            else:
                                X[l, j, i, m] = MM[l, j, i, m] * R[l, j, i, m]
                                if cutoff_X == True:
                                    if X[l, j, i, m] &gt; Q - 1:
                                        X[l, j, i, m] = Q - 1
                                cond_exp = M_X[l, i, j, m] * R[l, j, i, m] + mutuality * X[l, j, i, m]
                                X[l, i, j, m] = cond_exp

        else:
            R = np.ones((L, N, N, M))
            for l in range(L):
                for m in range(M):
                    for i in range(N):
                        for j in range(i + 1, N):
                            r = prng.rand(1)[0]
                            if r &lt; 0.5:
                                X[l, i, j, m] = prng.poisson(MM[l, i, j, m])
                                if cutoff_X:
                                    if X[l, i, j, m] &gt; Q - 1:
                                        X[l, i, j, m] = Q - 1
                                cond_exp = M_X[l, j, i, m] + mutuality * X[l, i, j, m]
                                X[l, j, i, m] = prng.poisson(cond_exp)
                            else:
                                X[l, j, i, m] = prng.poisson(MM[l, j, i, m])
                                if cutoff_X:
                                    if X[l, j, i, m] &gt; Q - 1:
                                        X[l, j, i, m] = Q - 1
                                cond_exp = M_X[l, i, j, m] + mutuality * X[l, j, i, m]
                                X[l, i, j, m] = prng.poisson(cond_exp)

        if cutoff_X:
            X[X &gt; Q - 1] = Q - 1  # cut-off, max entry has to be equal to K - 1

        self.X = preprocess(X)
        self.R = preprocess(R)
        self.theta = theta
        self.lambda_k = lambda_k
        self.mutuality = mutuality

        subs_lijm = self.X.subs

        &#34;&#34;&#34;
        BASELINE: UNION

        This is one of the simplest possible ways of collating all networks reported by independent reporters (X) 
        into a single adjacency matrix (Y).

        The baseline union we use here takes the union of all ties (l, i, j) that were reported at least once by someone
        regardless of who reported it or the strength given to each tie.

        Note: the output adjacency matrix is binary

        # TODO: Maybe the calculation of baseline adjacency matrices shouldn&#39;t be inside the build_X() function, 
        #       since it isn&#39;t a feature of the package itself?
        &#34;&#34;&#34;
        lij = (subs_lijm[0], subs_lijm[1], subs_lijm[2])
        all_ties = np.stack(lij).T  # dim ((lij)_subs, 3)
        union_ties, count_ties = np.unique(all_ties, axis=0, return_counts=True)

        # Convert (l,i,j) to format understood by sktensor
        union_subs = tuple(np.moveaxis(union_ties, 1, 0))
        X_union = skt.sptensor(
            subs=union_subs,
            vals=np.ones(union_ties.shape[0]),
            shape=(self.L, self.N, self.N),
            dtype=np.int8,
        )
        self.X_union = X_union

        &#34;&#34;&#34;
        BASELINE: INTERSECTION

        The intersection is another simple way to combine all networks reported by independent reporters (X) 
        into a single adjacency matrix (Y).

        Here, a tie (l, i, j) is only considered if all reporters **who were allowed to report on that tie, 
        as indicated by the reporter&#39;s mask R,** reported this tie.

        Similar to the union baseline, here we disregard the strength of ties. 

        # TODO: Maybe the calculation of baseline adjacency matrices shouldn&#39;t be inside the build_X() function, 
        #       since it isn&#39;t a feature of the package itself?
        &#34;&#34;&#34;

        &#34;&#34;&#34;
        Intersection Baseline | Step 1
        
        First create a skt.sptensor called max_reports_lij to hold the maximum number of reports a tie (l, i, j) could receive
        &#34;&#34;&#34;
        if isinstance(R, skt.dtensor) or isinstance(R, np.ndarray):
            # Since R is a dense tensor of dimensions (l, i, j, m), we just need to sum over the m dimension
            max_reports_lij = sptensor_from_dense_array(R.sum(axis=3))

            df_max_reports_lij = pd.DataFrame(max_reports_lij.subs).T
            df_max_reports_lij.columns = [&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]
            df_max_reports_lij[&#34;max&#34;] = max_reports_lij.vals
        elif isinstance(R, skt.sptensor):
            # Because R is a sparse tensor, we need to work out the sum another way.
            
            # When performing operations with skt.sptensor, I (@jonjoncardoso) find it more intuitive -- and faster -- 
            # to work with pandas groupby -&gt; apply() than manipulating their subs+vals with numpy directly.
            R_vals_df = pd.DataFrame(np.stack(self.R.subs).T, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;, &#34;m&#34;])

            df_max_reports_lij = R_vals_df.groupby([&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])\
                .apply(lambda x: pd.Series({&#34;max&#34;: sum([R[l, i, j, m]
                                                   if type(R[l, i, j, m]) == int else R[l, i, j, m][0] 
                                                   for m in range(self.M)])}))


        &#34;&#34;&#34;
        Intersection Baseline | Step 2

        Convert `union_ties` to pd.DataFrame
        &#34;&#34;&#34;
        df_union_ties = pd.DataFrame(union_ties, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])
        df_union_ties[&#34;count&#34;] = count_ties

        &#34;&#34;&#34;
        Intersection Baseline | Step 3

        Find out how many, of the union ties, were unanimous. 
        That is: all reporters **that could report** on the tie (l, i, j) did agree that this tie existed and have reported it.

        NOTE: When performing operations with skt.sptensor, I (@jonjoncardoso) find it more intuitive -- and faster -- 
              to work with pandas groupby -&gt; apply() or pandas merge than manipulating their subs+vals with numpy directly.
        &#34;&#34;&#34;
        aux_df = pd.merge(df_union_ties, df_max_reports_lij, how=&#34;left&#34;)
        # Results DataFrame has columns: [l, i, j, count, max]
        df_intersection_ties = aux_df[aux_df[&#34;count&#34;] == aux_df[&#34;max&#34;]]
        
        &#34;&#34;&#34;
        Intersection Baseline | Step 4

        Create a skt.sptensor to represent the intersection of X ties. 
        &#34;&#34;&#34;

        if df_intersection_ties.empty:
            # If NO REPORTER has agreed on any ties, than intersection is empty
            self.X_intersection = None
        else:
            self.X_intersection = skt.sptensor(
                subs=tuple(df_intersection_ties[[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]].T.values.tolist()),
                vals=np.ones(df_intersection_ties.shape[0]).tolist(),
                shape=(self.L, self.N, self.N),
                dtype=np.int8,
            )          

        # TODO: Rethink the baseline union &amp; intersections for the case that X is not a binary matrix (or even if Y is binary)

        return self

    def __repr__(self):
        return f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, K={self.K}, seed={self.seed})&#34;

    def __str__(self):
        return f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, K={self.K}, seed={self.seed})&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vimure.io.BaseNetwork" href="io.html#vimure.io.BaseNetwork">BaseNetwork</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="vimure.synthetic.HollandLaskeyLeinhardtModel" href="#vimure.synthetic.HollandLaskeyLeinhardtModel">HollandLaskeyLeinhardtModel</a></li>
<li><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vimure.synthetic.BaseSyntheticNetwork.build_X"><code class="name flex">
<span>def <span class="ident">build_X</span></span>(<span>self, mutuality: float = 0.5, sh_theta: float = 2.0, sc_theta: float = 0.5, flag_self_reporter: bool = True, cutoff_X: bool = False, lambda_diff: float = None, Q: int = None, seed: int = None, theta: numpy.ndarray = None, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Any object inhereted from BaseSyntheticNetwork will have a ground truth network Y.
Given that Y, generate the observed network X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mutuality</code></strong> :&ensp;<code>float</code></dt>
<dd>The mutuality parameter from 0 to 1.</dd>
<dt><strong><code>sh_theta</code></strong> :&ensp;<code>float</code></dt>
<dd>Shape of gamma distribution from which to draw theta.
The 'reliability' of nodes is represented by the parameter <code>theta_{lm}</code>
and by default are modelled as a gamma function with shape <code>sh_theta</code> and scale <code>sc_theta</code>.</dd>
<dt><strong><code>sc_theta</code></strong> :&ensp;<code>float</code></dt>
<dd>Scale of gamma distribution from which to draw theta.</dd>
<dt><strong><code>flag_self_reporter</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether a node can only report about their own ties.</dd>
<dt><strong><code>Q</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum value of X entries. If None, it will use the network's K parameter.</dd>
<dt><strong><code>cutoff_X</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to set X as a binary.</dd>
<dt><strong><code>lambda_diff</code></strong> :&ensp;<code>float</code></dt>
<dd>The difference between each subsequent K.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Pseudo random generator seed to use.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Provides additional details.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>sktensor</code></dt>
<dd>Observed network.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_X(
    self,
    mutuality: float = 0.5,
    sh_theta: float = 2.0,
    sc_theta: float = 0.5,
    flag_self_reporter: bool = True,
    cutoff_X: bool = False,
    lambda_diff: float = None,
    Q: int = None,
    seed: int = None,
    theta: np.ndarray = None,
    verbose: bool = True,
):
    &#34;&#34;&#34;
    Any object inhereted from BaseSyntheticNetwork will have a ground truth network Y.
    Given that Y, generate the observed network X.
    
    Parameters
    ----------

    mutuality : float
        The mutuality parameter from 0 to 1.
    sh_theta : float
        Shape of gamma distribution from which to draw theta. 
        The &#39;reliability&#39; of nodes is represented by the parameter `theta_{lm}`
        and by default are modelled as a gamma function with shape `sh_theta` and scale `sc_theta`.
    sc_theta : float
        Scale of gamma distribution from which to draw theta.
    flag_self_reporter : bool
        Indicates whether a node can only report about their own ties.
    Q : int
        Maximum value of X entries. If None, it will use the network&#39;s K parameter.
    cutoff_X : bool
        Whether to set X as a binary.
    lambda_diff : float
        The difference between each subsequent K.
    seed : int
        Pseudo random generator seed to use.
    verbose : bool
        Provides additional details.

    Returns
    -------
    X : sktensor
        Observed network.
    &#34;&#34;&#34;

    logger = setup_logging(&#34;vm.synthetic.generate_X&#34;, verbose)

    if mutuality &lt; 0 or mutuality &gt;= 1:
        msg = &#34;The mutuality parameter has to be in [0, 1)!&#34;
        raise ValueError(msg)

    # generate_x uses its own pseudo-random seed generator, so we can better control the generation of theta.
    # We can generate the same theta, while varying mutuality (to check how X change);
    if seed is None:
        seed = self.seed
    prng = np.random.RandomState(seed)

    Y = self.Y
    Y_subs = Y.subs
    N = self.N
    M = self.M
    L = self.L
    K = self.K

    if theta is not None:
        warn_msg = &#34;Ignoring sh_theta and sc_theta since a full theta matrix was informed&#34;
        logger.debug(warn_msg)

        if type(theta) != np.ndarray or theta.shape != (L, M):
            msg = &#34;theta matrix is not valid. When using this parameter, make sure to inform a %d x %d matrix.&#34;
            raise ValueError(msg % (L, M))
    else:
        # Generate theta (reliability)
        theta = prng.gamma(shape=sh_theta, scale=sc_theta, size=(L, M))

    LAMBDA_0 = 0.01
    # Generate theta (ties average interactions)
    lambda_k = np.ones(shape=Y.shape).astype(&#34;float&#34;) * LAMBDA_0

    if lambda_diff is not None:
        if lambda_diff &lt;= 0:
            msg = &#34;lambda_diff is optional but when set should be higher than 0!&#34;
            raise ValueError(msg)

        for k in range(1, K):
            vals_k = np.argwhere(Y.vals == k).flatten()
            lij = (Y_subs[0][vals_k], Y_subs[1][vals_k], Y_subs[2][vals_k])
            lambda_k[lij] = LAMBDA_0 + lambda_diff
    else:
        for k in range(1, K):
            vals_k = np.argwhere(Y.vals == k).flatten()
            lij = (Y_subs[0][vals_k], Y_subs[1][vals_k], Y_subs[2][vals_k])
            lambda_k[lij] = k

    M_X = np.einsum(&#34;lm,lij-&gt;lijm&#34;, theta, lambda_k)
    MM = (M_X + mutuality * np.transpose(M_X, axes=(0, 2, 1, 3))) / (1.0 - mutuality * mutuality)

    X = np.zeros_like(MM).astype(&#34;int&#34;)

    if cutoff_X and Q is None:
        Q = self.K

    if flag_self_reporter:
        R = build_self_reporter_mask(self)

        for l in range(L):
            for m in range(M):
                subs_nz = np.where(R[l, :, :, m] &gt; 0)
                len_subs_nz = subs_nz[0].shape[0]
                for n in range(len_subs_nz):
                    i, j = subs_nz[0][n], subs_nz[1][n]
                    r = prng.rand(1)[0]

                    # for those reporters that report perfectly, i.e. theta=1, do not extract from a poisson.
                    # Rather, assign the X deterministically using the mean of the poisson
                    if np.allclose(theta[l, m], 1.0) == False:
                        if r &lt; 0.5:
                            X[l, i, j, m] = prng.poisson(MM[l, i, j, m] * R[l, i, j, m])
                            if cutoff_X:
                                if X[l, i, j, m] &gt; Q - 1:
                                    X[l, i, j, m] = Q - 1
                            cond_exp = M_X[l, j, i, m] * R[l, i, j, m] + mutuality * X[l, i, j, m]
                            X[l, j, i, m] = prng.poisson(cond_exp)
                        else:
                            X[l, j, i, m] = prng.poisson(MM[l, j, i, m] * R[l, j, i, m])
                            if cutoff_X:
                                if X[l, j, i, m] &gt; Q - 1:
                                    X[l, j, i, m] = Q - 1
                            cond_exp = M_X[l, i, j, m] * R[l, j, i, m] + mutuality * X[l, j, i, m]
                            X[l, i, j, m] = prng.poisson(cond_exp)
                    else:
                        if r &lt; 0.5:
                            X[l, i, j, m] = MM[l, i, j, m] * R[l, i, j, m]
                            if cutoff_X == True:
                                if X[l, i, j, m] &gt; Q - 1:
                                    X[l, i, j, m] = Q - 1
                            cond_exp = M_X[l, j, i, m] * R[l, i, j, m] + mutuality * X[l, i, j, m]
                            X[l, j, i, m] = cond_exp
                        else:
                            X[l, j, i, m] = MM[l, j, i, m] * R[l, j, i, m]
                            if cutoff_X == True:
                                if X[l, j, i, m] &gt; Q - 1:
                                    X[l, j, i, m] = Q - 1
                            cond_exp = M_X[l, i, j, m] * R[l, j, i, m] + mutuality * X[l, j, i, m]
                            X[l, i, j, m] = cond_exp

    else:
        R = np.ones((L, N, N, M))
        for l in range(L):
            for m in range(M):
                for i in range(N):
                    for j in range(i + 1, N):
                        r = prng.rand(1)[0]
                        if r &lt; 0.5:
                            X[l, i, j, m] = prng.poisson(MM[l, i, j, m])
                            if cutoff_X:
                                if X[l, i, j, m] &gt; Q - 1:
                                    X[l, i, j, m] = Q - 1
                            cond_exp = M_X[l, j, i, m] + mutuality * X[l, i, j, m]
                            X[l, j, i, m] = prng.poisson(cond_exp)
                        else:
                            X[l, j, i, m] = prng.poisson(MM[l, j, i, m])
                            if cutoff_X:
                                if X[l, j, i, m] &gt; Q - 1:
                                    X[l, j, i, m] = Q - 1
                            cond_exp = M_X[l, i, j, m] + mutuality * X[l, j, i, m]
                            X[l, i, j, m] = prng.poisson(cond_exp)

    if cutoff_X:
        X[X &gt; Q - 1] = Q - 1  # cut-off, max entry has to be equal to K - 1

    self.X = preprocess(X)
    self.R = preprocess(R)
    self.theta = theta
    self.lambda_k = lambda_k
    self.mutuality = mutuality

    subs_lijm = self.X.subs

    &#34;&#34;&#34;
    BASELINE: UNION

    This is one of the simplest possible ways of collating all networks reported by independent reporters (X) 
    into a single adjacency matrix (Y).

    The baseline union we use here takes the union of all ties (l, i, j) that were reported at least once by someone
    regardless of who reported it or the strength given to each tie.

    Note: the output adjacency matrix is binary

    # TODO: Maybe the calculation of baseline adjacency matrices shouldn&#39;t be inside the build_X() function, 
    #       since it isn&#39;t a feature of the package itself?
    &#34;&#34;&#34;
    lij = (subs_lijm[0], subs_lijm[1], subs_lijm[2])
    all_ties = np.stack(lij).T  # dim ((lij)_subs, 3)
    union_ties, count_ties = np.unique(all_ties, axis=0, return_counts=True)

    # Convert (l,i,j) to format understood by sktensor
    union_subs = tuple(np.moveaxis(union_ties, 1, 0))
    X_union = skt.sptensor(
        subs=union_subs,
        vals=np.ones(union_ties.shape[0]),
        shape=(self.L, self.N, self.N),
        dtype=np.int8,
    )
    self.X_union = X_union

    &#34;&#34;&#34;
    BASELINE: INTERSECTION

    The intersection is another simple way to combine all networks reported by independent reporters (X) 
    into a single adjacency matrix (Y).

    Here, a tie (l, i, j) is only considered if all reporters **who were allowed to report on that tie, 
    as indicated by the reporter&#39;s mask R,** reported this tie.

    Similar to the union baseline, here we disregard the strength of ties. 

    # TODO: Maybe the calculation of baseline adjacency matrices shouldn&#39;t be inside the build_X() function, 
    #       since it isn&#39;t a feature of the package itself?
    &#34;&#34;&#34;

    &#34;&#34;&#34;
    Intersection Baseline | Step 1
    
    First create a skt.sptensor called max_reports_lij to hold the maximum number of reports a tie (l, i, j) could receive
    &#34;&#34;&#34;
    if isinstance(R, skt.dtensor) or isinstance(R, np.ndarray):
        # Since R is a dense tensor of dimensions (l, i, j, m), we just need to sum over the m dimension
        max_reports_lij = sptensor_from_dense_array(R.sum(axis=3))

        df_max_reports_lij = pd.DataFrame(max_reports_lij.subs).T
        df_max_reports_lij.columns = [&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]
        df_max_reports_lij[&#34;max&#34;] = max_reports_lij.vals
    elif isinstance(R, skt.sptensor):
        # Because R is a sparse tensor, we need to work out the sum another way.
        
        # When performing operations with skt.sptensor, I (@jonjoncardoso) find it more intuitive -- and faster -- 
        # to work with pandas groupby -&gt; apply() than manipulating their subs+vals with numpy directly.
        R_vals_df = pd.DataFrame(np.stack(self.R.subs).T, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;, &#34;m&#34;])

        df_max_reports_lij = R_vals_df.groupby([&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])\
            .apply(lambda x: pd.Series({&#34;max&#34;: sum([R[l, i, j, m]
                                               if type(R[l, i, j, m]) == int else R[l, i, j, m][0] 
                                               for m in range(self.M)])}))


    &#34;&#34;&#34;
    Intersection Baseline | Step 2

    Convert `union_ties` to pd.DataFrame
    &#34;&#34;&#34;
    df_union_ties = pd.DataFrame(union_ties, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])
    df_union_ties[&#34;count&#34;] = count_ties

    &#34;&#34;&#34;
    Intersection Baseline | Step 3

    Find out how many, of the union ties, were unanimous. 
    That is: all reporters **that could report** on the tie (l, i, j) did agree that this tie existed and have reported it.

    NOTE: When performing operations with skt.sptensor, I (@jonjoncardoso) find it more intuitive -- and faster -- 
          to work with pandas groupby -&gt; apply() or pandas merge than manipulating their subs+vals with numpy directly.
    &#34;&#34;&#34;
    aux_df = pd.merge(df_union_ties, df_max_reports_lij, how=&#34;left&#34;)
    # Results DataFrame has columns: [l, i, j, count, max]
    df_intersection_ties = aux_df[aux_df[&#34;count&#34;] == aux_df[&#34;max&#34;]]
    
    &#34;&#34;&#34;
    Intersection Baseline | Step 4

    Create a skt.sptensor to represent the intersection of X ties. 
    &#34;&#34;&#34;

    if df_intersection_ties.empty:
        # If NO REPORTER has agreed on any ties, than intersection is empty
        self.X_intersection = None
    else:
        self.X_intersection = skt.sptensor(
            subs=tuple(df_intersection_ties[[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]].T.values.tolist()),
            vals=np.ones(df_intersection_ties.shape[0]).tolist(),
            shape=(self.L, self.N, self.N),
            dtype=np.int8,
        )          

    # TODO: Rethink the baseline union &amp; intersections for the case that X is not a binary matrix (or even if Y is binary)

    return self</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.BaseSyntheticNetwork.build_Y"><code class="name flex">
<span>def <span class="ident">build_Y</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def build_Y(self):
    pass</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.BaseSyntheticNetwork.generate_lv"><code class="name flex">
<span>def <span class="ident">generate_lv</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def generate_lv(self):
    pass</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vimure.io.BaseNetwork" href="io.html#vimure.io.BaseNetwork">BaseNetwork</a></b></code>:
<ul class="hlist">
<li><code><a title="vimure.io.BaseNetwork.get_layer" href="io.html#vimure.io.BaseNetwork.get_layer">get_layer</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="vimure.synthetic.DegreeCorrectedSBM"><code class="flex name class">
<span>class <span class="ident">DegreeCorrectedSBM</span></span>
<span>(</span><span>exp_in: float = 2, exp_out: float = 2.5, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><strong>Degree-corrected stochastic blockmodel.</strong></p>
<p>A generative model that incorporates heterogeneous vertex degrees into stochastic blockmodels, improving the performance of the models for statistical inference of group structure.
For more information about this model, see Karrer, B., &amp; Newman, M. E. (2011). <em>Stochastic blockmodels and community structure in networks</em>. Physical review E, 83(1), 016107.
<a href="https://arxiv.org/pdf/1008.3926.pdf">DOI:10.1103/PhysRevE.83.016107</a>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>exp_in</code></strong> :&ensp;<code>float</code></dt>
<dd>Exponent power law of in-degree distribution.</dd>
<dt><strong><code>exp_out</code></strong> :&ensp;<code>float</code></dt>
<dd>Exponent power law of out-degree distribution.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional arguments of <code><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DegreeCorrectedSBM(StandardSBM):
    &#34;&#34;&#34;
    **Degree-corrected stochastic blockmodel.**

    A generative model that incorporates heterogeneous vertex degrees into stochastic blockmodels, improving the performance of the models for statistical inference of group structure.
    For more information about this model, see Karrer, B., &amp; Newman, M. E. (2011). _Stochastic blockmodels and community structure in networks_. Physical review E, 83(1), 016107.
    [DOI:10.1103/PhysRevE.83.016107](https://arxiv.org/pdf/1008.3926.pdf).
    &#34;&#34;&#34;
    def __init__(
            self, exp_in: float = DEFAULT_EXP_IN,
            exp_out: float = DEFAULT_EXP_OUT,
            **kwargs):
        &#34;&#34;&#34;
        Parameters
        ----------
        exp_in : float
            Exponent power law of in-degree distribution.
        exp_out : float
            Exponent power law of out-degree distribution.
        kwargs : 
            Additional arguments of `vimure.synthetic.StandardSBM` 
        &#34;&#34;&#34;

        self.exp_in = exp_in  # exponent power law distribution in degree
        self.exp_out = exp_out  # exponent power law distribution out degree

        # Initialize all other variables
        super().__init__(**kwargs)

    def generate_lv(self):
        &#34;&#34;&#34;
        Overwrite standard SBM model to add degree distribution
        &#34;&#34;&#34;

        u, v, w = super().generate_lv()

        # We add +1 to the degree distributions to avoid creating disconnected nodes
        self.d_in = np.array(
            [int(x) + 2 for x in nx.utils.powerlaw_sequence(self.N, exponent=self.exp_in, seed=self.seed)]
        )
        self.d_out = np.array(
            [int(x) + 1 for x in nx.utils.powerlaw_sequence(self.N, exponent=self.exp_out, seed=self.seed)]
        )

        u_hat = u * self.d_out[:, np.newaxis]
        v_hat = v * self.d_in[:, np.newaxis]

        return u_hat, v_hat, w

    def __repr__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;exp_in={self.exp_in}, exp_out={self.exp_out})&#34;
        return return_str

    def __str__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;exp_in={self.exp_in}, exp_out={self.exp_out})&#34;
        return return_str</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></li>
<li><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></li>
<li><a title="vimure.io.BaseNetwork" href="io.html#vimure.io.BaseNetwork">BaseNetwork</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vimure.synthetic.DegreeCorrectedSBM.generate_lv"><code class="name flex">
<span>def <span class="ident">generate_lv</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Overwrite standard SBM model to add degree distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_lv(self):
    &#34;&#34;&#34;
    Overwrite standard SBM model to add degree distribution
    &#34;&#34;&#34;

    u, v, w = super().generate_lv()

    # We add +1 to the degree distributions to avoid creating disconnected nodes
    self.d_in = np.array(
        [int(x) + 2 for x in nx.utils.powerlaw_sequence(self.N, exponent=self.exp_in, seed=self.seed)]
    )
    self.d_out = np.array(
        [int(x) + 1 for x in nx.utils.powerlaw_sequence(self.N, exponent=self.exp_out, seed=self.seed)]
    )

    u_hat = u * self.d_out[:, np.newaxis]
    v_hat = v * self.d_in[:, np.newaxis]

    return u_hat, v_hat, w</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></b></code>:
<ul class="hlist">
<li><code><a title="vimure.synthetic.StandardSBM.build_X" href="#vimure.synthetic.BaseSyntheticNetwork.build_X">build_X</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.build_Y" href="#vimure.synthetic.StandardSBM.build_Y">build_Y</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.get_layer" href="io.html#vimure.io.BaseNetwork.get_layer">get_layer</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.init_sbm_params" href="#vimure.synthetic.StandardSBM.init_sbm_params">init_sbm_params</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="vimure.synthetic.HollandLaskeyLeinhardtModel"><code class="flex name class">
<span>class <span class="ident">HollandLaskeyLeinhardtModel</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A base abstract class for generation and management of synthetic networks.
Suitable for representing any type of synthetic network (whether SBM or not).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of nodes.</dd>
<dt><strong><code>M</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of reporters.</dd>
<dt><strong><code>L</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of layers.</dd>
<dt><strong><code>K</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum edge weight in the adjacency matrix.
When <code>K=2</code>, the adjacency matrix will contain some <code>Y_{ij}=0</code> and <code>Y_{ij}=1</code>.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Pseudo random generator seed to use.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HollandLaskeyLeinhardtModel(BaseSyntheticNetwork):
    def __init__(self, **kwargs):
        raise NotImplementedError</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></li>
<li><a title="vimure.io.BaseNetwork" href="io.html#vimure.io.BaseNetwork">BaseNetwork</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></b></code>:
<ul class="hlist">
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.build_X" href="#vimure.synthetic.BaseSyntheticNetwork.build_X">build_X</a></code></li>
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.get_layer" href="io.html#vimure.io.BaseNetwork.get_layer">get_layer</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="vimure.synthetic.Multitensor"><code class="flex name class">
<span>class <span class="ident">Multitensor</span></span>
<span>(</span><span>eta=0.5, ExpM=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><strong>A generative model with reciprocity</strong></p>
<p>A mathematically principled generative model for capturing both community and reciprocity patterns in directed networks.
Adapted from Safdari H., Contisciani M. &amp; De Bacco C. (2021). Generative model for reciprocity and community detection in networks, Phys. Rev. Research 3, 023209.
<a href="https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.023209">DOI:10.1103/PhysRevResearch.3.023209</a>.</p>
<p>Generate a directed, possibly weighted network by using the reciprocity generative model.
Can be used to generate benchmarks for networks with reciprocity.</p>
<h2 id="steps">Steps</h2>
<ol>
<li>Generate the latent variables.</li>
<li>Extract <code>A_{ij}</code> entries (network edges) from a Poisson distribution; its mean depends on the latent variables.</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;Open Source code available at <a href="https://github.com/mcontisc/CRep">https://github.com/mcontisc/CRep</a> and modified in accordance with its <a href="https://github.com/mcontisc/CRep/blob/master/LICENSE">license</a>.</p>
</div>
<hr>
<p>Copyright (c) 2020 Hadiseh Safdari, Martina Contisciani and Caterina De Bacco.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>eta</code></strong> :&ensp;<code>float</code></dt>
<dd>Initial value for the reciprocity coefficient. Eta has to be in [0, 1).</dd>
<dt><strong><code>ExpM</code></strong> :&ensp;<code>int</code></dt>
<dd>Expected number of edges</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional arguments of <code><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Multitensor(StandardSBM):
    &#34;&#34;&#34;
    **A generative model with reciprocity**

    A mathematically principled generative model for capturing both community and reciprocity patterns in directed networks.
    Adapted from Safdari H., Contisciani M. &amp; De Bacco C. (2021). Generative model for reciprocity and community detection in networks, Phys. Rev. Research 3, 023209.
    [DOI:10.1103/PhysRevResearch.3.023209](https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.3.023209).
    
    Generate a directed, possibly weighted network by using the reciprocity generative model.
    Can be used to generate benchmarks for networks with reciprocity.
    Steps:
        1. Generate the latent variables.
        2. Extract `A_{ij}` entries (network edges) from a Poisson distribution; its mean depends on the latent variables.
        
    .. note:: Open Source code available at https://github.com/mcontisc/CRep and modified in accordance with its [license](https://github.com/mcontisc/CRep/blob/master/LICENSE).

    -------------------------------------------------------------------------------
    Copyright (c) 2020 Hadiseh Safdari, Martina Contisciani and Caterina De Bacco.
    &#34;&#34;&#34;

    def __init__(self, eta=DEFAULT_ETA, ExpM=None, **kwargs):
        &#34;&#34;&#34;       
        Parameters
        ----------
        eta : float
            Initial value for the reciprocity coefficient. Eta has to be in [0, 1).
        ExpM : int
            Expected number of edges
        kwargs : 
            Additional arguments of `vimure.synthetic.StandardSBM` 
        &#34;&#34;&#34;
        super().init_sbm_params(**kwargs)

        if eta &lt; 0 or eta &gt;= 1:
            msg = &#34;The reciprocity parameter eta has to be in [0, 1)!&#34;
            raise ValueError(msg)
        self.eta = eta
        if ExpM is None:  # expected number of edges
            self.ExpM = int(self.N * self.avg_degree / 2.0)
        else:
            self.ExpM = int(ExpM)
            self.avg_degree = 2 * self.ExpM / float(self.N)

        self.build_Y()

    def Exp_ija_matrix(self, u, v, w):
        &#34;&#34;&#34;
        Compute the mean lambda0_ij for all entries.

        Parameters
        ----------
        u : numpy.array
            Out-going membership matrix.
        v : numpy.array
            In-coming membership matrix.
        w : numpy.array
            Affinity matrix.
            
        Returns
        -------
        M : numpy.array
            Mean $\lambda^{0}_{ij}$ for all entries.
        &#34;&#34;&#34;

        M = np.einsum(&#34;ik,jq-&gt;ijkq&#34;, u, v)
        M = np.einsum(&#34;ijkq,kq-&gt;ij&#34;, M, w)

        return M

    def build_Y(self):
        &#34;&#34;&#34;
        Generate network layers G (and adjacency matrix A) using the latent variables,
        with the generative model `(A_{ij},A_{ji}) ~ P(A_{ij}|u,v,w,eta) P(A_{ji}|A_{ij},u,v,w,eta)`
        &#34;&#34;&#34;

        self.Y = np.zeros((self.L, self.N, self.N))

        self.u, self.v, self.w = self.generate_lv()

        for l in range(self.L):

            &#34;&#34;&#34;
            # TODO:Document this section
            &#34;&#34;&#34;

            M0 = self.Exp_ija_matrix(self.u, self.v, self.w[l])  # whose elements are lambda0_{ij}
            np.fill_diagonal(M0, 0)

            if self.sparsify:
                # constant to enforce sparsity
                c = (self.ExpM * (1.0 - self.eta)) / M0.sum()

            # whose elements are m_{ij}
            MM = (M0 + self.eta * transpose_ij(M0)) / (1.0 - self.eta * self.eta)
            Mt = transpose_ij(MM)
            MM0 = M0.copy()  # to be not influenced by c_lambda

            if self.sparsify:
                M0 *= c
                self.w *= c  # To allow reconstruction of the network from u, v, w

            # whose elements are lambda0_{ji}
            M0t = transpose_ij(M0)

            # whose elements are m_{ij}
            M = (M0 + self.eta * M0t) / (1.0 - self.eta * self.eta)
            np.fill_diagonal(M, 0)

            # expected reciprocity
            rw = self.eta + ((MM0 * Mt + self.eta * Mt ** 2).sum() / MM.sum())

            &#34;&#34;&#34;
            # TODO:Document this section
            &#34;&#34;&#34;

            G = nx.DiGraph()
            for i in range(self.N):
                G.add_node(i)

            counter = 0
            totM = 0

            for i in range(self.N):
                for j in range(i + 1, self.N):
                    r = self.prng.rand(1)[0]
                    if r &lt; 0.5:
                        A_ij = self.prng.poisson(M[i, j], 1)[0]  # draw A_ij from P(A_ij) = Poisson(m_ij)
                        if A_ij &gt; 0:
                            G.add_edge(i, j, weight=A_ij)
                        lambda_ji = M0[j, i] + self.eta * A_ij

                        # draw A_ji from P(A_ji|A_ij) = Poisson(lambda0_ji + eta*A_ij)
                        A_ji = self.prng.poisson(lambda_ji, 1)[0]
                        if A_ji &gt; 0:
                            G.add_edge(j, i, weight=A_ji)
                    else:
                        # draw A_ij from P(A_ij) = Poisson(m_ij)
                        A_ji = self.prng.poisson(M[j, i], 1)[0]
                        if A_ji &gt; 0:
                            G.add_edge(j, i, weight=A_ji)
                        lambda_ij = M0[i, j] + self.eta * A_ji

                        # draw A_ji from P(A_ji|A_ij) = Poisson(lambda0_ji + eta*A_ij)
                        A_ij = self.prng.poisson(lambda_ij, 1)[0]
                        if A_ij &gt; 0:
                            G.add_edge(i, j, weight=A_ij)
                    counter += 1
                    totM += A_ij + A_ji

            # number of connected components
            n_connected_comp = len(list(nx.weakly_connected_components(G)))
            if n_connected_comp &gt; 1:
                msg = f&#34;Multitensor has produced a network with {n_connected_comp} connected components. &#34;
                msg += &#34;You can try increasing avg_degree and/or running with different seeds &#34;
                msg += &#34;until you get a network with just a single giant component.&#34;
                module_logger.warning(msg)

            self.Y[l] = nx.to_numpy_array(G)
            # cut-off, max entry has to be equal to K - 1
            self.Y[self.Y &gt; self.K - 1] = self.K - 1

        self.Y = preprocess(self.Y)

    def __repr__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;, eta={self.eta}, ExpM={self.ExpM})&#34;
        return return_str

    def __str__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping}, &#34;
        return_str += f&#34;, eta={self.eta}, ExpM={self.ExpM})&#34;
        return return_str</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></li>
<li><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></li>
<li><a title="vimure.io.BaseNetwork" href="io.html#vimure.io.BaseNetwork">BaseNetwork</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vimure.synthetic.Multitensor.Exp_ija_matrix"><code class="name flex">
<span>def <span class="ident">Exp_ija_matrix</span></span>(<span>self, u, v, w)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the mean lambda0_ij for all entries.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>u</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Out-going membership matrix.</dd>
<dt><strong><code>v</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>In-coming membership matrix.</dd>
<dt><strong><code>w</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Affinity matrix.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>Mean $\lambda^{0}_{ij}$ for all entries.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Exp_ija_matrix(self, u, v, w):
    &#34;&#34;&#34;
    Compute the mean lambda0_ij for all entries.

    Parameters
    ----------
    u : numpy.array
        Out-going membership matrix.
    v : numpy.array
        In-coming membership matrix.
    w : numpy.array
        Affinity matrix.
        
    Returns
    -------
    M : numpy.array
        Mean $\lambda^{0}_{ij}$ for all entries.
    &#34;&#34;&#34;

    M = np.einsum(&#34;ik,jq-&gt;ijkq&#34;, u, v)
    M = np.einsum(&#34;ijkq,kq-&gt;ij&#34;, M, w)

    return M</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.Multitensor.build_Y"><code class="name flex">
<span>def <span class="ident">build_Y</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate network layers G (and adjacency matrix A) using the latent variables,
with the generative model <code>(A_{ij},A_{ji}) ~ P(A_{ij}|u,v,w,eta) P(A_{ji}|A_{ij},u,v,w,eta)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_Y(self):
    &#34;&#34;&#34;
    Generate network layers G (and adjacency matrix A) using the latent variables,
    with the generative model `(A_{ij},A_{ji}) ~ P(A_{ij}|u,v,w,eta) P(A_{ji}|A_{ij},u,v,w,eta)`
    &#34;&#34;&#34;

    self.Y = np.zeros((self.L, self.N, self.N))

    self.u, self.v, self.w = self.generate_lv()

    for l in range(self.L):

        &#34;&#34;&#34;
        # TODO:Document this section
        &#34;&#34;&#34;

        M0 = self.Exp_ija_matrix(self.u, self.v, self.w[l])  # whose elements are lambda0_{ij}
        np.fill_diagonal(M0, 0)

        if self.sparsify:
            # constant to enforce sparsity
            c = (self.ExpM * (1.0 - self.eta)) / M0.sum()

        # whose elements are m_{ij}
        MM = (M0 + self.eta * transpose_ij(M0)) / (1.0 - self.eta * self.eta)
        Mt = transpose_ij(MM)
        MM0 = M0.copy()  # to be not influenced by c_lambda

        if self.sparsify:
            M0 *= c
            self.w *= c  # To allow reconstruction of the network from u, v, w

        # whose elements are lambda0_{ji}
        M0t = transpose_ij(M0)

        # whose elements are m_{ij}
        M = (M0 + self.eta * M0t) / (1.0 - self.eta * self.eta)
        np.fill_diagonal(M, 0)

        # expected reciprocity
        rw = self.eta + ((MM0 * Mt + self.eta * Mt ** 2).sum() / MM.sum())

        &#34;&#34;&#34;
        # TODO:Document this section
        &#34;&#34;&#34;

        G = nx.DiGraph()
        for i in range(self.N):
            G.add_node(i)

        counter = 0
        totM = 0

        for i in range(self.N):
            for j in range(i + 1, self.N):
                r = self.prng.rand(1)[0]
                if r &lt; 0.5:
                    A_ij = self.prng.poisson(M[i, j], 1)[0]  # draw A_ij from P(A_ij) = Poisson(m_ij)
                    if A_ij &gt; 0:
                        G.add_edge(i, j, weight=A_ij)
                    lambda_ji = M0[j, i] + self.eta * A_ij

                    # draw A_ji from P(A_ji|A_ij) = Poisson(lambda0_ji + eta*A_ij)
                    A_ji = self.prng.poisson(lambda_ji, 1)[0]
                    if A_ji &gt; 0:
                        G.add_edge(j, i, weight=A_ji)
                else:
                    # draw A_ij from P(A_ij) = Poisson(m_ij)
                    A_ji = self.prng.poisson(M[j, i], 1)[0]
                    if A_ji &gt; 0:
                        G.add_edge(j, i, weight=A_ji)
                    lambda_ij = M0[i, j] + self.eta * A_ji

                    # draw A_ji from P(A_ji|A_ij) = Poisson(lambda0_ji + eta*A_ij)
                    A_ij = self.prng.poisson(lambda_ij, 1)[0]
                    if A_ij &gt; 0:
                        G.add_edge(i, j, weight=A_ij)
                counter += 1
                totM += A_ij + A_ji

        # number of connected components
        n_connected_comp = len(list(nx.weakly_connected_components(G)))
        if n_connected_comp &gt; 1:
            msg = f&#34;Multitensor has produced a network with {n_connected_comp} connected components. &#34;
            msg += &#34;You can try increasing avg_degree and/or running with different seeds &#34;
            msg += &#34;until you get a network with just a single giant component.&#34;
            module_logger.warning(msg)

        self.Y[l] = nx.to_numpy_array(G)
        # cut-off, max entry has to be equal to K - 1
        self.Y[self.Y &gt; self.K - 1] = self.K - 1

    self.Y = preprocess(self.Y)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></b></code>:
<ul class="hlist">
<li><code><a title="vimure.synthetic.StandardSBM.build_X" href="#vimure.synthetic.BaseSyntheticNetwork.build_X">build_X</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.generate_lv" href="#vimure.synthetic.StandardSBM.generate_lv">generate_lv</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.get_layer" href="io.html#vimure.io.BaseNetwork.get_layer">get_layer</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.init_sbm_params" href="#vimure.synthetic.StandardSBM.init_sbm_params">init_sbm_params</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="vimure.synthetic.StandardSBM"><code class="flex name class">
<span>class <span class="ident">StandardSBM</span></span>
<span>(</span><span>C: int = 2, structure: str = None, avg_degree: float = 2, sparsify: bool = True, overlapping: float = 0.0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p><strong>Creates a standard stochastic block-model synthetic network.</strong></p>
<p>A generative graph model which assumes the probability of connecting two nodes in a graph is determined entirely by their block assignments.
For more information about this model, see Holland, P. W., Laskey, K. B., &amp; Leinhardt, S. (1983). <em>Stochastic blockmodels: First steps. Social networks</em>, 5(2), 109-137.
<a href="https://www.sciencedirect.com/science/article/abs/pii/0378873383900217">DOI:10.1016/0378-8733(83)90021-7</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>C</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of communities</dd>
<dt><strong><code>structure</code></strong> :&ensp;<code>str</code></dt>
<dd>Structures for the affinity tensor <code>w</code>. It can be 'assortative' or 'disassortative'.
It can be a list to map structure for each layer in a multilayer graph.</dd>
<dt><strong><code>avg_degree</code></strong> :&ensp;<code>float</code></dt>
<dd>Desired average degree for the network. It is not guaranteed that the
ultimate network will have that exact average degree value.
Try tweaking this parameter if you want to increase or decrease the
density of the network.</dd>
<dt><strong><code>sparsify</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True (default), enforce sparsity.</dd>
<dt><strong><code>overlapping</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of nodes with mixed membership. It has to be in <code>[0, 1)</code>.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional arguments of BaseSyntheticNetwork.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardSBM(BaseSyntheticNetwork):
    &#34;&#34;&#34;
    **Creates a standard stochastic block-model synthetic network.**

    A generative graph model which assumes the probability of connecting two nodes in a graph is determined entirely by their block assignments.
    For more information about this model, see Holland, P. W., Laskey, K. B., &amp; Leinhardt, S. (1983). _Stochastic blockmodels: First steps. Social networks_, 5(2), 109-137.
    [DOI:10.1016/0378-8733(83)90021-7](https://www.sciencedirect.com/science/article/abs/pii/0378873383900217)
    &#34;&#34;&#34;

    # TODO: Document overlapping communities separately as it involves setting several other parameters

    def __init__(
        self,
        C: int = DEFAULT_C,
        structure: str = DEFAULT_STRUCTURE,
        avg_degree: float = DEFAULT_AVG_DEGREE,
        sparsify: bool = DEFAULT_SPARSIFY,
        overlapping: float = DEFAULT_OVERLAPPING,
        **kwargs,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        C : int
            Number of communities
        structure: str
            Structures for the affinity tensor `w`. It can be &#39;assortative&#39; or &#39;disassortative&#39;. 
            It can be a list to map structure for each layer in a multilayer graph.
        avg_degree : float
            Desired average degree for the network. It is not guaranteed that the 
            ultimate network will have that exact average degree value. 
            Try tweaking this parameter if you want to increase or decrease the 
            density of the network.
        sparsify : bool
            If True (default), enforce sparsity.
        overlapping : float
            Fraction of nodes with mixed membership. It has to be in `[0, 1)`.
        kwargs : 
            Additional arguments of BaseSyntheticNetwork.
        &#34;&#34;&#34;
        self.init_sbm_params(
            C=C,
            structure=structure,
            avg_degree=avg_degree,
            sparsify=sparsify,
            overlapping=overlapping,
            **kwargs,
        )
        self.build_Y()

    def init_sbm_params(self, **kwargs):
        &#34;&#34;&#34;
        Check SBM-specific parameters
        &#34;&#34;&#34;

        super().__init__(**kwargs)

        self.u = np.zeros((self.N, self.K), dtype=float)  # out-going membership
        self.v = np.zeros((self.N, self.K), dtype=float)  # in-going membership

        if &#34;C&#34; in kwargs:
            self.C = kwargs[&#34;C&#34;]  # number of communities
        else:
            msg = &#34;C parameter was not set. Defaulting to C=%d&#34; % DEFAULT_C
            module_logger.warning(msg)
            self.C = DEFAULT_C

        if &#34;avg_degree&#34; in kwargs:
            avg_degree = kwargs[&#34;avg_degree&#34;]
        else:
            msg = &#34;avg_degree parameter was not set. Defaulting to avg_degree=%d&#34; % DEFAULT_AVG_DEGREE
            module_logger.warning(msg)
            avg_degree = DEFAULT_AVG_DEGREE
        self.avg_degree = avg_degree

        if &#34;sparsify&#34; in kwargs:
            sparsify = kwargs[&#34;sparsify&#34;]
        else:
            msg = &#34;sparsify parameter was not set. Defaulting to sparsify=False&#34;
            module_logger.warning(msg)
            sparsify = False
        self.sparsify = sparsify

        &#34;&#34;&#34;
        SETUP overlapping communities
        &#34;&#34;&#34;
        if &#34;overlapping&#34; in kwargs:
            overlapping = kwargs[&#34;overlapping&#34;]
            # fraction of nodes with mixed membership
            if (overlapping &lt; 0) or (overlapping &gt; 1):
                err_msg = &#34;The overlapping parameter has to be in [0, 1]!&#34;
                raise ValueError(err_msg)
        else:
            overlapping = False
        self.overlapping = overlapping
        if self.overlapping:

            if &#34;corr&#34; in kwargs:
                # correlation between u and v synthetically generated
                if (kwargs[&#34;corr&#34;] &lt; 0) or (kwargs[&#34;corr&#34;] &gt; 1):
                    msg = &#34;The correlation parameter corr has to be in [0, 1]!&#34;
                    raise ValueError(msg)

                corr = float(kwargs[&#34;corr&#34;])
            else:
                msg = &#34;corr parameter for overlapping communities was not set. Defaulting to corr=0.&#34;
                module_logger.warning(msg)
                corr = 0
            self.corr = corr

        if self.overlapping &gt; 0:
            if &#34;normalization&#34; in kwargs:
                self.normalization = bool(kwargs[&#34;normalization&#34;])
            else:
                msg = &#34;Normalization parameter was not set. Defaulting to normalization=False (Dirichlet overlapping communities)&#34;
                module_logger.warning(msg)
                self.normalization = False

            if self.normalization:
                if &#34;ag&#34; in kwargs:
                    self.ag = float(kwargs[&#34;ag&#34;])
                else:
                    msg = &#34;Parameter alpha for the Gamma distribution was not set. Defaulting to alpha=0.1&#34;
                    module_logger.warning(msg)
                    self.ag = 0.1

                if &#34;beta&#34; in kwargs:
                    self.beta = float(kwargs[&#34;beta&#34;])
                else:
                    msg = &#34;Parameter beta for the Gamma distribution was not set. Defaulting to beta=0.1&#34;

                    module_logger.warning(msg)
                    self.beta = 0.1
            else:
                if &#34;alpha&#34; in kwargs:
                    self.alpha = float(kwargs[&#34;alpha&#34;])
                else:
                    msg = (
                        &#34;Parameter alpha for the Dirichlet distribution was not set. Defaulting to alpha=0.1&#34;
                    )
                    module_logger.warning(msg)
                    self.alpha = 0.1

        &#34;&#34;&#34;
        SETUP informed structure
        &#34;&#34;&#34;
        if &#34;structure&#34; in kwargs:
            structure = kwargs[&#34;structure&#34;]
        else:
            structure = None

        if structure is None:
            structure = [&#34;assortative&#34;] * self.L
        elif type(structure) == str:
            if structure not in [&#34;assortative&#34;, &#34;disassortative&#34;]:
                msg = &#34;The available structures for the affinity tensor w are: assortative, disassortative!&#34;
                raise ValueError(msg)
            else:
                structure = [structure] * self.L
        elif len(structure) != self.L:  # list of structures of the affinity tensor w
            msg = (
                &#34;The parameter structure should be a list of length L. &#34;
                &#34;Each entry defines the structure of the corresponding layer!&#34;
            )
            raise ValueError(msg)
        for e in structure:
            if e not in [&#34;assortative&#34;, &#34;disassortative&#34;]:
                msg = (
                    &#34;The available structures for the affinity tensor w are: &#34; &#34;assortative, disassortative.!&#34;
                )
                raise ValueError(msg)
        self.structure = structure

    def __repr__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping})&#34;
        return return_str

    def __str__(self):
        return_str = f&#34;{self.__class__.__name__} (N={self.N}, M={self.M}, L={self.L}, &#34;
        return_str + f&#34;K={self.K}, seed={self.seed}, &#34;
        return_str += f&#34;C={self.C}, structure={self.structure}, avg_degree={self.avg_degree}, &#34;
        return_str += f&#34;sparsify={self.sparsify}, overlapping={self.overlapping})&#34;
        return return_str

    def build_Y(self):
        &#34;&#34;&#34;
        Latent variables
        &#34;&#34;&#34;
        self.u, self.v, self.w = self.generate_lv()

        &#34;&#34;&#34;
        Generate Y
        &#34;&#34;&#34;
        M_Y = np.einsum(&#34;ik,jq-&gt;ijkq&#34;, self.u, self.v)
        M_Y = np.einsum(&#34;ijkq,akq-&gt;aij&#34;, M_Y, self.w)
        # sparsity parameter for Y
        if self.sparsify:
            # TODO: Explain rationale behind this particular formula
            c = (float(self.N) * self.avg_degree) / M_Y.sum()
            M_Y *= c
            self.w *= c

        Y = self.prng.poisson(M_Y)
        for l in range(self.L):
            np.fill_diagonal(Y[l], 0)
        Y[Y &gt; self.K - 1] = self.K - 1  # cut-off, max entry has to be equal to K - 1

        self.Y = preprocess(Y)

    def __sample_membership_vectors(self):
        &#34;&#34;&#34;
        Compute the NxK membership vectors u, v using a Dirichlet distribution.

        INPUT
        ----------
        prng: Numpy Random object
              Random number generator container.
        alpha : float
                Parameter for Dirichlet.
        N : int
            Number of nodes.
        C : int
            Number of communities.
        corr : float
               Correlation between u and v synthetically generated.
        over : float
               Fraction of nodes with mixed membership.

        OUTPUT
        -------
        u : Numpy array
            Matrix NxC of out-going membership vectors, positive element-wise.
            With unitary L1 norm computed row-wise.

        v : Numpy array
            Matrix NxC of in-coming membership vectors, positive element-wise.
            With unitary L1 norm computed row-wise.
        &#34;&#34;&#34;

        # Generate equal-size unmixed group membership
        size = int(self.N / self.C)
        u = np.zeros((self.N, self.C))
        v = np.zeros((self.N, self.C))
        for i in range(self.N):
            q = int(math.floor(float(i) / float(size)))
            if q == self.C:
                u[i:, self.C - 1] = 1.0
                v[i:, self.C - 1] = 1.0
            else:
                for j in range(q * size, q * size + size):
                    u[j, q] = 1.0
                    v[j, q] = 1.0

        return u, v

    def __normalize_nonzero_membership(self, u):
        &#34;&#34;&#34;
        Given a matrix, it returns the same matrix normalized by row.

        INPUT
        ----------
        u: Numpy array
           Numpy Matrix.

        OUTPUT
        -------
        The matrix normalized by row.
        &#34;&#34;&#34;

        den1 = u.sum(axis=1, keepdims=True)
        nzz = den1 == 0.0
        den1[nzz] = 1.0

        return u / den1

    def __compute_affinity_matrix(self, structure, a=0.1):
        &#34;&#34;&#34;
        Compute the CxC affinity matrix w with probabilities between and within groups.

        Parameters
        ----------
        structure : string
            Structure of the network.
        a : float
            Parameter for secondary probabilities.

        Returns
        -------
        p : Numpy array
            Array with probabilities between and within groups. Element (k,h)
            gives the density of edges going from the nodes of group k to nodes of group h.
        &#34;&#34;&#34;

        p1 = self.avg_degree * self.C / self.N

        if structure == &#34;assortative&#34;:
            p = p1 * a * np.ones((self.C, self.C))  # secondary-probabilities
            np.fill_diagonal(p, p1 * np.ones(self.C))  # primary-probabilities

        elif structure == &#34;disassortative&#34;:
            p = p1 * np.ones((self.C, self.C))  # primary-probabilities
            np.fill_diagonal(p, a * p1 * np.ones(self.C))  # secondary-probabilities

        return p

    def __apply_overlapping(self, u, v):
        overlapping = int(self.N * self.overlapping)  # number of nodes belonging to more communities
        ind_over = np.random.randint(len(u), size=overlapping)

        if not self.normalization:
            # u and v from a Dirichlet distribution
            u[ind_over] = self.prng.dirichlet(self.alpha * np.ones(self.C), overlapping)
            v[ind_over] = self.corr * u[ind_over] + (1.0 - self.corr) * self.prng.dirichlet(
                self.alpha * np.ones(self.C), overlapping
            )
            if self.corr == 1.0:
                assert np.allclose(u, v)
            if self.corr &gt; 0:
                v = self.__normalize_nonzero_membership(v)
        else:
            # u and v from a Gamma distribution
            u[ind_over] = self.prng.gamma(self.ag, 1.0 / self.beta, size=(overlapping, self.C))
            v[ind_over] = self.corr * self.u[ind_over] + (1.0 - self.corr) * self.prng.gamma(
                self.ag, 1.0 / self.beta, size=(overlapping, self.C)
            )
            u = self.__normalize_nonzero_membership(u)
            v = self.__normalize_nonzero_membership(v)

        return u, v

    def generate_lv(self):
        &#34;&#34;&#34;
        Generate latent variables for a Stochastic BlockModel, assuming network layers are independent.
        &#34;&#34;&#34;

        # Generate u, v for overlapping communities
        u, v = self.__sample_membership_vectors()

        if self.overlapping &gt; 0:
            u, v = self.__apply_overlapping(u, v)

        # Generate w
        w = np.zeros((self.L, self.C, self.C))
        for l in range(self.L):
            w[l, :, :] = self.__compute_affinity_matrix(self.structure[l])

        return u, v, w</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></li>
<li><a title="vimure.io.BaseNetwork" href="io.html#vimure.io.BaseNetwork">BaseNetwork</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="vimure.synthetic.DegreeCorrectedSBM" href="#vimure.synthetic.DegreeCorrectedSBM">DegreeCorrectedSBM</a></li>
<li><a title="vimure.synthetic.Multitensor" href="#vimure.synthetic.Multitensor">Multitensor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vimure.synthetic.StandardSBM.build_Y"><code class="name flex">
<span>def <span class="ident">build_Y</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Latent variables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_Y(self):
    &#34;&#34;&#34;
    Latent variables
    &#34;&#34;&#34;
    self.u, self.v, self.w = self.generate_lv()

    &#34;&#34;&#34;
    Generate Y
    &#34;&#34;&#34;
    M_Y = np.einsum(&#34;ik,jq-&gt;ijkq&#34;, self.u, self.v)
    M_Y = np.einsum(&#34;ijkq,akq-&gt;aij&#34;, M_Y, self.w)
    # sparsity parameter for Y
    if self.sparsify:
        # TODO: Explain rationale behind this particular formula
        c = (float(self.N) * self.avg_degree) / M_Y.sum()
        M_Y *= c
        self.w *= c

    Y = self.prng.poisson(M_Y)
    for l in range(self.L):
        np.fill_diagonal(Y[l], 0)
    Y[Y &gt; self.K - 1] = self.K - 1  # cut-off, max entry has to be equal to K - 1

    self.Y = preprocess(Y)</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.StandardSBM.generate_lv"><code class="name flex">
<span>def <span class="ident">generate_lv</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate latent variables for a Stochastic BlockModel, assuming network layers are independent.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_lv(self):
    &#34;&#34;&#34;
    Generate latent variables for a Stochastic BlockModel, assuming network layers are independent.
    &#34;&#34;&#34;

    # Generate u, v for overlapping communities
    u, v = self.__sample_membership_vectors()

    if self.overlapping &gt; 0:
        u, v = self.__apply_overlapping(u, v)

    # Generate w
    w = np.zeros((self.L, self.C, self.C))
    for l in range(self.L):
        w[l, :, :] = self.__compute_affinity_matrix(self.structure[l])

    return u, v, w</code></pre>
</details>
</dd>
<dt id="vimure.synthetic.StandardSBM.init_sbm_params"><code class="name flex">
<span>def <span class="ident">init_sbm_params</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Check SBM-specific parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_sbm_params(self, **kwargs):
    &#34;&#34;&#34;
    Check SBM-specific parameters
    &#34;&#34;&#34;

    super().__init__(**kwargs)

    self.u = np.zeros((self.N, self.K), dtype=float)  # out-going membership
    self.v = np.zeros((self.N, self.K), dtype=float)  # in-going membership

    if &#34;C&#34; in kwargs:
        self.C = kwargs[&#34;C&#34;]  # number of communities
    else:
        msg = &#34;C parameter was not set. Defaulting to C=%d&#34; % DEFAULT_C
        module_logger.warning(msg)
        self.C = DEFAULT_C

    if &#34;avg_degree&#34; in kwargs:
        avg_degree = kwargs[&#34;avg_degree&#34;]
    else:
        msg = &#34;avg_degree parameter was not set. Defaulting to avg_degree=%d&#34; % DEFAULT_AVG_DEGREE
        module_logger.warning(msg)
        avg_degree = DEFAULT_AVG_DEGREE
    self.avg_degree = avg_degree

    if &#34;sparsify&#34; in kwargs:
        sparsify = kwargs[&#34;sparsify&#34;]
    else:
        msg = &#34;sparsify parameter was not set. Defaulting to sparsify=False&#34;
        module_logger.warning(msg)
        sparsify = False
    self.sparsify = sparsify

    &#34;&#34;&#34;
    SETUP overlapping communities
    &#34;&#34;&#34;
    if &#34;overlapping&#34; in kwargs:
        overlapping = kwargs[&#34;overlapping&#34;]
        # fraction of nodes with mixed membership
        if (overlapping &lt; 0) or (overlapping &gt; 1):
            err_msg = &#34;The overlapping parameter has to be in [0, 1]!&#34;
            raise ValueError(err_msg)
    else:
        overlapping = False
    self.overlapping = overlapping
    if self.overlapping:

        if &#34;corr&#34; in kwargs:
            # correlation between u and v synthetically generated
            if (kwargs[&#34;corr&#34;] &lt; 0) or (kwargs[&#34;corr&#34;] &gt; 1):
                msg = &#34;The correlation parameter corr has to be in [0, 1]!&#34;
                raise ValueError(msg)

            corr = float(kwargs[&#34;corr&#34;])
        else:
            msg = &#34;corr parameter for overlapping communities was not set. Defaulting to corr=0.&#34;
            module_logger.warning(msg)
            corr = 0
        self.corr = corr

    if self.overlapping &gt; 0:
        if &#34;normalization&#34; in kwargs:
            self.normalization = bool(kwargs[&#34;normalization&#34;])
        else:
            msg = &#34;Normalization parameter was not set. Defaulting to normalization=False (Dirichlet overlapping communities)&#34;
            module_logger.warning(msg)
            self.normalization = False

        if self.normalization:
            if &#34;ag&#34; in kwargs:
                self.ag = float(kwargs[&#34;ag&#34;])
            else:
                msg = &#34;Parameter alpha for the Gamma distribution was not set. Defaulting to alpha=0.1&#34;
                module_logger.warning(msg)
                self.ag = 0.1

            if &#34;beta&#34; in kwargs:
                self.beta = float(kwargs[&#34;beta&#34;])
            else:
                msg = &#34;Parameter beta for the Gamma distribution was not set. Defaulting to beta=0.1&#34;

                module_logger.warning(msg)
                self.beta = 0.1
        else:
            if &#34;alpha&#34; in kwargs:
                self.alpha = float(kwargs[&#34;alpha&#34;])
            else:
                msg = (
                    &#34;Parameter alpha for the Dirichlet distribution was not set. Defaulting to alpha=0.1&#34;
                )
                module_logger.warning(msg)
                self.alpha = 0.1

    &#34;&#34;&#34;
    SETUP informed structure
    &#34;&#34;&#34;
    if &#34;structure&#34; in kwargs:
        structure = kwargs[&#34;structure&#34;]
    else:
        structure = None

    if structure is None:
        structure = [&#34;assortative&#34;] * self.L
    elif type(structure) == str:
        if structure not in [&#34;assortative&#34;, &#34;disassortative&#34;]:
            msg = &#34;The available structures for the affinity tensor w are: assortative, disassortative!&#34;
            raise ValueError(msg)
        else:
            structure = [structure] * self.L
    elif len(structure) != self.L:  # list of structures of the affinity tensor w
        msg = (
            &#34;The parameter structure should be a list of length L. &#34;
            &#34;Each entry defines the structure of the corresponding layer!&#34;
        )
        raise ValueError(msg)
    for e in structure:
        if e not in [&#34;assortative&#34;, &#34;disassortative&#34;]:
            msg = (
                &#34;The available structures for the affinity tensor w are: &#34; &#34;assortative, disassortative.!&#34;
            )
            raise ValueError(msg)
    self.structure = structure</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></b></code>:
<ul class="hlist">
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.build_X" href="#vimure.synthetic.BaseSyntheticNetwork.build_X">build_X</a></code></li>
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.get_layer" href="io.html#vimure.io.BaseNetwork.get_layer">get_layer</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="vimure" href="index.html">vimure</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="vimure.synthetic.build_custom_theta" href="#vimure.synthetic.build_custom_theta">build_custom_theta</a></code></li>
<li><code><a title="vimure.synthetic.build_self_reporter_mask" href="#vimure.synthetic.build_self_reporter_mask">build_self_reporter_mask</a></code></li>
<li><code><a title="vimure.synthetic.transpose_ij" href="#vimure.synthetic.transpose_ij">transpose_ij</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vimure.synthetic.BaseSyntheticNetwork" href="#vimure.synthetic.BaseSyntheticNetwork">BaseSyntheticNetwork</a></code></h4>
<ul class="">
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.build_X" href="#vimure.synthetic.BaseSyntheticNetwork.build_X">build_X</a></code></li>
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.build_Y" href="#vimure.synthetic.BaseSyntheticNetwork.build_Y">build_Y</a></code></li>
<li><code><a title="vimure.synthetic.BaseSyntheticNetwork.generate_lv" href="#vimure.synthetic.BaseSyntheticNetwork.generate_lv">generate_lv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vimure.synthetic.DegreeCorrectedSBM" href="#vimure.synthetic.DegreeCorrectedSBM">DegreeCorrectedSBM</a></code></h4>
<ul class="">
<li><code><a title="vimure.synthetic.DegreeCorrectedSBM.generate_lv" href="#vimure.synthetic.DegreeCorrectedSBM.generate_lv">generate_lv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vimure.synthetic.HollandLaskeyLeinhardtModel" href="#vimure.synthetic.HollandLaskeyLeinhardtModel">HollandLaskeyLeinhardtModel</a></code></h4>
</li>
<li>
<h4><code><a title="vimure.synthetic.Multitensor" href="#vimure.synthetic.Multitensor">Multitensor</a></code></h4>
<ul class="">
<li><code><a title="vimure.synthetic.Multitensor.Exp_ija_matrix" href="#vimure.synthetic.Multitensor.Exp_ija_matrix">Exp_ija_matrix</a></code></li>
<li><code><a title="vimure.synthetic.Multitensor.build_Y" href="#vimure.synthetic.Multitensor.build_Y">build_Y</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="vimure.synthetic.StandardSBM" href="#vimure.synthetic.StandardSBM">StandardSBM</a></code></h4>
<ul class="">
<li><code><a title="vimure.synthetic.StandardSBM.build_Y" href="#vimure.synthetic.StandardSBM.build_Y">build_Y</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.generate_lv" href="#vimure.synthetic.StandardSBM.generate_lv">generate_lv</a></code></li>
<li><code><a title="vimure.synthetic.StandardSBM.init_sbm_params" href="#vimure.synthetic.StandardSBM.init_sbm_params">init_sbm_params</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>