<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>vimure.model API documentation</title>
<meta name="description" content="Inference model" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>vimure.model</code></h1>
</header>
<section id="section-intro">
<p>Inference model</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Inference model&#34;&#34;&#34;
import sys
import time
import warnings

import numpy as np
import pandas as pd
import sktensor as skt
import scipy.special as sp
from scipy.stats import poisson

from .log import setup_logging
from .utils import preprocess, get_item_array_from_subs

from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.utils.validation import _deprecate_positional_args


INF = 1e10
DEFAULT_EPS = 1e-12
DEFAULT_BIAS0 = 0.0
DEFAULT_MAX_ITER = 20
DEFAULT_NUM_REALISATIONS = 20


class VimureModel(TransformerMixin, BaseEstimator):
    @_deprecate_positional_args
    def __init__(
        self,
        undirected: bool = False,
        mutuality: bool = True,
        convergence_tol: float = 0.1,
        decision: int = 1,
        verbose: bool = True,
    ):
        &#34;&#34;&#34;

        This closely follows the scikit-learn structure of classes:
        https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py

        Parameters
        ----------
        undirected : boolean
                     If True, the given and the estimate networks are undirected.
        mutuality: boolean
                     If this is false, then do not estimate mutuality (network cond. independent)
        convergence_tol : float
                    Controls when to stop the optimisation algorithm (CAVI)
        &#34;&#34;&#34;

        self.undirected = undirected

        if undirected:
            msg = &#34;Overriding mutuality to False since the network is undirected&#34;
            warnings.warn(msg)
            self.mutuality = False
        else:
            self.mutuality = mutuality

        self.convergence_tol = convergence_tol  # tolerance parameter for convergence
        self.decision = decision  # convergence parameter

        self.verbose = verbose
        self.logger = setup_logging(&#34;vm.model.VimureModel&#34;, verbose)

    # TODO Minor refactoring: make this function easier to read (High cyclomatic complexity)
    #      https://betterembsw.blogspot.com/2014/06/avoid-high-cyclomatic-complexity.html

    def __str__(self) -&gt; str:
        return super().__str__()
    
    def __repr__(self) -&gt; str:
        return super().__repr__()

    def __check_fit_params(
        self,
        X: np.ndarray,
        lambda_prior=(10.0, 10.0),
        theta_prior=(0.1, 0.1),
        eta_prior=(0.5, 1.0),
        rho_prior=None,
        seed: int = None,
        **extra_params,
    ):

        available_extra_params = [
            &#34;R&#34;,
            &#34;EPS&#34;,
            &#34;K&#34;,
            &#34;bias0&#34;,
            &#34;max_iter&#34;,
            &#34;alpha_lambda&#34;,
            &#34;beta_lambda&#34;,
            &#34;alpha_teta&#34;,
            &#34;beta_teta&#34;,
            &#34;num_realisations&#34;,
        ]
        for extra_param in extra_params:
            if extra_param not in available_extra_params:
                msg = &#34;Ignoring unrecognised parameter %s.&#34; % extra_param
                self.logger.warn(msg)

        # If the network is undirected, then do not estimate the mutuality
        if self.undirected and not np.array_equal(X, np.transpose(X, axes=(0, 2, 1, 3))):
            msg = &#34;If undirected is True, the given network has to be symmetric wrt l and m!&#34;
            self.logger.error(msg)
            raise ValueError(msg)

        # TODO: Instead of having these variables (data_T and data_T_vals),
        #       why not just use X with the appropriate (l,j,i,m) mapping?
        if isinstance(X, np.ndarray) or isinstance(X, skt.dtensor):  # if data is dense array
            if self.mutuality:
                # to use mutuality
                self.data_T = np.einsum(&#34;aijm-&gt;ajim&#34;, X)
                # to calculate denominator of z1 (Xjim)
                self.data_T_vals = get_item_array_from_subs(self.data_T, X.nonzero())
            else:
                self.data_T = np.zeros_like(X)
                self.data_T_vals = None
            self.X = preprocess(X)  # transform into sp tensor
        else:
            self.X = preprocess(X)  # transform into sp tensor
            if self.mutuality:
                # to use mutuality
                layer, i, j, m = self.X.subs

                # Transpose lijm -&gt; ljim
                self.data_T = skt.sptensor(
                    subs=(layer, j, i, m), vals=self.X.vals.tolist(), shape=self.X.shape
                )
                # to calculate denominator of z1 (Xjim)
                self.data_T_vals = get_item_array_from_subs(self.data_T, self.X.subs).astype(int)

            else:
                self.data_T = skt.sptensor(
                    subs=tuple([np.array([], dtype=&#34;int8&#34;) for i in range(len(self.X.shape))]),
                    vals=[],
                    shape=self.X.shape,
                )
                self.data_T_vals = None

        self.subs_nz = self.X.subs
        self.sumX = self.X.vals.sum()

        self.L, self.N, self.M = X.shape[0], X.shape[1], X.shape[3]

        if &#34;K&#34; in extra_params:
            if extra_params[&#34;K&#34;] is None:
                self.K = np.max(X.vals) + 1

                # logger.warning() vs warnings.warn() https://stackoverflow.com/a/14762106/843365
                msg = f&#34;Parameter K was None. Defaulting to: {self.K}&#34;
                warnings.warn(msg, UserWarning)
            else:
                self.K = int(extra_params[&#34;K&#34;])
        else:

            if isinstance(X, skt.sptensor):
                self.K = X.vals.max() + 1
            else:
                self.K = int(X.max()) + 1

            # logger.warning() vs warnings.warn() https://stackoverflow.com/a/14762106/843365
            msg = f&#34;Parameter K was None. Defaulting to: {self.K}&#34;
            warnings.warn(msg, UserWarning)

        if &#34;R&#34; in extra_params:
            R = extra_params[&#34;R&#34;]
            if R.shape != (self.L, self.N, self.N, self.M):
                msg = &#34;Dimensions of reporter mask (R) do not match L x N x N x M&#34;
                self.logger.error(msg)
                raise ValueError(msg)
        else:
            msg = &#34;Reporters Mask was not informed (parameter R). &#34;
            msg += &#34;The model will assume that every reporter can report on any tie.&#34;

            # logger.warning() vs warnings.warn() https://stackoverflow.com/a/14762106/843365
            warnings.warn(msg, UserWarning)
            R = np.ones((self.L, self.N, self.N, self.M))
        self.R = preprocess(R)

        if &#34;EPS&#34; in extra_params:
            self.EPS = float(extra_params[&#34;EPS&#34;])
        else:
            self.EPS = DEFAULT_EPS

        if &#34;bias0&#34; in extra_params:
            self.bias0 = float(extra_params[&#34;bias0&#34;])
        else:
            self.bias0 = DEFAULT_BIAS0

        if &#34;max_iter&#34; in extra_params:
            self.max_iter = int(extra_params[&#34;max_iter&#34;])
        else:
            self.max_iter = DEFAULT_MAX_ITER

        if &#34;num_realisations&#34; in extra_params:
            self.num_realisations = int(extra_params[&#34;num_realisations&#34;])
        else:
            self.num_realisations = DEFAULT_NUM_REALISATIONS

        &#34;&#34;&#34;
        HANDLE theta priors
        &#34;&#34;&#34;
        if &#34;alpha_theta&#34; in extra_params or &#34;beta_theta&#34; in extra_params:

            self.alpha_theta = extra_params[&#34;alpha_theta&#34;]
            self.beta_theta = extra_params[&#34;beta_theta&#34;]

            if self.alpha_theta.shape != (self.L, self.M):
                msg = &#34;alpha_theta matrix is not valid.&#34;
                msg += &#34; When using this parameter, make sure to inform a %d x %d matrix.&#34;
                self.logger.error(msg)
                raise ValueError(msg % (self.L, self.M))

            if self.beta_theta.shape != (self.L, self.M):
                msg = &#34;beta_theta matrix is not valid. When using this parameter, make sure to inform a %d x %d matrix.&#34;
                self.logger.error(msg)
                raise ValueError(msg % (self.L, self.M))

            warn_msg = &#34;Ignoring theta_prior since full alpha_theta and beta_theta tensors were informed&#34;
            self.logger.debug(warn_msg)

        else:
            if type(theta_prior) is not tuple or len(theta_prior) != 2:
                msg = &#34;theta_prior must be a 2D tuple!&#34;
                self.logger.error(msg)
                raise ValueError(msg)
            self.alpha_theta = theta_prior[0]
            self.beta_theta = theta_prior[1]

        &#34;&#34;&#34;
        HANDLE lambda priors
        &#34;&#34;&#34;
        if &#34;alpha_lambda&#34; in extra_params or &#34;beta_lambda&#34; in extra_params:

            self.alpha_lambda = extra_params[&#34;alpha_lambda&#34;]
            self.beta_lambda = extra_params[&#34;beta_lambda&#34;]

            if self.alpha_lambda.shape != (self.L, self.K):
                msg = &#34;alpha_lambda matrix is not valid (dimensions = %d x %d).&#34;
                msg += &#34;When using this parameter, make sure to pass a %d x %d matrix.&#34;
                msg = msg % (self.alpha_lambda.shape[0], self.alpha_lambda.shape[1], self.L, self.K,)
                self.logger.error(msg)
                raise ValueError(msg)

            if self.beta_lambda.shape != (self.L, self.K):
                msg = &#34;beta_lambda matrix is not valid (dimensions = %d x %d).&#34;
                msg += &#34;When using this parameter, make sure to pass a %d x %d matrix.&#34;
                msg = msg % (self.beta_lambda.shape[0], self.beta_lambda.shape[1], self.L, self.K,)
                self.logger.error(msg)
                raise ValueError(msg)

            warn_msg = &#34;Ignoring lambda_prior since full alpha_lambda and beta_lambda tensors were informed&#34;
            self.logger.debug(warn_msg)

        else:
            if type(lambda_prior) is not tuple or len(lambda_prior) != 2:
                msg = &#34;lambda_prior must be a 2D tuple!&#34;
                self.logger.error(msg)
                raise ValueError(msg)
            self.alpha_lambda = lambda_prior[0]
            self.beta_lambda = lambda_prior[1]

        if type(eta_prior) is not tuple or len(eta_prior) != 2:
            msg = &#34;eta_prior must be a 2D tuple!&#34;
            self.logger.error(msg)
            raise ValueError(msg)
        self.alpha_mutuality = eta_prior[0]
        self.beta_mutuality = eta_prior[1]

        if rho_prior is not None and rho_prior.shape != (self.L, self.N, self.N):
            msg = &#34;rho_prior has to have shape equal to (L, N, N)!&#34;
            self.logger.error(msg)
            raise ValueError(msg)
        self.rho_prior = rho_prior

        self._change_seed(seed)

    def fit(
        self,
        X: np.ndarray,
        theta_prior=(0.1, 0.1),
        lambda_prior=(10.0, 10.0),
        eta_prior=(0.5, 1.0),
        rho_prior=None,
        seed: int = None,
        **extra_params,
    ):
        &#34;&#34;&#34;
        Fit a probabilistic generative model to double sampled networks. It returns reliability parameters for the
        reporters (theta), average interactions for the links (lambda) and the estimate of the true and unknown
        network (rho). The inference is performed with a Variational Inference approach.

        Parameters
        ----------
        X : ndarray
            Network adjacency tensor.
        theta_prior: 2D tuple
            Shape and scale hyperparameters for variable theta
        lambda_prior: 2D tuple
            Shape and scale hyperparameters for variable lambda
        eta_prior: 2D tuple
            Shape and scale hyperparameters for variable eta
        rho_prior : None/ndarray
            Array with prior values of the rho parameter - if ndarray.

        Extra parameters (Advanced tuning of inference)
        ----------------
        R: ndarray
            a multidimensional array L x N x N x M indicating which reports to consider
        K: None/int
            Value of the maximum entry of the network - i
        EPS : float
            White noise. Default: 1e-12
        bias0: float
            Bias for rho_prior entry 0. Default: 0.2
        max_iter: int
            Maximum number of iteration steps before aborting. Default=500

        Returns
        -------
        self.rho_f, self.G_exp_theta_f, self.G_exp_lambda_f, self.G_exp_nu_f, self.maxL
        &#34;&#34;&#34;

        self.logger.debug(&#34;Checking user parameters passed to the VimureModel.fit()&#34;)
        self.__check_fit_params(
            X=X,
            theta_prior=theta_prior,
            lambda_prior=lambda_prior,
            rho_prior=rho_prior,
            eta_prior=eta_prior,
            seed=seed,
            **extra_params,
        )

        &#34;&#34;&#34;
        Inference
        &#34;&#34;&#34;
        maxL = -INF  # initialization of the maximum elbo
        trace = []  # Keep track of elbo values and running time

        for r in range(self.num_realisations):

            self.logger.debug(&#34;Initializing priors&#34;)
            if r &lt; 5:  # the first 5 runs are with bias = 0
                self._set_rho_prior()
            else:
                curr_bias0 = (r - 4) * self.bias0
                self._set_rho_prior(bias0=curr_bias0)

            self._initialize_priors()
            self._initialize_old_variables()

            coincide = 0
            iter = 1
            reached_convergence = False

            elbo = -INF  # initialization of the elbo

            while not reached_convergence and iter &lt;= self.max_iter:
                time_start = time.time()
                delta_gamma, delta_phi, delta_rho, delta_nu = self._update_CAVI(
                    data=self.X, subs_nz=self.subs_nz, data_T_vals=self.data_T_vals
                )
                runtime = time.time() - time_start

                iter, elbo, coincide, reached_convergence = self._check_for_convergence(
                    data=self.X,
                    data_T=self.data_T,
                    subs_nz=self.subs_nz,
                    r=r,
                    iter=iter,
                    elbo=elbo,
                    coincide=coincide,
                    reached_convergence=reached_convergence,
                )

                if (iter - 1) % 10 == 0:
                    trace.append((r, self.seed, iter - 1, elbo, runtime, reached_convergence))

            if maxL &lt; elbo:
                self._update_optimal_parameters()
                maxL = elbo

            if self.seed is None:
                new_seed = self.prng.randint(1, 500)
            else:
                new_seed = self.seed + self.prng.randint(1, 500)

            self._change_seed(new_seed)
            # end cycle over realizations

        cols = [&#34;realisation&#34;, &#34;seed&#34;, &#34;iter&#34;, &#34;elbo&#34;, &#34;runtime&#34;, &#34;reached_convergence&#34;]
        self.trace = pd.DataFrame(trace, columns=cols)

        self.maxL = maxL

        # TODO: Consider removing non-final copies of internal dataframes such as self.rho, self.gamma_rte
        #       and keep only the final versions (self.rho_f, self.gamma_rte) to save disk space
        #       when saving model to disk
        return self

    def _change_seed(self, seed):
        self.seed = seed
        self.prng = np.random.RandomState(seed)

    &#34;&#34;&#34;
    INITIALISATION
    &#34;&#34;&#34;

    def _set_rho_prior(self, bias0=DEFAULT_BIAS0):
        &#34;&#34;&#34;
        Set prior on rho
        &#34;&#34;&#34;

        # TODO: Allow self.rho_prior to be skt.sptensor if a user prefers sparse tensors.

        self.logger.debug(&#34;Setting priors on rho&#34;)
        # TODO: Make pr_rho sparse
        pr_rho = np.zeros((self.L, self.N, self.N, self.K))
        if self.rho_prior is None:
            pr_rho = 1 + 0.01 * self.prng.rand(self.L, self.N, self.N, self.K)

            if self.mutuality is True:
                pr_rho[:, :, :, 0] += bias0  # bias the 0-th entry to be higher
            else:
                pr_rho[:, :, :, 0] += bias0  # bias the 0-th entry to be higher

            if self.undirected:  # impose symmetry
                pr_rho = (pr_rho + np.transpose(pr_rho, axes=(0, 2, 1, 3))) / 2.0

            # Normalizing
            norm = pr_rho.sum(axis=-1)
            pr_rho /= norm[:, :, :, np.newaxis]
        else:

            sub_nz = self.rho_prior.nonzero()
            for k in range(self.K):
                pr_rho[(*sub_nz, k * np.ones(sub_nz[0].shape[0]).astype(&#34;int&#34;))] = poisson.pmf(
                    k, self.rho_prior[sub_nz]
                ) + 1.0 * self.prng.rand(sub_nz[0].shape[0])

            if self.undirected:  # impose symmetry
                for layer in range(self.L):
                    for k in range(self.K):
                        pr_rho[layer, :, :, k] = (pr_rho[layer, :, :, k] + pr_rho[layer, :, :, k].T) / 2.0
            norm = pr_rho[sub_nz].sum(axis=-1)
            pr_rho[sub_nz] /= norm[:, np.newaxis]

        &#34;&#34;&#34;
        INFORMATIVE PRIORS BASED ON REPORTERS&#39; MASK &amp; X UNION

        Calculate ties lij that have not been reported by anyone. R(lij,m) == 0 forall m
        # TODO: The code below is quite slow. Think of a faster way to do that without increasing memory footprint
        &#34;&#34;&#34;
        # Step 1: Calculate all possible combinations of lij ties in format: dim (L*N*N, 3)
        all_possible_ties = (
            np.stack(np.meshgrid(range(self.L), range(self.N), range(self.N)))
            .reshape(-1, self.L * self.N * self.N)
            .T
        )
        all_possible_ties = pd.DataFrame(all_possible_ties, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])

        def get_ties_not_reported(R_or_X):

            # Step 2: Calculate the union of lij ties in R (or X)
            if isinstance(R_or_X, skt.dtensor):
                subs_lij = R_or_X.nonzero()[0:3]
            else:
                subs_lij = R_or_X.subs[0:3]
            all_reported_ties = pd.DataFrame.from_dict({&#34;l&#34;: subs_lij[0], &#34;i&#34;: subs_lij[1], &#34;j&#34;: subs_lij[2]})
            all_reported_ties.drop_duplicates(inplace=True)

            # Step 3: Which of the all_possible_ties do not appear in all_reported_ties?
            df = pd.merge(all_possible_ties, all_reported_ties, how=&#34;left&#34;, indicator=True)
            relevant_set = df[df[&#34;_merge&#34;] == &#34;left_only&#34;][[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]]

            return relevant_set

        ties_not_reported_by_R = get_ties_not_reported(self.R)

        if len(ties_not_reported_by_R) &gt; 0:

            layer = ties_not_reported_by_R[&#34;l&#34;]
            i = ties_not_reported_by_R[&#34;i&#34;]
            j = ties_not_reported_by_R[&#34;j&#34;]
            # Step 4: Prepare lij indices and set appropriate values in pr_rho(lij,k)
            pr_rho[layer, i, j, :] = 0
            pr_rho[layer, i, j, 0] = 1

        ties_not_reported_by_X = get_ties_not_reported(self.X)

        if len(ties_not_reported_by_X) &gt; 0:

            layer = ties_not_reported_by_X[&#34;l&#34;]
            i = ties_not_reported_by_X[&#34;i&#34;]
            j = ties_not_reported_by_X[&#34;j&#34;]
            # Step 4: Prepare lij indices and set appropriate values in pr_rho(lij,k)
            pr_rho[layer, i, j, :] = 0
            pr_rho[layer, i, j, 0] = 1

        self.pr_rho = pr_rho
        self.logpr_rho = np.log(self.pr_rho + self.EPS)

    def _initialize_priors(self):
        &#34;&#34;&#34;
        Random initialization of the parameters theta, lambda, eta, rho.
        &#34;&#34;&#34;

        self.logger.verbose(&#34;Setting priors for gamma_shp, phi_shp, gamma_rte, phi_rte&#34;)
        # TODO: Could these variables be made sparse?

        # we include some randomness
        self.gamma_shp = self.alpha_theta * self.prng.random_sample(size=(self.L, self.M)) + self.alpha_theta
        self.phi_shp = self.alpha_lambda * self.prng.random_sample(size=(self.L, self.K)) + self.alpha_lambda
        self.gamma_rte = self.beta_theta * self.prng.random_sample(size=(self.L, self.M)) + self.beta_theta
        self.phi_rte = self.beta_lambda * self.prng.random_sample(size=(self.L, self.K)) + self.beta_lambda

        self.logger.verbose(&#34;Setting priors for nu_shp, nu_rte&#34;)
        if self.mutuality:
            self.nu_shp = self.alpha_mutuality * self.prng.random_sample(1)[0] + self.alpha_mutuality
            self.nu_rte = self.beta_mutuality + self.sumX  # this is fixed once and for all
            self.G_exp_nu = np.exp(sp.psi(self.nu_shp) - np.log(self.nu_rte))
        else:  # not use the mutuality (eta ~ 0.)
            self.nu_shp = 0.000001
            self.nu_rte = 1.0
            self.G_exp_nu = 0.0

        self.rho = np.copy(self.pr_rho)

        self.G_exp_theta = np.exp(sp.psi(self.gamma_shp) - np.log(self.gamma_rte))
        self.G_exp_lambda = np.exp(sp.psi(self.phi_shp) - np.log(self.phi_rte))

    def _initialize_old_variables(self):
        &#34;&#34;&#34;
        Initialize variables to keep the values of the parameters in the previous iteration.
        &#34;&#34;&#34;

        self.gamma_shp_old = np.copy(self.gamma_shp)
        self.gamma_rte_old = np.copy(self.gamma_rte)
        self.phi_shp_old = np.copy(self.phi_shp)
        self.phi_rte_old = np.copy(self.phi_rte)
        self.rho_old = np.copy(self.rho)
        self.nu_shp_old = np.copy(self.nu_shp)

    &#34;&#34;&#34;
    UPDATE VARIABLES
    &#34;&#34;&#34;

    def _update_CAVI(self, data, subs_nz, data_T_vals=None):
        &#34;&#34;&#34;
        Update parameters using Coordinate Ascent Variational Inference (CAVI)

        References:
            Blei, D. M., Kucukelbir, A., &amp; McAuliffe, J. D. (2017).
            Variational Inference: A Review for Statisticians.
            Journal of the American Statistical Association, 112(518), 859–877.
            https://doi.org/10.1080/01621459.2017.1285773

        Parameters
        ----------
        data : sptensor/dtensor
            Network adjacency tensor.
        subs_nz : tuple
            Indices of elements of data that are non-zero.
        data_T_vals : ndarray/None
            Array with values of entries A[j, i] given non-zero entry (i, j) - if mutuality=True.
        &#34;&#34;&#34;
        self.logger.verbose(&#34;Updating gamma&#34;)
        self._update_cache(data, subs_nz, data_T_vals)
        delta_gamma = self._update_gamma(subs_nz)

        self.logger.verbose(&#34;Updating phi&#34;)
        self._update_cache(data, subs_nz, data_T_vals)
        delta_phi = self._update_phi(subs_nz)

        self.logger.verbose(&#34;Updating rho&#34;)
        self._update_cache(data, subs_nz, data_T_vals)
        delta_rho = self._update_rho(subs_nz)
        if self.mutuality:
            self.logger.verbose(&#34;Updating nu&#34;)
            self._update_cache(data, subs_nz, data_T_vals)
            delta_nu = self._update_nu(subs_nz)
        else:
            delta_nu = 0.0

        return (delta_gamma, delta_phi, delta_rho, delta_nu)

    def _update_cache(self, data, subs_nz, data_T_vals=None):
        &#34;&#34;&#34;
        Update the cache used in the CAVI update.

        Parameters
        ----------
        data : sptensor
            Network adjacency tensor.
        subs_nz : tuple
            Indices of elements of data that are non-zero.
        data_T_vals : ndarray/None
            Array with values of entries A[j, i] given non-zero entry (i, j) - if mutuality=True.
        &#34;&#34;&#34;

        self.G_exp_theta = np.exp(sp.psi(self.gamma_shp) - np.log(self.gamma_rte))
        self.G_exp_lambda = np.exp(sp.psi(self.phi_shp) - np.log(self.phi_rte))

        if not self.mutuality:
            self.data_z1_nz = data.vals[:, np.newaxis].astype(float)
            self.data_z2_nz = None
        else:

            self.G_exp_nu = np.exp(sp.psi(self.nu_shp) - np.log(self.nu_rte))
            self.z1_nz = np.einsum(
                &#34;I,Ik-&gt;Ik&#34;, self.G_exp_theta[subs_nz[0], subs_nz[3]], self.G_exp_lambda[subs_nz[0], :],
            )  # has dim= (I,K)
            self.z2_nz = self.G_exp_nu * data_T_vals  # has dim= (I)
            self.z_den_nz = self.z1_nz + self.z2_nz[:, np.newaxis]  # has dim= (I,K)
            self.z_den_nz[self.z_den_nz == 0] = 1
            self.data_z1_nz = data.vals[:, np.newaxis] * self.z1_nz / self.z_den_nz
            self.data_z2_nz = data.vals[:, np.newaxis] * self.z2_nz[:, np.newaxis] / self.z_den_nz

    def _update_gamma(self, subs_nz):

        self.gamma_shp = self.alpha_theta + self.sp_uttkrp_theta(self.data_z1_nz, subs_nz)

        E_phi_rho = np.einsum(&#34;lijk,lk-&gt;lij&#34;, self.rho, self.phi_shp / self.phi_rte)

        if isinstance(self.R, skt.dtensor):
            self.gamma_rte = self.beta_theta + np.einsum(&#34;lij,lijm-&gt;lm&#34;, E_phi_rho, np.array(self.R))
        else:

            self.gamma_rte = self.beta_theta * np.ones(shape=(self.L, self.M))
            # sum over k, final dim=I
            tmp = E_phi_rho[self.R.subs[0], self.R.subs[1], self.R.subs[2]]
            for c, (l, m) in enumerate(zip(*(self.R.subs[0], self.R.subs[3]))):  # sum over i,j
                self.gamma_rte[l, m] += tmp[c]

        dist_gs = np.amax(abs(self.gamma_shp - self.gamma_shp_old))
        dist_gr = np.amax(abs(self.gamma_rte - self.gamma_rte_old))
        dist_gamma = max(dist_gs, dist_gr)

        self.gamma_shp_old = np.copy(self.gamma_shp)
        self.gamma_rte_old = np.copy(self.gamma_rte)

        return dist_gamma

    def _update_phi(self, subs_nz):

        self.phi_shp = self.alpha_lambda + self.sp_uttkrp_lambda(self.data_z1_nz, subs_nz)

        out = np.zeros_like(self.phi_rte)

        if isinstance(self.R, skt.dtensor):
            subs = self.R.nonzero()
        else:
            subs = self.R.subs

        Egamma = self.gamma_shp[subs[0], subs[3]] / self.gamma_rte[subs[0], subs[3]]
        tmp = self.rho[subs[0], subs[1], subs[2], :] * Egamma[:, np.newaxis]  # dim is (I,K)
        for k in range(self.K):  # sum over i,j,m
            out[:, k] += np.bincount(subs[0], weights=tmp[:, k], minlength=self.L)

        self.phi_rte = self.beta_lambda + out

        # self.phi_rte = np.einsum(&#39;lijk,lm-&gt;lijmk&#39;, self.rho, self.gamma_shp / self.gamma_rte)
        # self.phi_rte = self.beta_lambda + np.einsum(&#39;lijmk,lijm-&gt;lk&#39;, self.phi_rte, self.R)

        dist_pa = np.amax(abs(self.phi_shp - self.phi_shp_old))
        dist_pr = np.amax(abs(self.phi_rte - self.phi_shp_old))
        dist_phi = max(dist_pa, dist_pr)

        self.phi_shp_old = np.copy(self.phi_shp)
        self.phi_rte_old = np.copy(self.phi_rte)

        return dist_phi

    def _update_rho(self, subs_nz):

        # TODO: Make Exp_theta_lambda sparse
        if isinstance(self.R, skt.dtensor):
            Exp_theta_lambda = np.einsum(&#34;lijm,lm-&gt;lij&#34;, np.array(self.R), self.gamma_shp / self.gamma_rte)
        else:

            tmp = pd.DataFrame.from_dict(
                {
                    &#34;l&#34;: self.R.subs[0],
                    &#34;i&#34;: self.R.subs[1],
                    &#34;j&#34;: self.R.subs[2],
                    &#34;m&#34;: self.R.subs[3],
                    &#34;val&#34;: (self.gamma_shp / self.gamma_rte)[(self.R.subs[0], self.R.subs[3])] * self.R.vals,
                }
            )

            tmp = tmp.groupby([&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])[[&#34;val&#34;]].sum()
            tmp.reset_index(inplace=True)
            Exp_theta_lambda = skt.sptensor(
                subs=(tmp[&#34;l&#34;], tmp[&#34;i&#34;], tmp[&#34;j&#34;]), vals=tmp[&#34;val&#34;].tolist(), shape=(self.L, self.N, self.N)
            )
            Exp_theta_lambda = Exp_theta_lambda.toarray()

        self.logger.verbose(&#34;calculating einsum (Exp_theta_lambda vs phi_shp/phi_rte)&#34;)
        Exp_theta_lambda = np.einsum(&#34;lij,lk-&gt;lijk&#34;, Exp_theta_lambda, self.phi_shp / self.phi_rte)

        self.logger.verbose(&#34;calculating log_rho&#34;)
        log_rho = self.logpr_rho + self.sp_uttkrp_rho(self.data_z1_nz, subs_nz) - Exp_theta_lambda

        self.logger.verbose(&#34;finishing updating rho&#34;)
        self.rho = np.exp(log_rho)
        sums_over_k = self.rho.sum(axis=3)

        # normalize so that the sum over k is 1
        self.rho[sums_over_k &gt; 0] /= sums_over_k[sums_over_k &gt; 0, np.newaxis]

        dist_rho = np.amax(abs(self.rho - self.rho_old))

        self.rho_old = np.copy(self.rho)
        self.logger.verbose(&#34;finished updating rho&#34;)

        return dist_rho

    def _update_nu(self, subs_nz):

        self.nu_shp = (
            self.alpha_mutuality + (self.data_z2_nz * self.rho[subs_nz[0], subs_nz[1], subs_nz[2], :]).sum()
        )
        dist_nu = abs(self.nu_shp - self.nu_shp_old)

        self.nu_shp_old = np.copy(self.nu_shp)

        return dist_nu

    def sp_uttkrp_theta(self, vals, subs):
        &#34;&#34;&#34;
        Compute the Khatri-Rao product (sparse version).

        Parameters
        ----------
        vals : ndarray
               Values of the non-zero entries.
        subs : tuple
               Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
               be equal to the dimension of tensor.

        Returns
        -------
        out : ndarray
              Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
              product of the membership matrix.
        &#34;&#34;&#34;

        out = np.zeros_like(self.gamma_shp)

        # sum over k, final dim=I
        tmp = (self.rho[subs[0], subs[1], subs[2], :].astype(vals.dtype) * vals).sum(axis=1)
        for c, (l, m) in enumerate(zip(*(subs[0], subs[3]))):  # sum over i,j
            out[l, m] += tmp[c]
        return out

    def sp_uttkrp_lambda(self, vals, subs):
        &#34;&#34;&#34;
        Compute the Khatri-Rao product (sparse version).

        Parameters
        ----------
        vals : ndarray
               Values of the non-zero entries.
        subs : tuple
               Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
               be equal to the dimension of tensor.

        Returns
        -------
        out : ndarray
              Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
              product of the membership matrix.
        &#34;&#34;&#34;

        out = np.zeros_like(self.phi_shp)
        tmp = self.rho[subs[0], subs[1], subs[2], :].astype(vals.dtype) * vals  # dim is (I,K)
        for k in range(self.K):  # sum over i,j,m
            out[:, k] += np.bincount(subs[0], weights=tmp[:, k], minlength=self.L)

        return out

    def sp_uttkrp_rho(self, vals, subs):
        &#34;&#34;&#34;
        Compute the Khatri-Rao product (sparse version).

        Parameters
        ----------
        vals : ndarray
               Values of the non-zero entries.
        subs : tuple
               Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
               be equal to the dimension of tensor.

        Returns
        -------
        out : ndarray
              Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
              product of the membership matrix.
        &#34;&#34;&#34;

        out = np.zeros_like(self.rho)

        # TODO: try by removing ExpLog_theta
        ExpLog_theta = sp.psi(self.gamma_shp) - np.log(self.gamma_rte)
        ExpLog_lambda = sp.psi(self.phi_shp) - np.log(self.phi_rte)

        # dim is (I,K)
        # tmp = (ExpLog_lambda[subs[0], :]).astype(vals.dtype) * vals
        tmp = (ExpLog_theta[subs[0], subs[3]][:, np.newaxis] + ExpLog_lambda[subs[0], :]).astype(
            vals.dtype
        ) * vals
        # sum over m
        for c, (l, i, j) in enumerate(zip(*(subs[0], subs[1], subs[2]))):
            out[l, i, j, :] += tmp[c, :]

        return out

    def _update_optimal_parameters(self):
        &#34;&#34;&#34;
        Update values of the parameters after convergence.
        &#34;&#34;&#34;

        # Parameters
        self.gamma_shp_f = np.copy(self.gamma_shp)
        self.gamma_rte_f = np.copy(self.gamma_rte)
        self.phi_shp_f = np.copy(self.phi_shp)
        self.phi_rte_f = np.copy(self.phi_rte)
        self.nu_shp_f = np.copy(self.nu_shp)
        self.nu_rte_f = np.copy(self.nu_rte)
        self.rho_f = np.copy(self.rho)

        # Geometric expectations
        self.G_exp_theta_f = np.exp(sp.psi(self.gamma_shp_f) - np.log(self.gamma_rte_f))
        self.G_exp_lambda_f = np.exp(sp.psi(self.phi_shp_f) - np.log(self.phi_rte_f))
        self.G_exp_nu_f = np.exp(sp.psi(self.nu_shp_f) - np.log(self.nu_rte_f))

    &#34;&#34;&#34;
    ELBO
    &#34;&#34;&#34;

    def __ELBO(self, data, data_T, subs_nz):
        &#34;&#34;&#34;
        Function to compute the ELBO.
        &#34;&#34;&#34;

        E_exp_theta = self.gamma_shp / self.gamma_rte
        E_exp_lambda = self.phi_shp / self.phi_rte
        E_exp_rec = self.nu_shp / self.nu_rte

        E_PoissonMean = self.calculate_mean_poisson(
            G_exp_theta=E_exp_theta,
            G_exp_lambda=E_exp_lambda,
            G_exp_nu=E_exp_rec,
            rho=self.rho,
            X_T=data_T,
            R=self.R,
        )
        elbo = -E_PoissonMean.vals.sum()

        G_PoissonMean = self.calculate_mean_poisson(
            G_exp_theta=self.G_exp_theta,
            G_exp_lambda=self.G_exp_lambda,
            G_exp_nu=self.G_exp_nu,
            rho=np.exp(self.rho),
            X_T=data_T,
            R=self.R,
        )

        # Filter it to only include (l,i,j,m) that are in X
        rho_prior_lijm = pd.DataFrame.from_dict(
            {&#34;l&#34;: subs_nz[0], &#34;i&#34;: subs_nz[1], &#34;j&#34;: subs_nz[2], &#34;m&#34;: subs_nz[3]}
        )
        G_PoissonMean_lijm = pd.DataFrame.from_dict(
            {
                &#34;l&#34;: G_PoissonMean.subs[0],
                &#34;i&#34;: G_PoissonMean.subs[1],
                &#34;j&#34;: G_PoissonMean.subs[2],
                &#34;m&#34;: G_PoissonMean.subs[3],
                &#34;val&#34;: G_PoissonMean.vals,
            }
        )

        logPoissonMean = pd.merge(rho_prior_lijm, G_PoissonMean_lijm, how=&#34;left&#34;)
        logPoissonMean.fillna(0, inplace=True)
        logPoissonMean = logPoissonMean[&#34;val&#34;].values  # convert to numpy array

        logPoissonMean = np.log(logPoissonMean + self.EPS)
        elbo += (data.vals * logPoissonMean).sum()

        elbo += gamma_elbo_term(
            pa=self.alpha_theta, pb=self.beta_theta, qa=self.gamma_shp, qb=self.gamma_rte
        ).sum()
        elbo += gamma_elbo_term(
            pa=self.alpha_lambda, pb=self.beta_lambda, qa=self.phi_shp, qb=self.phi_rte
        ).sum()
        elbo += gamma_elbo_term(
            pa=self.alpha_mutuality, pb=self.beta_mutuality, qa=self.nu_shp, qb=self.nu_rte
        )

        elbo += categorical_elbo_term(self.rho, self.pr_rho, self.EPS).sum()

        if np.isnan(elbo):
            raise ValueError(&#34;ELBO is NaN!!!!&#34;)
            sys.exit(1)
        else:
            return elbo

    def _check_for_convergence(
        self,
        data: np.ndarray,
        data_T: np.ndarray,
        subs_nz,
        r: int,
        iter: int,
        elbo: float,
        coincide: int,
        reached_convergence,
    ):
        &#34;&#34;&#34;
        Check for convergence by using the ELBO values.
        &#34;&#34;&#34;

        if iter == 1 or iter % 10 == 0 or iter == self.max_iter:
            old_L = elbo
            # loglik = self.__Likelihood(data, data_T, subs_nz)
            elbo = self.__ELBO(data, data_T, subs_nz)

            if abs(elbo - old_L) &lt; self.convergence_tol:
                coincide += 1
            else:
                coincide = 0

        if coincide &gt; self.decision:
            reached_convergence = True

        if iter == 1 or iter % 10 == 0 or iter == self.max_iter:
            msg = f&#34;Realisation {r:2} | Iter {iter:4} | ELBO value: {elbo:6.12f} | &#34;
            msg += f&#34;Reached convergence: {reached_convergence}&#34;
            self.logger.debug(msg)

        iter += 1

        return iter, elbo, coincide, reached_convergence

    &#34;&#34;&#34;
    UTILS
    &#34;&#34;&#34;

    def calculate_mean_poisson(
        self, G_exp_theta=None, G_exp_lambda=None, G_exp_nu=None, rho=None, X_T=None, R=None,
    ):

        if G_exp_theta is None:
            G_exp_theta = self.G_exp_theta_f

        if G_exp_lambda is None:
            G_exp_lambda = self.G_exp_lambda_f

        if G_exp_nu is None:
            G_exp_nu = self.G_exp_nu_f

        if rho is None:
            rho = self.rho_f

        if X_T is None:
            X_T = self.data_T

        if R is None:
            R = self.R

        &#34;&#34;&#34;
        exp value of: [theta * lambda + eta * X.T]
        &#34;&#34;&#34;
        if isinstance(self.R, skt.dtensor):
            R_subs = self.R.nonzero()
        else:
            R_subs = self.R.subs

        # Following Equation 2:
        ThetaLambda = np.einsum(&#34;I,Ik-&gt;Ik&#34;, G_exp_theta[R_subs[0], R_subs[3]], G_exp_lambda[R_subs[0], :],)

        if isinstance(X_T, skt.dtensor) or isinstance(X_T, np.ndarray):
            PoissonMean = ThetaLambda + G_exp_nu * X_T[R_subs[0], R_subs[1], R_subs[2], R_subs[3], np.newaxis]
        else:
            R_lijm = pd.DataFrame.from_dict({&#34;l&#34;: R_subs[0], &#34;i&#34;: R_subs[1], &#34;j&#34;: R_subs[2], &#34;m&#34;: R_subs[3]})
            X_lijm = pd.DataFrame.from_dict(
                {&#34;l&#34;: X_T.subs[0], &#34;i&#34;: X_T.subs[1], &#34;j&#34;: X_T.subs[2], &#34;m&#34;: X_T.subs[3], &#34;val&#34;: X_T.vals}
            )

            X_T_array = pd.merge(R_lijm, X_lijm, how=&#34;left&#34;)
            X_T_array.fillna(0, inplace=True)
            X_T_array = X_T_array[&#34;val&#34;].values  # convert to numpy array

            PoissonMean = ThetaLambda + G_exp_nu * X_T_array[:, np.newaxis]

        vals = np.einsum(&#34;Ik,Ik-&gt;I&#34;, rho[R_subs[0], R_subs[1], R_subs[2], :], PoissonMean)
        rho_PoissonMean = skt.sptensor(subs=R_subs, vals=vals)

        return rho_PoissonMean


&#34;&#34;&#34;
UTIL FUNCTIONS FOR INFERENCE
&#34;&#34;&#34;


def gamma_elbo_term(pa, pb, qa, qb):
    return sp.gammaln(qa) - pa * np.log(qb) + (pa - qa) * sp.psi(qa) + qa * (1 - pb / qb)


def categorical_elbo_term(rho, prior_rho, EPS):
    K = rho.shape[-1]
    layer = np.zeros((rho.shape[0], rho.shape[1], rho.shape[2]))
    for k in range(K):
        layer += rho[:, :, :, k] * np.log(prior_rho[:, :, :, k] + EPS) - rho[:, :, :, k] * np.log(
            rho[:, :, :, k] + EPS
        )
    return layer</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="vimure.model.categorical_elbo_term"><code class="name flex">
<span>def <span class="ident">categorical_elbo_term</span></span>(<span>rho, prior_rho, EPS)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def categorical_elbo_term(rho, prior_rho, EPS):
    K = rho.shape[-1]
    layer = np.zeros((rho.shape[0], rho.shape[1], rho.shape[2]))
    for k in range(K):
        layer += rho[:, :, :, k] * np.log(prior_rho[:, :, :, k] + EPS) - rho[:, :, :, k] * np.log(
            rho[:, :, :, k] + EPS
        )
    return layer</code></pre>
</details>
</dd>
<dt id="vimure.model.gamma_elbo_term"><code class="name flex">
<span>def <span class="ident">gamma_elbo_term</span></span>(<span>pa, pb, qa, qb)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gamma_elbo_term(pa, pb, qa, qb):
    return sp.gammaln(qa) - pa * np.log(qb) + (pa - qa) * sp.psi(qa) + qa * (1 - pb / qb)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="vimure.model.VimureModel"><code class="flex name class">
<span>class <span class="ident">VimureModel</span></span>
<span>(</span><span>undirected: bool = False, mutuality: bool = True, convergence_tol: float = 0.1, decision: int = 1, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Mixin class for all transformers in scikit-learn.</p>
<p>This closely follows the scikit-learn structure of classes:
<a href="https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py">https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>undirected</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the given and the estimate networks are undirected.</dd>
<dt><strong><code>mutuality</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If this is false, then do not estimate mutuality (network cond. independent)</dd>
<dt><strong><code>convergence_tol</code></strong> :&ensp;<code>float</code></dt>
<dd>Controls when to stop the optimisation algorithm (CAVI)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VimureModel(TransformerMixin, BaseEstimator):
    @_deprecate_positional_args
    def __init__(
        self,
        undirected: bool = False,
        mutuality: bool = True,
        convergence_tol: float = 0.1,
        decision: int = 1,
        verbose: bool = True,
    ):
        &#34;&#34;&#34;

        This closely follows the scikit-learn structure of classes:
        https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py

        Parameters
        ----------
        undirected : boolean
                     If True, the given and the estimate networks are undirected.
        mutuality: boolean
                     If this is false, then do not estimate mutuality (network cond. independent)
        convergence_tol : float
                    Controls when to stop the optimisation algorithm (CAVI)
        &#34;&#34;&#34;

        self.undirected = undirected

        if undirected:
            msg = &#34;Overriding mutuality to False since the network is undirected&#34;
            warnings.warn(msg)
            self.mutuality = False
        else:
            self.mutuality = mutuality

        self.convergence_tol = convergence_tol  # tolerance parameter for convergence
        self.decision = decision  # convergence parameter

        self.verbose = verbose
        self.logger = setup_logging(&#34;vm.model.VimureModel&#34;, verbose)

    # TODO Minor refactoring: make this function easier to read (High cyclomatic complexity)
    #      https://betterembsw.blogspot.com/2014/06/avoid-high-cyclomatic-complexity.html

    def __str__(self) -&gt; str:
        return super().__str__()
    
    def __repr__(self) -&gt; str:
        return super().__repr__()

    def __check_fit_params(
        self,
        X: np.ndarray,
        lambda_prior=(10.0, 10.0),
        theta_prior=(0.1, 0.1),
        eta_prior=(0.5, 1.0),
        rho_prior=None,
        seed: int = None,
        **extra_params,
    ):

        available_extra_params = [
            &#34;R&#34;,
            &#34;EPS&#34;,
            &#34;K&#34;,
            &#34;bias0&#34;,
            &#34;max_iter&#34;,
            &#34;alpha_lambda&#34;,
            &#34;beta_lambda&#34;,
            &#34;alpha_teta&#34;,
            &#34;beta_teta&#34;,
            &#34;num_realisations&#34;,
        ]
        for extra_param in extra_params:
            if extra_param not in available_extra_params:
                msg = &#34;Ignoring unrecognised parameter %s.&#34; % extra_param
                self.logger.warn(msg)

        # If the network is undirected, then do not estimate the mutuality
        if self.undirected and not np.array_equal(X, np.transpose(X, axes=(0, 2, 1, 3))):
            msg = &#34;If undirected is True, the given network has to be symmetric wrt l and m!&#34;
            self.logger.error(msg)
            raise ValueError(msg)

        # TODO: Instead of having these variables (data_T and data_T_vals),
        #       why not just use X with the appropriate (l,j,i,m) mapping?
        if isinstance(X, np.ndarray) or isinstance(X, skt.dtensor):  # if data is dense array
            if self.mutuality:
                # to use mutuality
                self.data_T = np.einsum(&#34;aijm-&gt;ajim&#34;, X)
                # to calculate denominator of z1 (Xjim)
                self.data_T_vals = get_item_array_from_subs(self.data_T, X.nonzero())
            else:
                self.data_T = np.zeros_like(X)
                self.data_T_vals = None
            self.X = preprocess(X)  # transform into sp tensor
        else:
            self.X = preprocess(X)  # transform into sp tensor
            if self.mutuality:
                # to use mutuality
                layer, i, j, m = self.X.subs

                # Transpose lijm -&gt; ljim
                self.data_T = skt.sptensor(
                    subs=(layer, j, i, m), vals=self.X.vals.tolist(), shape=self.X.shape
                )
                # to calculate denominator of z1 (Xjim)
                self.data_T_vals = get_item_array_from_subs(self.data_T, self.X.subs).astype(int)

            else:
                self.data_T = skt.sptensor(
                    subs=tuple([np.array([], dtype=&#34;int8&#34;) for i in range(len(self.X.shape))]),
                    vals=[],
                    shape=self.X.shape,
                )
                self.data_T_vals = None

        self.subs_nz = self.X.subs
        self.sumX = self.X.vals.sum()

        self.L, self.N, self.M = X.shape[0], X.shape[1], X.shape[3]

        if &#34;K&#34; in extra_params:
            if extra_params[&#34;K&#34;] is None:
                self.K = np.max(X.vals) + 1

                # logger.warning() vs warnings.warn() https://stackoverflow.com/a/14762106/843365
                msg = f&#34;Parameter K was None. Defaulting to: {self.K}&#34;
                warnings.warn(msg, UserWarning)
            else:
                self.K = int(extra_params[&#34;K&#34;])
        else:

            if isinstance(X, skt.sptensor):
                self.K = X.vals.max() + 1
            else:
                self.K = int(X.max()) + 1

            # logger.warning() vs warnings.warn() https://stackoverflow.com/a/14762106/843365
            msg = f&#34;Parameter K was None. Defaulting to: {self.K}&#34;
            warnings.warn(msg, UserWarning)

        if &#34;R&#34; in extra_params:
            R = extra_params[&#34;R&#34;]
            if R.shape != (self.L, self.N, self.N, self.M):
                msg = &#34;Dimensions of reporter mask (R) do not match L x N x N x M&#34;
                self.logger.error(msg)
                raise ValueError(msg)
        else:
            msg = &#34;Reporters Mask was not informed (parameter R). &#34;
            msg += &#34;The model will assume that every reporter can report on any tie.&#34;

            # logger.warning() vs warnings.warn() https://stackoverflow.com/a/14762106/843365
            warnings.warn(msg, UserWarning)
            R = np.ones((self.L, self.N, self.N, self.M))
        self.R = preprocess(R)

        if &#34;EPS&#34; in extra_params:
            self.EPS = float(extra_params[&#34;EPS&#34;])
        else:
            self.EPS = DEFAULT_EPS

        if &#34;bias0&#34; in extra_params:
            self.bias0 = float(extra_params[&#34;bias0&#34;])
        else:
            self.bias0 = DEFAULT_BIAS0

        if &#34;max_iter&#34; in extra_params:
            self.max_iter = int(extra_params[&#34;max_iter&#34;])
        else:
            self.max_iter = DEFAULT_MAX_ITER

        if &#34;num_realisations&#34; in extra_params:
            self.num_realisations = int(extra_params[&#34;num_realisations&#34;])
        else:
            self.num_realisations = DEFAULT_NUM_REALISATIONS

        &#34;&#34;&#34;
        HANDLE theta priors
        &#34;&#34;&#34;
        if &#34;alpha_theta&#34; in extra_params or &#34;beta_theta&#34; in extra_params:

            self.alpha_theta = extra_params[&#34;alpha_theta&#34;]
            self.beta_theta = extra_params[&#34;beta_theta&#34;]

            if self.alpha_theta.shape != (self.L, self.M):
                msg = &#34;alpha_theta matrix is not valid.&#34;
                msg += &#34; When using this parameter, make sure to inform a %d x %d matrix.&#34;
                self.logger.error(msg)
                raise ValueError(msg % (self.L, self.M))

            if self.beta_theta.shape != (self.L, self.M):
                msg = &#34;beta_theta matrix is not valid. When using this parameter, make sure to inform a %d x %d matrix.&#34;
                self.logger.error(msg)
                raise ValueError(msg % (self.L, self.M))

            warn_msg = &#34;Ignoring theta_prior since full alpha_theta and beta_theta tensors were informed&#34;
            self.logger.debug(warn_msg)

        else:
            if type(theta_prior) is not tuple or len(theta_prior) != 2:
                msg = &#34;theta_prior must be a 2D tuple!&#34;
                self.logger.error(msg)
                raise ValueError(msg)
            self.alpha_theta = theta_prior[0]
            self.beta_theta = theta_prior[1]

        &#34;&#34;&#34;
        HANDLE lambda priors
        &#34;&#34;&#34;
        if &#34;alpha_lambda&#34; in extra_params or &#34;beta_lambda&#34; in extra_params:

            self.alpha_lambda = extra_params[&#34;alpha_lambda&#34;]
            self.beta_lambda = extra_params[&#34;beta_lambda&#34;]

            if self.alpha_lambda.shape != (self.L, self.K):
                msg = &#34;alpha_lambda matrix is not valid (dimensions = %d x %d).&#34;
                msg += &#34;When using this parameter, make sure to pass a %d x %d matrix.&#34;
                msg = msg % (self.alpha_lambda.shape[0], self.alpha_lambda.shape[1], self.L, self.K,)
                self.logger.error(msg)
                raise ValueError(msg)

            if self.beta_lambda.shape != (self.L, self.K):
                msg = &#34;beta_lambda matrix is not valid (dimensions = %d x %d).&#34;
                msg += &#34;When using this parameter, make sure to pass a %d x %d matrix.&#34;
                msg = msg % (self.beta_lambda.shape[0], self.beta_lambda.shape[1], self.L, self.K,)
                self.logger.error(msg)
                raise ValueError(msg)

            warn_msg = &#34;Ignoring lambda_prior since full alpha_lambda and beta_lambda tensors were informed&#34;
            self.logger.debug(warn_msg)

        else:
            if type(lambda_prior) is not tuple or len(lambda_prior) != 2:
                msg = &#34;lambda_prior must be a 2D tuple!&#34;
                self.logger.error(msg)
                raise ValueError(msg)
            self.alpha_lambda = lambda_prior[0]
            self.beta_lambda = lambda_prior[1]

        if type(eta_prior) is not tuple or len(eta_prior) != 2:
            msg = &#34;eta_prior must be a 2D tuple!&#34;
            self.logger.error(msg)
            raise ValueError(msg)
        self.alpha_mutuality = eta_prior[0]
        self.beta_mutuality = eta_prior[1]

        if rho_prior is not None and rho_prior.shape != (self.L, self.N, self.N):
            msg = &#34;rho_prior has to have shape equal to (L, N, N)!&#34;
            self.logger.error(msg)
            raise ValueError(msg)
        self.rho_prior = rho_prior

        self._change_seed(seed)

    def fit(
        self,
        X: np.ndarray,
        theta_prior=(0.1, 0.1),
        lambda_prior=(10.0, 10.0),
        eta_prior=(0.5, 1.0),
        rho_prior=None,
        seed: int = None,
        **extra_params,
    ):
        &#34;&#34;&#34;
        Fit a probabilistic generative model to double sampled networks. It returns reliability parameters for the
        reporters (theta), average interactions for the links (lambda) and the estimate of the true and unknown
        network (rho). The inference is performed with a Variational Inference approach.

        Parameters
        ----------
        X : ndarray
            Network adjacency tensor.
        theta_prior: 2D tuple
            Shape and scale hyperparameters for variable theta
        lambda_prior: 2D tuple
            Shape and scale hyperparameters for variable lambda
        eta_prior: 2D tuple
            Shape and scale hyperparameters for variable eta
        rho_prior : None/ndarray
            Array with prior values of the rho parameter - if ndarray.

        Extra parameters (Advanced tuning of inference)
        ----------------
        R: ndarray
            a multidimensional array L x N x N x M indicating which reports to consider
        K: None/int
            Value of the maximum entry of the network - i
        EPS : float
            White noise. Default: 1e-12
        bias0: float
            Bias for rho_prior entry 0. Default: 0.2
        max_iter: int
            Maximum number of iteration steps before aborting. Default=500

        Returns
        -------
        self.rho_f, self.G_exp_theta_f, self.G_exp_lambda_f, self.G_exp_nu_f, self.maxL
        &#34;&#34;&#34;

        self.logger.debug(&#34;Checking user parameters passed to the VimureModel.fit()&#34;)
        self.__check_fit_params(
            X=X,
            theta_prior=theta_prior,
            lambda_prior=lambda_prior,
            rho_prior=rho_prior,
            eta_prior=eta_prior,
            seed=seed,
            **extra_params,
        )

        &#34;&#34;&#34;
        Inference
        &#34;&#34;&#34;
        maxL = -INF  # initialization of the maximum elbo
        trace = []  # Keep track of elbo values and running time

        for r in range(self.num_realisations):

            self.logger.debug(&#34;Initializing priors&#34;)
            if r &lt; 5:  # the first 5 runs are with bias = 0
                self._set_rho_prior()
            else:
                curr_bias0 = (r - 4) * self.bias0
                self._set_rho_prior(bias0=curr_bias0)

            self._initialize_priors()
            self._initialize_old_variables()

            coincide = 0
            iter = 1
            reached_convergence = False

            elbo = -INF  # initialization of the elbo

            while not reached_convergence and iter &lt;= self.max_iter:
                time_start = time.time()
                delta_gamma, delta_phi, delta_rho, delta_nu = self._update_CAVI(
                    data=self.X, subs_nz=self.subs_nz, data_T_vals=self.data_T_vals
                )
                runtime = time.time() - time_start

                iter, elbo, coincide, reached_convergence = self._check_for_convergence(
                    data=self.X,
                    data_T=self.data_T,
                    subs_nz=self.subs_nz,
                    r=r,
                    iter=iter,
                    elbo=elbo,
                    coincide=coincide,
                    reached_convergence=reached_convergence,
                )

                if (iter - 1) % 10 == 0:
                    trace.append((r, self.seed, iter - 1, elbo, runtime, reached_convergence))

            if maxL &lt; elbo:
                self._update_optimal_parameters()
                maxL = elbo

            if self.seed is None:
                new_seed = self.prng.randint(1, 500)
            else:
                new_seed = self.seed + self.prng.randint(1, 500)

            self._change_seed(new_seed)
            # end cycle over realizations

        cols = [&#34;realisation&#34;, &#34;seed&#34;, &#34;iter&#34;, &#34;elbo&#34;, &#34;runtime&#34;, &#34;reached_convergence&#34;]
        self.trace = pd.DataFrame(trace, columns=cols)

        self.maxL = maxL

        # TODO: Consider removing non-final copies of internal dataframes such as self.rho, self.gamma_rte
        #       and keep only the final versions (self.rho_f, self.gamma_rte) to save disk space
        #       when saving model to disk
        return self

    def _change_seed(self, seed):
        self.seed = seed
        self.prng = np.random.RandomState(seed)

    &#34;&#34;&#34;
    INITIALISATION
    &#34;&#34;&#34;

    def _set_rho_prior(self, bias0=DEFAULT_BIAS0):
        &#34;&#34;&#34;
        Set prior on rho
        &#34;&#34;&#34;

        # TODO: Allow self.rho_prior to be skt.sptensor if a user prefers sparse tensors.

        self.logger.debug(&#34;Setting priors on rho&#34;)
        # TODO: Make pr_rho sparse
        pr_rho = np.zeros((self.L, self.N, self.N, self.K))
        if self.rho_prior is None:
            pr_rho = 1 + 0.01 * self.prng.rand(self.L, self.N, self.N, self.K)

            if self.mutuality is True:
                pr_rho[:, :, :, 0] += bias0  # bias the 0-th entry to be higher
            else:
                pr_rho[:, :, :, 0] += bias0  # bias the 0-th entry to be higher

            if self.undirected:  # impose symmetry
                pr_rho = (pr_rho + np.transpose(pr_rho, axes=(0, 2, 1, 3))) / 2.0

            # Normalizing
            norm = pr_rho.sum(axis=-1)
            pr_rho /= norm[:, :, :, np.newaxis]
        else:

            sub_nz = self.rho_prior.nonzero()
            for k in range(self.K):
                pr_rho[(*sub_nz, k * np.ones(sub_nz[0].shape[0]).astype(&#34;int&#34;))] = poisson.pmf(
                    k, self.rho_prior[sub_nz]
                ) + 1.0 * self.prng.rand(sub_nz[0].shape[0])

            if self.undirected:  # impose symmetry
                for layer in range(self.L):
                    for k in range(self.K):
                        pr_rho[layer, :, :, k] = (pr_rho[layer, :, :, k] + pr_rho[layer, :, :, k].T) / 2.0
            norm = pr_rho[sub_nz].sum(axis=-1)
            pr_rho[sub_nz] /= norm[:, np.newaxis]

        &#34;&#34;&#34;
        INFORMATIVE PRIORS BASED ON REPORTERS&#39; MASK &amp; X UNION

        Calculate ties lij that have not been reported by anyone. R(lij,m) == 0 forall m
        # TODO: The code below is quite slow. Think of a faster way to do that without increasing memory footprint
        &#34;&#34;&#34;
        # Step 1: Calculate all possible combinations of lij ties in format: dim (L*N*N, 3)
        all_possible_ties = (
            np.stack(np.meshgrid(range(self.L), range(self.N), range(self.N)))
            .reshape(-1, self.L * self.N * self.N)
            .T
        )
        all_possible_ties = pd.DataFrame(all_possible_ties, columns=[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])

        def get_ties_not_reported(R_or_X):

            # Step 2: Calculate the union of lij ties in R (or X)
            if isinstance(R_or_X, skt.dtensor):
                subs_lij = R_or_X.nonzero()[0:3]
            else:
                subs_lij = R_or_X.subs[0:3]
            all_reported_ties = pd.DataFrame.from_dict({&#34;l&#34;: subs_lij[0], &#34;i&#34;: subs_lij[1], &#34;j&#34;: subs_lij[2]})
            all_reported_ties.drop_duplicates(inplace=True)

            # Step 3: Which of the all_possible_ties do not appear in all_reported_ties?
            df = pd.merge(all_possible_ties, all_reported_ties, how=&#34;left&#34;, indicator=True)
            relevant_set = df[df[&#34;_merge&#34;] == &#34;left_only&#34;][[&#34;l&#34;, &#34;i&#34;, &#34;j&#34;]]

            return relevant_set

        ties_not_reported_by_R = get_ties_not_reported(self.R)

        if len(ties_not_reported_by_R) &gt; 0:

            layer = ties_not_reported_by_R[&#34;l&#34;]
            i = ties_not_reported_by_R[&#34;i&#34;]
            j = ties_not_reported_by_R[&#34;j&#34;]
            # Step 4: Prepare lij indices and set appropriate values in pr_rho(lij,k)
            pr_rho[layer, i, j, :] = 0
            pr_rho[layer, i, j, 0] = 1

        ties_not_reported_by_X = get_ties_not_reported(self.X)

        if len(ties_not_reported_by_X) &gt; 0:

            layer = ties_not_reported_by_X[&#34;l&#34;]
            i = ties_not_reported_by_X[&#34;i&#34;]
            j = ties_not_reported_by_X[&#34;j&#34;]
            # Step 4: Prepare lij indices and set appropriate values in pr_rho(lij,k)
            pr_rho[layer, i, j, :] = 0
            pr_rho[layer, i, j, 0] = 1

        self.pr_rho = pr_rho
        self.logpr_rho = np.log(self.pr_rho + self.EPS)

    def _initialize_priors(self):
        &#34;&#34;&#34;
        Random initialization of the parameters theta, lambda, eta, rho.
        &#34;&#34;&#34;

        self.logger.verbose(&#34;Setting priors for gamma_shp, phi_shp, gamma_rte, phi_rte&#34;)
        # TODO: Could these variables be made sparse?

        # we include some randomness
        self.gamma_shp = self.alpha_theta * self.prng.random_sample(size=(self.L, self.M)) + self.alpha_theta
        self.phi_shp = self.alpha_lambda * self.prng.random_sample(size=(self.L, self.K)) + self.alpha_lambda
        self.gamma_rte = self.beta_theta * self.prng.random_sample(size=(self.L, self.M)) + self.beta_theta
        self.phi_rte = self.beta_lambda * self.prng.random_sample(size=(self.L, self.K)) + self.beta_lambda

        self.logger.verbose(&#34;Setting priors for nu_shp, nu_rte&#34;)
        if self.mutuality:
            self.nu_shp = self.alpha_mutuality * self.prng.random_sample(1)[0] + self.alpha_mutuality
            self.nu_rte = self.beta_mutuality + self.sumX  # this is fixed once and for all
            self.G_exp_nu = np.exp(sp.psi(self.nu_shp) - np.log(self.nu_rte))
        else:  # not use the mutuality (eta ~ 0.)
            self.nu_shp = 0.000001
            self.nu_rte = 1.0
            self.G_exp_nu = 0.0

        self.rho = np.copy(self.pr_rho)

        self.G_exp_theta = np.exp(sp.psi(self.gamma_shp) - np.log(self.gamma_rte))
        self.G_exp_lambda = np.exp(sp.psi(self.phi_shp) - np.log(self.phi_rte))

    def _initialize_old_variables(self):
        &#34;&#34;&#34;
        Initialize variables to keep the values of the parameters in the previous iteration.
        &#34;&#34;&#34;

        self.gamma_shp_old = np.copy(self.gamma_shp)
        self.gamma_rte_old = np.copy(self.gamma_rte)
        self.phi_shp_old = np.copy(self.phi_shp)
        self.phi_rte_old = np.copy(self.phi_rte)
        self.rho_old = np.copy(self.rho)
        self.nu_shp_old = np.copy(self.nu_shp)

    &#34;&#34;&#34;
    UPDATE VARIABLES
    &#34;&#34;&#34;

    def _update_CAVI(self, data, subs_nz, data_T_vals=None):
        &#34;&#34;&#34;
        Update parameters using Coordinate Ascent Variational Inference (CAVI)

        References:
            Blei, D. M., Kucukelbir, A., &amp; McAuliffe, J. D. (2017).
            Variational Inference: A Review for Statisticians.
            Journal of the American Statistical Association, 112(518), 859–877.
            https://doi.org/10.1080/01621459.2017.1285773

        Parameters
        ----------
        data : sptensor/dtensor
            Network adjacency tensor.
        subs_nz : tuple
            Indices of elements of data that are non-zero.
        data_T_vals : ndarray/None
            Array with values of entries A[j, i] given non-zero entry (i, j) - if mutuality=True.
        &#34;&#34;&#34;
        self.logger.verbose(&#34;Updating gamma&#34;)
        self._update_cache(data, subs_nz, data_T_vals)
        delta_gamma = self._update_gamma(subs_nz)

        self.logger.verbose(&#34;Updating phi&#34;)
        self._update_cache(data, subs_nz, data_T_vals)
        delta_phi = self._update_phi(subs_nz)

        self.logger.verbose(&#34;Updating rho&#34;)
        self._update_cache(data, subs_nz, data_T_vals)
        delta_rho = self._update_rho(subs_nz)
        if self.mutuality:
            self.logger.verbose(&#34;Updating nu&#34;)
            self._update_cache(data, subs_nz, data_T_vals)
            delta_nu = self._update_nu(subs_nz)
        else:
            delta_nu = 0.0

        return (delta_gamma, delta_phi, delta_rho, delta_nu)

    def _update_cache(self, data, subs_nz, data_T_vals=None):
        &#34;&#34;&#34;
        Update the cache used in the CAVI update.

        Parameters
        ----------
        data : sptensor
            Network adjacency tensor.
        subs_nz : tuple
            Indices of elements of data that are non-zero.
        data_T_vals : ndarray/None
            Array with values of entries A[j, i] given non-zero entry (i, j) - if mutuality=True.
        &#34;&#34;&#34;

        self.G_exp_theta = np.exp(sp.psi(self.gamma_shp) - np.log(self.gamma_rte))
        self.G_exp_lambda = np.exp(sp.psi(self.phi_shp) - np.log(self.phi_rte))

        if not self.mutuality:
            self.data_z1_nz = data.vals[:, np.newaxis].astype(float)
            self.data_z2_nz = None
        else:

            self.G_exp_nu = np.exp(sp.psi(self.nu_shp) - np.log(self.nu_rte))
            self.z1_nz = np.einsum(
                &#34;I,Ik-&gt;Ik&#34;, self.G_exp_theta[subs_nz[0], subs_nz[3]], self.G_exp_lambda[subs_nz[0], :],
            )  # has dim= (I,K)
            self.z2_nz = self.G_exp_nu * data_T_vals  # has dim= (I)
            self.z_den_nz = self.z1_nz + self.z2_nz[:, np.newaxis]  # has dim= (I,K)
            self.z_den_nz[self.z_den_nz == 0] = 1
            self.data_z1_nz = data.vals[:, np.newaxis] * self.z1_nz / self.z_den_nz
            self.data_z2_nz = data.vals[:, np.newaxis] * self.z2_nz[:, np.newaxis] / self.z_den_nz

    def _update_gamma(self, subs_nz):

        self.gamma_shp = self.alpha_theta + self.sp_uttkrp_theta(self.data_z1_nz, subs_nz)

        E_phi_rho = np.einsum(&#34;lijk,lk-&gt;lij&#34;, self.rho, self.phi_shp / self.phi_rte)

        if isinstance(self.R, skt.dtensor):
            self.gamma_rte = self.beta_theta + np.einsum(&#34;lij,lijm-&gt;lm&#34;, E_phi_rho, np.array(self.R))
        else:

            self.gamma_rte = self.beta_theta * np.ones(shape=(self.L, self.M))
            # sum over k, final dim=I
            tmp = E_phi_rho[self.R.subs[0], self.R.subs[1], self.R.subs[2]]
            for c, (l, m) in enumerate(zip(*(self.R.subs[0], self.R.subs[3]))):  # sum over i,j
                self.gamma_rte[l, m] += tmp[c]

        dist_gs = np.amax(abs(self.gamma_shp - self.gamma_shp_old))
        dist_gr = np.amax(abs(self.gamma_rte - self.gamma_rte_old))
        dist_gamma = max(dist_gs, dist_gr)

        self.gamma_shp_old = np.copy(self.gamma_shp)
        self.gamma_rte_old = np.copy(self.gamma_rte)

        return dist_gamma

    def _update_phi(self, subs_nz):

        self.phi_shp = self.alpha_lambda + self.sp_uttkrp_lambda(self.data_z1_nz, subs_nz)

        out = np.zeros_like(self.phi_rte)

        if isinstance(self.R, skt.dtensor):
            subs = self.R.nonzero()
        else:
            subs = self.R.subs

        Egamma = self.gamma_shp[subs[0], subs[3]] / self.gamma_rte[subs[0], subs[3]]
        tmp = self.rho[subs[0], subs[1], subs[2], :] * Egamma[:, np.newaxis]  # dim is (I,K)
        for k in range(self.K):  # sum over i,j,m
            out[:, k] += np.bincount(subs[0], weights=tmp[:, k], minlength=self.L)

        self.phi_rte = self.beta_lambda + out

        # self.phi_rte = np.einsum(&#39;lijk,lm-&gt;lijmk&#39;, self.rho, self.gamma_shp / self.gamma_rte)
        # self.phi_rte = self.beta_lambda + np.einsum(&#39;lijmk,lijm-&gt;lk&#39;, self.phi_rte, self.R)

        dist_pa = np.amax(abs(self.phi_shp - self.phi_shp_old))
        dist_pr = np.amax(abs(self.phi_rte - self.phi_shp_old))
        dist_phi = max(dist_pa, dist_pr)

        self.phi_shp_old = np.copy(self.phi_shp)
        self.phi_rte_old = np.copy(self.phi_rte)

        return dist_phi

    def _update_rho(self, subs_nz):

        # TODO: Make Exp_theta_lambda sparse
        if isinstance(self.R, skt.dtensor):
            Exp_theta_lambda = np.einsum(&#34;lijm,lm-&gt;lij&#34;, np.array(self.R), self.gamma_shp / self.gamma_rte)
        else:

            tmp = pd.DataFrame.from_dict(
                {
                    &#34;l&#34;: self.R.subs[0],
                    &#34;i&#34;: self.R.subs[1],
                    &#34;j&#34;: self.R.subs[2],
                    &#34;m&#34;: self.R.subs[3],
                    &#34;val&#34;: (self.gamma_shp / self.gamma_rte)[(self.R.subs[0], self.R.subs[3])] * self.R.vals,
                }
            )

            tmp = tmp.groupby([&#34;l&#34;, &#34;i&#34;, &#34;j&#34;])[[&#34;val&#34;]].sum()
            tmp.reset_index(inplace=True)
            Exp_theta_lambda = skt.sptensor(
                subs=(tmp[&#34;l&#34;], tmp[&#34;i&#34;], tmp[&#34;j&#34;]), vals=tmp[&#34;val&#34;].tolist(), shape=(self.L, self.N, self.N)
            )
            Exp_theta_lambda = Exp_theta_lambda.toarray()

        self.logger.verbose(&#34;calculating einsum (Exp_theta_lambda vs phi_shp/phi_rte)&#34;)
        Exp_theta_lambda = np.einsum(&#34;lij,lk-&gt;lijk&#34;, Exp_theta_lambda, self.phi_shp / self.phi_rte)

        self.logger.verbose(&#34;calculating log_rho&#34;)
        log_rho = self.logpr_rho + self.sp_uttkrp_rho(self.data_z1_nz, subs_nz) - Exp_theta_lambda

        self.logger.verbose(&#34;finishing updating rho&#34;)
        self.rho = np.exp(log_rho)
        sums_over_k = self.rho.sum(axis=3)

        # normalize so that the sum over k is 1
        self.rho[sums_over_k &gt; 0] /= sums_over_k[sums_over_k &gt; 0, np.newaxis]

        dist_rho = np.amax(abs(self.rho - self.rho_old))

        self.rho_old = np.copy(self.rho)
        self.logger.verbose(&#34;finished updating rho&#34;)

        return dist_rho

    def _update_nu(self, subs_nz):

        self.nu_shp = (
            self.alpha_mutuality + (self.data_z2_nz * self.rho[subs_nz[0], subs_nz[1], subs_nz[2], :]).sum()
        )
        dist_nu = abs(self.nu_shp - self.nu_shp_old)

        self.nu_shp_old = np.copy(self.nu_shp)

        return dist_nu

    def sp_uttkrp_theta(self, vals, subs):
        &#34;&#34;&#34;
        Compute the Khatri-Rao product (sparse version).

        Parameters
        ----------
        vals : ndarray
               Values of the non-zero entries.
        subs : tuple
               Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
               be equal to the dimension of tensor.

        Returns
        -------
        out : ndarray
              Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
              product of the membership matrix.
        &#34;&#34;&#34;

        out = np.zeros_like(self.gamma_shp)

        # sum over k, final dim=I
        tmp = (self.rho[subs[0], subs[1], subs[2], :].astype(vals.dtype) * vals).sum(axis=1)
        for c, (l, m) in enumerate(zip(*(subs[0], subs[3]))):  # sum over i,j
            out[l, m] += tmp[c]
        return out

    def sp_uttkrp_lambda(self, vals, subs):
        &#34;&#34;&#34;
        Compute the Khatri-Rao product (sparse version).

        Parameters
        ----------
        vals : ndarray
               Values of the non-zero entries.
        subs : tuple
               Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
               be equal to the dimension of tensor.

        Returns
        -------
        out : ndarray
              Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
              product of the membership matrix.
        &#34;&#34;&#34;

        out = np.zeros_like(self.phi_shp)
        tmp = self.rho[subs[0], subs[1], subs[2], :].astype(vals.dtype) * vals  # dim is (I,K)
        for k in range(self.K):  # sum over i,j,m
            out[:, k] += np.bincount(subs[0], weights=tmp[:, k], minlength=self.L)

        return out

    def sp_uttkrp_rho(self, vals, subs):
        &#34;&#34;&#34;
        Compute the Khatri-Rao product (sparse version).

        Parameters
        ----------
        vals : ndarray
               Values of the non-zero entries.
        subs : tuple
               Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
               be equal to the dimension of tensor.

        Returns
        -------
        out : ndarray
              Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
              product of the membership matrix.
        &#34;&#34;&#34;

        out = np.zeros_like(self.rho)

        # TODO: try by removing ExpLog_theta
        ExpLog_theta = sp.psi(self.gamma_shp) - np.log(self.gamma_rte)
        ExpLog_lambda = sp.psi(self.phi_shp) - np.log(self.phi_rte)

        # dim is (I,K)
        # tmp = (ExpLog_lambda[subs[0], :]).astype(vals.dtype) * vals
        tmp = (ExpLog_theta[subs[0], subs[3]][:, np.newaxis] + ExpLog_lambda[subs[0], :]).astype(
            vals.dtype
        ) * vals
        # sum over m
        for c, (l, i, j) in enumerate(zip(*(subs[0], subs[1], subs[2]))):
            out[l, i, j, :] += tmp[c, :]

        return out

    def _update_optimal_parameters(self):
        &#34;&#34;&#34;
        Update values of the parameters after convergence.
        &#34;&#34;&#34;

        # Parameters
        self.gamma_shp_f = np.copy(self.gamma_shp)
        self.gamma_rte_f = np.copy(self.gamma_rte)
        self.phi_shp_f = np.copy(self.phi_shp)
        self.phi_rte_f = np.copy(self.phi_rte)
        self.nu_shp_f = np.copy(self.nu_shp)
        self.nu_rte_f = np.copy(self.nu_rte)
        self.rho_f = np.copy(self.rho)

        # Geometric expectations
        self.G_exp_theta_f = np.exp(sp.psi(self.gamma_shp_f) - np.log(self.gamma_rte_f))
        self.G_exp_lambda_f = np.exp(sp.psi(self.phi_shp_f) - np.log(self.phi_rte_f))
        self.G_exp_nu_f = np.exp(sp.psi(self.nu_shp_f) - np.log(self.nu_rte_f))

    &#34;&#34;&#34;
    ELBO
    &#34;&#34;&#34;

    def __ELBO(self, data, data_T, subs_nz):
        &#34;&#34;&#34;
        Function to compute the ELBO.
        &#34;&#34;&#34;

        E_exp_theta = self.gamma_shp / self.gamma_rte
        E_exp_lambda = self.phi_shp / self.phi_rte
        E_exp_rec = self.nu_shp / self.nu_rte

        E_PoissonMean = self.calculate_mean_poisson(
            G_exp_theta=E_exp_theta,
            G_exp_lambda=E_exp_lambda,
            G_exp_nu=E_exp_rec,
            rho=self.rho,
            X_T=data_T,
            R=self.R,
        )
        elbo = -E_PoissonMean.vals.sum()

        G_PoissonMean = self.calculate_mean_poisson(
            G_exp_theta=self.G_exp_theta,
            G_exp_lambda=self.G_exp_lambda,
            G_exp_nu=self.G_exp_nu,
            rho=np.exp(self.rho),
            X_T=data_T,
            R=self.R,
        )

        # Filter it to only include (l,i,j,m) that are in X
        rho_prior_lijm = pd.DataFrame.from_dict(
            {&#34;l&#34;: subs_nz[0], &#34;i&#34;: subs_nz[1], &#34;j&#34;: subs_nz[2], &#34;m&#34;: subs_nz[3]}
        )
        G_PoissonMean_lijm = pd.DataFrame.from_dict(
            {
                &#34;l&#34;: G_PoissonMean.subs[0],
                &#34;i&#34;: G_PoissonMean.subs[1],
                &#34;j&#34;: G_PoissonMean.subs[2],
                &#34;m&#34;: G_PoissonMean.subs[3],
                &#34;val&#34;: G_PoissonMean.vals,
            }
        )

        logPoissonMean = pd.merge(rho_prior_lijm, G_PoissonMean_lijm, how=&#34;left&#34;)
        logPoissonMean.fillna(0, inplace=True)
        logPoissonMean = logPoissonMean[&#34;val&#34;].values  # convert to numpy array

        logPoissonMean = np.log(logPoissonMean + self.EPS)
        elbo += (data.vals * logPoissonMean).sum()

        elbo += gamma_elbo_term(
            pa=self.alpha_theta, pb=self.beta_theta, qa=self.gamma_shp, qb=self.gamma_rte
        ).sum()
        elbo += gamma_elbo_term(
            pa=self.alpha_lambda, pb=self.beta_lambda, qa=self.phi_shp, qb=self.phi_rte
        ).sum()
        elbo += gamma_elbo_term(
            pa=self.alpha_mutuality, pb=self.beta_mutuality, qa=self.nu_shp, qb=self.nu_rte
        )

        elbo += categorical_elbo_term(self.rho, self.pr_rho, self.EPS).sum()

        if np.isnan(elbo):
            raise ValueError(&#34;ELBO is NaN!!!!&#34;)
            sys.exit(1)
        else:
            return elbo

    def _check_for_convergence(
        self,
        data: np.ndarray,
        data_T: np.ndarray,
        subs_nz,
        r: int,
        iter: int,
        elbo: float,
        coincide: int,
        reached_convergence,
    ):
        &#34;&#34;&#34;
        Check for convergence by using the ELBO values.
        &#34;&#34;&#34;

        if iter == 1 or iter % 10 == 0 or iter == self.max_iter:
            old_L = elbo
            # loglik = self.__Likelihood(data, data_T, subs_nz)
            elbo = self.__ELBO(data, data_T, subs_nz)

            if abs(elbo - old_L) &lt; self.convergence_tol:
                coincide += 1
            else:
                coincide = 0

        if coincide &gt; self.decision:
            reached_convergence = True

        if iter == 1 or iter % 10 == 0 or iter == self.max_iter:
            msg = f&#34;Realisation {r:2} | Iter {iter:4} | ELBO value: {elbo:6.12f} | &#34;
            msg += f&#34;Reached convergence: {reached_convergence}&#34;
            self.logger.debug(msg)

        iter += 1

        return iter, elbo, coincide, reached_convergence

    &#34;&#34;&#34;
    UTILS
    &#34;&#34;&#34;

    def calculate_mean_poisson(
        self, G_exp_theta=None, G_exp_lambda=None, G_exp_nu=None, rho=None, X_T=None, R=None,
    ):

        if G_exp_theta is None:
            G_exp_theta = self.G_exp_theta_f

        if G_exp_lambda is None:
            G_exp_lambda = self.G_exp_lambda_f

        if G_exp_nu is None:
            G_exp_nu = self.G_exp_nu_f

        if rho is None:
            rho = self.rho_f

        if X_T is None:
            X_T = self.data_T

        if R is None:
            R = self.R

        &#34;&#34;&#34;
        exp value of: [theta * lambda + eta * X.T]
        &#34;&#34;&#34;
        if isinstance(self.R, skt.dtensor):
            R_subs = self.R.nonzero()
        else:
            R_subs = self.R.subs

        # Following Equation 2:
        ThetaLambda = np.einsum(&#34;I,Ik-&gt;Ik&#34;, G_exp_theta[R_subs[0], R_subs[3]], G_exp_lambda[R_subs[0], :],)

        if isinstance(X_T, skt.dtensor) or isinstance(X_T, np.ndarray):
            PoissonMean = ThetaLambda + G_exp_nu * X_T[R_subs[0], R_subs[1], R_subs[2], R_subs[3], np.newaxis]
        else:
            R_lijm = pd.DataFrame.from_dict({&#34;l&#34;: R_subs[0], &#34;i&#34;: R_subs[1], &#34;j&#34;: R_subs[2], &#34;m&#34;: R_subs[3]})
            X_lijm = pd.DataFrame.from_dict(
                {&#34;l&#34;: X_T.subs[0], &#34;i&#34;: X_T.subs[1], &#34;j&#34;: X_T.subs[2], &#34;m&#34;: X_T.subs[3], &#34;val&#34;: X_T.vals}
            )

            X_T_array = pd.merge(R_lijm, X_lijm, how=&#34;left&#34;)
            X_T_array.fillna(0, inplace=True)
            X_T_array = X_T_array[&#34;val&#34;].values  # convert to numpy array

            PoissonMean = ThetaLambda + G_exp_nu * X_T_array[:, np.newaxis]

        vals = np.einsum(&#34;Ik,Ik-&gt;I&#34;, rho[R_subs[0], R_subs[1], R_subs[2], :], PoissonMean)
        rho_PoissonMean = skt.sptensor(subs=R_subs, vals=vals)

        return rho_PoissonMean</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.base.TransformerMixin</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="vimure.model.VimureModel.calculate_mean_poisson"><code class="name flex">
<span>def <span class="ident">calculate_mean_poisson</span></span>(<span>self, G_exp_theta=None, G_exp_lambda=None, G_exp_nu=None, rho=None, X_T=None, R=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_mean_poisson(
    self, G_exp_theta=None, G_exp_lambda=None, G_exp_nu=None, rho=None, X_T=None, R=None,
):

    if G_exp_theta is None:
        G_exp_theta = self.G_exp_theta_f

    if G_exp_lambda is None:
        G_exp_lambda = self.G_exp_lambda_f

    if G_exp_nu is None:
        G_exp_nu = self.G_exp_nu_f

    if rho is None:
        rho = self.rho_f

    if X_T is None:
        X_T = self.data_T

    if R is None:
        R = self.R

    &#34;&#34;&#34;
    exp value of: [theta * lambda + eta * X.T]
    &#34;&#34;&#34;
    if isinstance(self.R, skt.dtensor):
        R_subs = self.R.nonzero()
    else:
        R_subs = self.R.subs

    # Following Equation 2:
    ThetaLambda = np.einsum(&#34;I,Ik-&gt;Ik&#34;, G_exp_theta[R_subs[0], R_subs[3]], G_exp_lambda[R_subs[0], :],)

    if isinstance(X_T, skt.dtensor) or isinstance(X_T, np.ndarray):
        PoissonMean = ThetaLambda + G_exp_nu * X_T[R_subs[0], R_subs[1], R_subs[2], R_subs[3], np.newaxis]
    else:
        R_lijm = pd.DataFrame.from_dict({&#34;l&#34;: R_subs[0], &#34;i&#34;: R_subs[1], &#34;j&#34;: R_subs[2], &#34;m&#34;: R_subs[3]})
        X_lijm = pd.DataFrame.from_dict(
            {&#34;l&#34;: X_T.subs[0], &#34;i&#34;: X_T.subs[1], &#34;j&#34;: X_T.subs[2], &#34;m&#34;: X_T.subs[3], &#34;val&#34;: X_T.vals}
        )

        X_T_array = pd.merge(R_lijm, X_lijm, how=&#34;left&#34;)
        X_T_array.fillna(0, inplace=True)
        X_T_array = X_T_array[&#34;val&#34;].values  # convert to numpy array

        PoissonMean = ThetaLambda + G_exp_nu * X_T_array[:, np.newaxis]

    vals = np.einsum(&#34;Ik,Ik-&gt;I&#34;, rho[R_subs[0], R_subs[1], R_subs[2], :], PoissonMean)
    rho_PoissonMean = skt.sptensor(subs=R_subs, vals=vals)

    return rho_PoissonMean</code></pre>
</details>
</dd>
<dt id="vimure.model.VimureModel.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: numpy.ndarray, theta_prior=(0.1, 0.1), lambda_prior=(10.0, 10.0), eta_prior=(0.5, 1.0), rho_prior=None, seed: int = None, **extra_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit a probabilistic generative model to double sampled networks. It returns reliability parameters for the
reporters (theta), average interactions for the links (lambda) and the estimate of the true and unknown
network (rho). The inference is performed with a Variational Inference approach.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Network adjacency tensor.</dd>
<dt><strong><code>theta_prior</code></strong> :&ensp;<code>2D tuple</code></dt>
<dd>Shape and scale hyperparameters for variable theta</dd>
<dt><strong><code>lambda_prior</code></strong> :&ensp;<code>2D tuple</code></dt>
<dd>Shape and scale hyperparameters for variable lambda</dd>
<dt><strong><code>eta_prior</code></strong> :&ensp;<code>2D tuple</code></dt>
<dd>Shape and scale hyperparameters for variable eta</dd>
<dt><strong><code>rho_prior</code></strong> :&ensp;<code>None/ndarray</code></dt>
<dd>Array with prior values of the rho parameter - if ndarray.</dd>
</dl>
<h2 id="extra-parameters-advanced-tuning-of-inference">Extra parameters (Advanced tuning of inference)</h2>
<p>R: ndarray
a multidimensional array L x N x N x M indicating which reports to consider
K: None/int
Value of the maximum entry of the network - i
EPS : float
White noise. Default: 1e-12
bias0: float
Bias for rho_prior entry 0. Default: 0.2
max_iter: int
Maximum number of iteration steps before aborting. Default=500</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self.rho_f, self.G_exp_theta_f, self.G_exp_lambda_f, self.G_exp_nu_f, self.maxL</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(
    self,
    X: np.ndarray,
    theta_prior=(0.1, 0.1),
    lambda_prior=(10.0, 10.0),
    eta_prior=(0.5, 1.0),
    rho_prior=None,
    seed: int = None,
    **extra_params,
):
    &#34;&#34;&#34;
    Fit a probabilistic generative model to double sampled networks. It returns reliability parameters for the
    reporters (theta), average interactions for the links (lambda) and the estimate of the true and unknown
    network (rho). The inference is performed with a Variational Inference approach.

    Parameters
    ----------
    X : ndarray
        Network adjacency tensor.
    theta_prior: 2D tuple
        Shape and scale hyperparameters for variable theta
    lambda_prior: 2D tuple
        Shape and scale hyperparameters for variable lambda
    eta_prior: 2D tuple
        Shape and scale hyperparameters for variable eta
    rho_prior : None/ndarray
        Array with prior values of the rho parameter - if ndarray.

    Extra parameters (Advanced tuning of inference)
    ----------------
    R: ndarray
        a multidimensional array L x N x N x M indicating which reports to consider
    K: None/int
        Value of the maximum entry of the network - i
    EPS : float
        White noise. Default: 1e-12
    bias0: float
        Bias for rho_prior entry 0. Default: 0.2
    max_iter: int
        Maximum number of iteration steps before aborting. Default=500

    Returns
    -------
    self.rho_f, self.G_exp_theta_f, self.G_exp_lambda_f, self.G_exp_nu_f, self.maxL
    &#34;&#34;&#34;

    self.logger.debug(&#34;Checking user parameters passed to the VimureModel.fit()&#34;)
    self.__check_fit_params(
        X=X,
        theta_prior=theta_prior,
        lambda_prior=lambda_prior,
        rho_prior=rho_prior,
        eta_prior=eta_prior,
        seed=seed,
        **extra_params,
    )

    &#34;&#34;&#34;
    Inference
    &#34;&#34;&#34;
    maxL = -INF  # initialization of the maximum elbo
    trace = []  # Keep track of elbo values and running time

    for r in range(self.num_realisations):

        self.logger.debug(&#34;Initializing priors&#34;)
        if r &lt; 5:  # the first 5 runs are with bias = 0
            self._set_rho_prior()
        else:
            curr_bias0 = (r - 4) * self.bias0
            self._set_rho_prior(bias0=curr_bias0)

        self._initialize_priors()
        self._initialize_old_variables()

        coincide = 0
        iter = 1
        reached_convergence = False

        elbo = -INF  # initialization of the elbo

        while not reached_convergence and iter &lt;= self.max_iter:
            time_start = time.time()
            delta_gamma, delta_phi, delta_rho, delta_nu = self._update_CAVI(
                data=self.X, subs_nz=self.subs_nz, data_T_vals=self.data_T_vals
            )
            runtime = time.time() - time_start

            iter, elbo, coincide, reached_convergence = self._check_for_convergence(
                data=self.X,
                data_T=self.data_T,
                subs_nz=self.subs_nz,
                r=r,
                iter=iter,
                elbo=elbo,
                coincide=coincide,
                reached_convergence=reached_convergence,
            )

            if (iter - 1) % 10 == 0:
                trace.append((r, self.seed, iter - 1, elbo, runtime, reached_convergence))

        if maxL &lt; elbo:
            self._update_optimal_parameters()
            maxL = elbo

        if self.seed is None:
            new_seed = self.prng.randint(1, 500)
        else:
            new_seed = self.seed + self.prng.randint(1, 500)

        self._change_seed(new_seed)
        # end cycle over realizations

    cols = [&#34;realisation&#34;, &#34;seed&#34;, &#34;iter&#34;, &#34;elbo&#34;, &#34;runtime&#34;, &#34;reached_convergence&#34;]
    self.trace = pd.DataFrame(trace, columns=cols)

    self.maxL = maxL

    # TODO: Consider removing non-final copies of internal dataframes such as self.rho, self.gamma_rte
    #       and keep only the final versions (self.rho_f, self.gamma_rte) to save disk space
    #       when saving model to disk
    return self</code></pre>
</details>
</dd>
<dt id="vimure.model.VimureModel.sp_uttkrp_lambda"><code class="name flex">
<span>def <span class="ident">sp_uttkrp_lambda</span></span>(<span>self, vals, subs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Khatri-Rao product (sparse version).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vals</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Values of the non-zero entries.</dd>
<dt><strong><code>subs</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
be equal to the dimension of tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
product of the membership matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sp_uttkrp_lambda(self, vals, subs):
    &#34;&#34;&#34;
    Compute the Khatri-Rao product (sparse version).

    Parameters
    ----------
    vals : ndarray
           Values of the non-zero entries.
    subs : tuple
           Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
           be equal to the dimension of tensor.

    Returns
    -------
    out : ndarray
          Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
          product of the membership matrix.
    &#34;&#34;&#34;

    out = np.zeros_like(self.phi_shp)
    tmp = self.rho[subs[0], subs[1], subs[2], :].astype(vals.dtype) * vals  # dim is (I,K)
    for k in range(self.K):  # sum over i,j,m
        out[:, k] += np.bincount(subs[0], weights=tmp[:, k], minlength=self.L)

    return out</code></pre>
</details>
</dd>
<dt id="vimure.model.VimureModel.sp_uttkrp_rho"><code class="name flex">
<span>def <span class="ident">sp_uttkrp_rho</span></span>(<span>self, vals, subs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Khatri-Rao product (sparse version).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vals</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Values of the non-zero entries.</dd>
<dt><strong><code>subs</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
be equal to the dimension of tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
product of the membership matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sp_uttkrp_rho(self, vals, subs):
    &#34;&#34;&#34;
    Compute the Khatri-Rao product (sparse version).

    Parameters
    ----------
    vals : ndarray
           Values of the non-zero entries.
    subs : tuple
           Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
           be equal to the dimension of tensor.

    Returns
    -------
    out : ndarray
          Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
          product of the membership matrix.
    &#34;&#34;&#34;

    out = np.zeros_like(self.rho)

    # TODO: try by removing ExpLog_theta
    ExpLog_theta = sp.psi(self.gamma_shp) - np.log(self.gamma_rte)
    ExpLog_lambda = sp.psi(self.phi_shp) - np.log(self.phi_rte)

    # dim is (I,K)
    # tmp = (ExpLog_lambda[subs[0], :]).astype(vals.dtype) * vals
    tmp = (ExpLog_theta[subs[0], subs[3]][:, np.newaxis] + ExpLog_lambda[subs[0], :]).astype(
        vals.dtype
    ) * vals
    # sum over m
    for c, (l, i, j) in enumerate(zip(*(subs[0], subs[1], subs[2]))):
        out[l, i, j, :] += tmp[c, :]

    return out</code></pre>
</details>
</dd>
<dt id="vimure.model.VimureModel.sp_uttkrp_theta"><code class="name flex">
<span>def <span class="ident">sp_uttkrp_theta</span></span>(<span>self, vals, subs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the Khatri-Rao product (sparse version).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vals</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Values of the non-zero entries.</dd>
<dt><strong><code>subs</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
be equal to the dimension of tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
product of the membership matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sp_uttkrp_theta(self, vals, subs):
    &#34;&#34;&#34;
    Compute the Khatri-Rao product (sparse version).

    Parameters
    ----------
    vals : ndarray
           Values of the non-zero entries.
    subs : tuple
           Indices of elements that are non-zero. It is a n-tuple of array-likes and the length of tuple n must
           be equal to the dimension of tensor.

    Returns
    -------
    out : ndarray
          Matrix which is the result of the matrix product of the unfolding of the tensor and the Khatri-Rao
          product of the membership matrix.
    &#34;&#34;&#34;

    out = np.zeros_like(self.gamma_shp)

    # sum over k, final dim=I
    tmp = (self.rho[subs[0], subs[1], subs[2], :].astype(vals.dtype) * vals).sum(axis=1)
    for c, (l, m) in enumerate(zip(*(subs[0], subs[3]))):  # sum over i,j
        out[l, m] += tmp[c]
    return out</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="vimure" href="index.html">vimure</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="vimure.model.categorical_elbo_term" href="#vimure.model.categorical_elbo_term">categorical_elbo_term</a></code></li>
<li><code><a title="vimure.model.gamma_elbo_term" href="#vimure.model.gamma_elbo_term">gamma_elbo_term</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="vimure.model.VimureModel" href="#vimure.model.VimureModel">VimureModel</a></code></h4>
<ul class="">
<li><code><a title="vimure.model.VimureModel.calculate_mean_poisson" href="#vimure.model.VimureModel.calculate_mean_poisson">calculate_mean_poisson</a></code></li>
<li><code><a title="vimure.model.VimureModel.fit" href="#vimure.model.VimureModel.fit">fit</a></code></li>
<li><code><a title="vimure.model.VimureModel.sp_uttkrp_lambda" href="#vimure.model.VimureModel.sp_uttkrp_lambda">sp_uttkrp_lambda</a></code></li>
<li><code><a title="vimure.model.VimureModel.sp_uttkrp_rho" href="#vimure.model.VimureModel.sp_uttkrp_rho">sp_uttkrp_rho</a></code></li>
<li><code><a title="vimure.model.VimureModel.sp_uttkrp_theta" href="#vimure.model.VimureModel.sp_uttkrp_theta">sp_uttkrp_theta</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>